{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630303e2-1c93-4b37-b037-f505bc06e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LTSF_LinearModel_lib import LTSF_FF_DLinear\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "# 调用BTC爬取部分\n",
    "import sys\n",
    "sys.path.append(\"E:/Python_WorkSpace/量化交易/\")  # 增加指定的绝对路径,进入系统路径,从而便于该目录下的库调用\n",
    "\n",
    "from BTCCrawl_To_DataFrame_Class import BTC_data_acquire as BTC\n",
    "from BTCCrawl_To_DataFrame_Class import get_api_key\n",
    "Folder_base = \"E:/Python_WorkSpace/量化交易/data/\"\n",
    "config_file_path = \"E:/Python_WorkSpace/量化交易/BTCCrawl_To_DataFrame_Class_config.ini\"\n",
    "# URL = \"https://api.coincap.io/v2/candles?exchange=binance&interval=h12&baseId=bitcoin&quoteId=tether\"\n",
    "URL = 'https://data.binance.com'\n",
    "StartDate = \"2022-11-20\"\n",
    "EndDate = \"2023-03-01\"\n",
    "BTC_json = \"BTC_h12.json\"\n",
    "BinanceBTC_json = \"BinanceBTC_h12.json\"\n",
    "\n",
    "api_key, api_secret = get_api_key(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d0976b-ad5b-4638-b50e-93e4079d0175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binance Rest API 联通测试通过 !\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3992 entries, 2017-08-17 00:00:00+00:00 to 2023-02-03 00:00:00+00:00\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   open        3992 non-null   float32\n",
      " 1   high        3992 non-null   float32\n",
      " 2   low         3992 non-null   float32\n",
      " 3   close       3992 non-null   float32\n",
      " 4   volume      3992 non-null   float32\n",
      " 5   amount      3992 non-null   float32\n",
      " 6   num_trades  3992 non-null   float32\n",
      " 7   bid_volume  3992 non-null   float32\n",
      " 8   bid_amount  3992 non-null   float32\n",
      "dtypes: float32(9)\n",
      "memory usage: 171.5 KB\n",
      "从web中读出Data.info:None\n",
      "BTC数据合并存入:E:/Python_WorkSpace/量化交易/data/BinanceBTC_h12.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return</th>\n",
       "      <th>Roll_price_sma</th>\n",
       "      <th>Roll_price_min</th>\n",
       "      <th>Roll_price_max</th>\n",
       "      <th>Roll_return_mom</th>\n",
       "      <th>Roll_return_std</th>\n",
       "      <th>Mark_return</th>\n",
       "      <th>d_amplitude</th>\n",
       "      <th>volume</th>\n",
       "      <th>RSI_7</th>\n",
       "      <th>...</th>\n",
       "      <th>high_pre_close</th>\n",
       "      <th>low_pre_close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>amount</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>bid_amount</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3.958000e+03</td>\n",
       "      <td>3.958000e+03</td>\n",
       "      <td>3958.000000</td>\n",
       "      <td>3.958000e+03</td>\n",
       "      <td>3958.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>19524.773066</td>\n",
       "      <td>18041.433135</td>\n",
       "      <td>21019.626904</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.025382</td>\n",
       "      <td>0.525013</td>\n",
       "      <td>822.892029</td>\n",
       "      <td>36038.406250</td>\n",
       "      <td>51.661140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019639</td>\n",
       "      <td>0.978810</td>\n",
       "      <td>19952.273438</td>\n",
       "      <td>19129.390625</td>\n",
       "      <td>19564.921875</td>\n",
       "      <td>7.696118e+08</td>\n",
       "      <td>6.625322e+05</td>\n",
       "      <td>17933.429688</td>\n",
       "      <td>3.811720e+08</td>\n",
       "      <td>19569.742188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.028596</td>\n",
       "      <td>16572.614889</td>\n",
       "      <td>15350.890577</td>\n",
       "      <td>17825.753531</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.499437</td>\n",
       "      <td>1003.168396</td>\n",
       "      <td>39833.667969</td>\n",
       "      <td>13.928853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>16936.292969</td>\n",
       "      <td>16235.105469</td>\n",
       "      <td>16607.675781</td>\n",
       "      <td>9.680476e+08</td>\n",
       "      <td>8.871228e+05</td>\n",
       "      <td>19755.556641</td>\n",
       "      <td>4.788796e+08</td>\n",
       "      <td>16606.134766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.268357</td>\n",
       "      <td>3359.310486</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>3504.770020</td>\n",
       "      <td>-0.034640</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.820068</td>\n",
       "      <td>127.036331</td>\n",
       "      <td>10.604572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998651</td>\n",
       "      <td>0.699475</td>\n",
       "      <td>3249.000000</td>\n",
       "      <td>2817.000000</td>\n",
       "      <td>2870.899902</td>\n",
       "      <td>5.881363e+05</td>\n",
       "      <td>1.007000e+03</td>\n",
       "      <td>55.618614</td>\n",
       "      <td>2.577815e+05</td>\n",
       "      <td>2919.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.010006</td>\n",
       "      <td>7365.368378</td>\n",
       "      <td>6735.000000</td>\n",
       "      <td>8059.979980</td>\n",
       "      <td>-0.003078</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>194.652466</td>\n",
       "      <td>14796.493896</td>\n",
       "      <td>41.878640</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005836</td>\n",
       "      <td>0.973590</td>\n",
       "      <td>7541.964966</td>\n",
       "      <td>7275.552368</td>\n",
       "      <td>7389.625000</td>\n",
       "      <td>1.234654e+08</td>\n",
       "      <td>1.288170e+05</td>\n",
       "      <td>7586.004395</td>\n",
       "      <td>6.426903e+07</td>\n",
       "      <td>7391.865112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000795</td>\n",
       "      <td>10691.134790</td>\n",
       "      <td>9966.530273</td>\n",
       "      <td>11528.250000</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>416.659912</td>\n",
       "      <td>23630.557617</td>\n",
       "      <td>50.576717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012710</td>\n",
       "      <td>0.986596</td>\n",
       "      <td>11029.654785</td>\n",
       "      <td>10562.500000</td>\n",
       "      <td>10798.834961</td>\n",
       "      <td>2.989546e+08</td>\n",
       "      <td>3.331350e+05</td>\n",
       "      <td>11835.131348</td>\n",
       "      <td>1.504583e+08</td>\n",
       "      <td>10808.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.012285</td>\n",
       "      <td>31058.163867</td>\n",
       "      <td>28715.320312</td>\n",
       "      <td>34258.988281</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.031014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1147.425781</td>\n",
       "      <td>39806.104492</td>\n",
       "      <td>61.130992</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025559</td>\n",
       "      <td>0.993800</td>\n",
       "      <td>31737.980469</td>\n",
       "      <td>29843.435547</td>\n",
       "      <td>30815.485352</td>\n",
       "      <td>1.167595e+09</td>\n",
       "      <td>7.901168e+05</td>\n",
       "      <td>19614.153809</td>\n",
       "      <td>5.755117e+08</td>\n",
       "      <td>30815.502930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.237092</td>\n",
       "      <td>64595.437500</td>\n",
       "      <td>60574.488281</td>\n",
       "      <td>67594.976562</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>0.104382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11858.800781</td>\n",
       "      <td>474581.750000</td>\n",
       "      <td>94.898909</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321000</td>\n",
       "      <td>1.000920</td>\n",
       "      <td>69000.000000</td>\n",
       "      <td>67015.203125</td>\n",
       "      <td>67594.976562</td>\n",
       "      <td>9.054959e+09</td>\n",
       "      <td>8.848036e+06</td>\n",
       "      <td>235625.812500</td>\n",
       "      <td>4.496810e+09</td>\n",
       "      <td>67594.976562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        log_return  Roll_price_sma  Roll_price_min  Roll_price_max  \\\n",
       "count  3958.000000     3958.000000     3958.000000     3958.000000   \n",
       "mean      0.000419    19524.773066    18041.433135    21019.626904   \n",
       "std       0.028596    16572.614889    15350.890577    17825.753531   \n",
       "min      -0.268357     3359.310486     2919.000000     3504.770020   \n",
       "25%      -0.010006     7365.368378     6735.000000     8059.979980   \n",
       "50%       0.000795    10691.134790     9966.530273    11528.250000   \n",
       "75%       0.012285    31058.163867    28715.320312    34258.988281   \n",
       "max       0.237092    64595.437500    60574.488281    67594.976562   \n",
       "\n",
       "       Roll_return_mom  Roll_return_std  Mark_return   d_amplitude  \\\n",
       "count      3958.000000      3958.000000  3958.000000   3958.000000   \n",
       "mean          0.000416         0.025382     0.525013    822.892029   \n",
       "std           0.006604         0.013094     0.499437   1003.168396   \n",
       "min          -0.034640         0.003899     0.000000     17.820068   \n",
       "25%          -0.003078         0.016689     0.000000    194.652466   \n",
       "50%           0.000492         0.023021     1.000000    416.659912   \n",
       "75%           0.004020         0.031014     1.000000   1147.425781   \n",
       "max           0.027119         0.104382     1.000000  11858.800781   \n",
       "\n",
       "              volume        RSI_7  ...  high_pre_close  low_pre_close  \\\n",
       "count    3958.000000  3958.000000  ...     3958.000000    3958.000000   \n",
       "mean    36038.406250    51.661140  ...        1.019639       0.978810   \n",
       "std     39833.667969    13.928853  ...        0.022351       0.025552   \n",
       "min       127.036331    10.604572  ...        0.998651       0.699475   \n",
       "25%     14796.493896    41.878640  ...        1.005836       0.973590   \n",
       "50%     23630.557617    50.576717  ...        1.012710       0.986596   \n",
       "75%     39806.104492    61.130992  ...        1.025559       0.993800   \n",
       "max    474581.750000    94.898909  ...        1.321000       1.000920   \n",
       "\n",
       "               high           low          open        amount    num_trades  \\\n",
       "count   3958.000000   3958.000000   3958.000000  3.958000e+03  3.958000e+03   \n",
       "mean   19952.273438  19129.390625  19564.921875  7.696118e+08  6.625322e+05   \n",
       "std    16936.292969  16235.105469  16607.675781  9.680476e+08  8.871228e+05   \n",
       "min     3249.000000   2817.000000   2870.899902  5.881363e+05  1.007000e+03   \n",
       "25%     7541.964966   7275.552368   7389.625000  1.234654e+08  1.288170e+05   \n",
       "50%    11029.654785  10562.500000  10798.834961  2.989546e+08  3.331350e+05   \n",
       "75%    31737.980469  29843.435547  30815.485352  1.167595e+09  7.901168e+05   \n",
       "max    69000.000000  67015.203125  67594.976562  9.054959e+09  8.848036e+06   \n",
       "\n",
       "          bid_volume    bid_amount         close  \n",
       "count    3958.000000  3.958000e+03   3958.000000  \n",
       "mean    17933.429688  3.811720e+08  19569.742188  \n",
       "std     19755.556641  4.788796e+08  16606.134766  \n",
       "min        55.618614  2.577815e+05   2919.000000  \n",
       "25%      7586.004395  6.426903e+07   7391.865112  \n",
       "50%     11835.131348  1.504583e+08  10808.020020  \n",
       "75%     19614.153809  5.755117e+08  30815.502930  \n",
       "max    235625.812500  4.496810e+09  67594.976562  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return</th>\n",
       "      <th>Roll_price_sma</th>\n",
       "      <th>Roll_price_min</th>\n",
       "      <th>Roll_price_max</th>\n",
       "      <th>Roll_return_mom</th>\n",
       "      <th>Roll_return_std</th>\n",
       "      <th>Mark_return</th>\n",
       "      <th>d_amplitude</th>\n",
       "      <th>volume</th>\n",
       "      <th>RSI_7</th>\n",
       "      <th>...</th>\n",
       "      <th>high_pre_close</th>\n",
       "      <th>low_pre_close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>amount</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>bid_amount</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-03 08:00:00+08:00</th>\n",
       "      <td>0.003094</td>\n",
       "      <td>4475.064014</td>\n",
       "      <td>4280.680176</td>\n",
       "      <td>4834.910156</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>1</td>\n",
       "      <td>257.959961</td>\n",
       "      <td>335.535156</td>\n",
       "      <td>57.173478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054251</td>\n",
       "      <td>0.996570</td>\n",
       "      <td>4714.759766</td>\n",
       "      <td>4456.799805</td>\n",
       "      <td>4508.50000</td>\n",
       "      <td>1541936.75</td>\n",
       "      <td>2991.0</td>\n",
       "      <td>123.085823</td>\n",
       "      <td>565981.3750</td>\n",
       "      <td>4486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-03 20:00:00+08:00</th>\n",
       "      <td>0.005132</td>\n",
       "      <td>4484.717529</td>\n",
       "      <td>4280.680176</td>\n",
       "      <td>4834.910156</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.020353</td>\n",
       "      <td>1</td>\n",
       "      <td>285.509766</td>\n",
       "      <td>355.681030</td>\n",
       "      <td>58.092390</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021810</td>\n",
       "      <td>0.958165</td>\n",
       "      <td>4583.839844</td>\n",
       "      <td>4298.330078</td>\n",
       "      <td>4475.22998</td>\n",
       "      <td>1579116.00</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>135.848709</td>\n",
       "      <td>603663.3125</td>\n",
       "      <td>4509.080078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           log_return  Roll_price_sma  Roll_price_min  \\\n",
       "2017-09-03 08:00:00+08:00    0.003094     4475.064014     4280.680176   \n",
       "2017-09-03 20:00:00+08:00    0.005132     4484.717529     4280.680176   \n",
       "\n",
       "                           Roll_price_max  Roll_return_mom  Roll_return_std  \\\n",
       "2017-09-03 08:00:00+08:00     4834.910156         0.003270         0.021080   \n",
       "2017-09-03 20:00:00+08:00     4834.910156         0.002188         0.020353   \n",
       "\n",
       "                           Mark_return  d_amplitude      volume      RSI_7  \\\n",
       "2017-09-03 08:00:00+08:00            1   257.959961  335.535156  57.173478   \n",
       "2017-09-03 20:00:00+08:00            1   285.509766  355.681030  58.092390   \n",
       "\n",
       "                           ...  high_pre_close  low_pre_close         high  \\\n",
       "2017-09-03 08:00:00+08:00  ...        1.054251       0.996570  4714.759766   \n",
       "2017-09-03 20:00:00+08:00  ...        1.021810       0.958165  4583.839844   \n",
       "\n",
       "                                   low        open      amount  num_trades  \\\n",
       "2017-09-03 08:00:00+08:00  4456.799805  4508.50000  1541936.75      2991.0   \n",
       "2017-09-03 20:00:00+08:00  4298.330078  4475.22998  1579116.00      3031.0   \n",
       "\n",
       "                           bid_volume   bid_amount        close  \n",
       "2017-09-03 08:00:00+08:00  123.085823  565981.3750  4486.000000  \n",
       "2017-09-03 20:00:00+08:00  135.848709  603663.3125  4509.080078  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_return</th>\n",
       "      <th>Roll_price_sma</th>\n",
       "      <th>Roll_price_min</th>\n",
       "      <th>Roll_price_max</th>\n",
       "      <th>Roll_return_mom</th>\n",
       "      <th>Roll_return_std</th>\n",
       "      <th>Mark_return</th>\n",
       "      <th>d_amplitude</th>\n",
       "      <th>volume</th>\n",
       "      <th>RSI_7</th>\n",
       "      <th>...</th>\n",
       "      <th>high_pre_close</th>\n",
       "      <th>low_pre_close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>amount</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>bid_amount</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-02 20:00:00+08:00</th>\n",
       "      <td>-0.014016</td>\n",
       "      <td>23122.327637</td>\n",
       "      <td>22598.470703</td>\n",
       "      <td>23820.490234</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0</td>\n",
       "      <td>781.349609</td>\n",
       "      <td>231634.671875</td>\n",
       "      <td>62.829218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013607</td>\n",
       "      <td>0.980806</td>\n",
       "      <td>24144.619141</td>\n",
       "      <td>23363.269531</td>\n",
       "      <td>23820.490234</td>\n",
       "      <td>5.509089e+09</td>\n",
       "      <td>5610778.0</td>\n",
       "      <td>115050.062500</td>\n",
       "      <td>2.736556e+09</td>\n",
       "      <td>23488.939453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 08:00:00+08:00</th>\n",
       "      <td>0.001003</td>\n",
       "      <td>23152.069629</td>\n",
       "      <td>22598.470703</td>\n",
       "      <td>23820.490234</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>1</td>\n",
       "      <td>178.638672</td>\n",
       "      <td>59731.914062</td>\n",
       "      <td>63.110775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004307</td>\n",
       "      <td>0.996702</td>\n",
       "      <td>23590.109375</td>\n",
       "      <td>23411.470703</td>\n",
       "      <td>23489.330078</td>\n",
       "      <td>1.404998e+09</td>\n",
       "      <td>1594179.0</td>\n",
       "      <td>29920.123047</td>\n",
       "      <td>7.037849e+08</td>\n",
       "      <td>23512.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           log_return  Roll_price_sma  Roll_price_min  \\\n",
       "2023-02-02 20:00:00+08:00   -0.014016    23122.327637    22598.470703   \n",
       "2023-02-03 08:00:00+08:00    0.001003    23152.069629    22598.470703   \n",
       "\n",
       "                           Roll_price_max  Roll_return_mom  Roll_return_std  \\\n",
       "2023-02-02 20:00:00+08:00    23820.490234         0.001234         0.012837   \n",
       "2023-02-03 08:00:00+08:00    23820.490234         0.001281         0.012834   \n",
       "\n",
       "                           Mark_return  d_amplitude         volume      RSI_7  \\\n",
       "2023-02-02 20:00:00+08:00            0   781.349609  231634.671875  62.829218   \n",
       "2023-02-03 08:00:00+08:00            1   178.638672   59731.914062  63.110775   \n",
       "\n",
       "                           ...  high_pre_close  low_pre_close          high  \\\n",
       "2023-02-02 20:00:00+08:00  ...        1.013607       0.980806  24144.619141   \n",
       "2023-02-03 08:00:00+08:00  ...        1.004307       0.996702  23590.109375   \n",
       "\n",
       "                                    low          open        amount  \\\n",
       "2023-02-02 20:00:00+08:00  23363.269531  23820.490234  5.509089e+09   \n",
       "2023-02-03 08:00:00+08:00  23411.470703  23489.330078  1.404998e+09   \n",
       "\n",
       "                           num_trades     bid_volume    bid_amount  \\\n",
       "2023-02-02 20:00:00+08:00   5610778.0  115050.062500  2.736556e+09   \n",
       "2023-02-03 08:00:00+08:00   1594179.0   29920.123047  7.037849e+08   \n",
       "\n",
       "                                  close  \n",
       "2023-02-02 20:00:00+08:00  23488.939453  \n",
       "2023-02-03 08:00:00+08:00  23512.500000  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "data.shape=(3958, 24)\n",
      "最后两行数据:\n",
      "[[0.50319753 0.3227346  0.34132866 0.31698634 0.58086103 0.08894926\n",
      "  0.         0.06448195 0.48794464 0.61955107 0.56821127 0.07434574\n",
      "  0.55184607 0.66961518 0.04639618 0.93327198 0.31779926 0.32004431\n",
      "  0.32367538 0.60838036 0.6340853  0.48815362 0.60853245 0.31804606]\n",
      " [0.53291171 0.3232203  0.34132866 0.31698634 0.58162992 0.08892097\n",
      "  1.         0.01358153 0.12562817 0.62289124 0.58169162 0.01228501\n",
      "  0.56814218 0.67029337 0.01754548 0.98600569 0.30936578 0.32079513\n",
      "  0.31855889 0.15510849 0.18007989 0.1267754  0.15645922 0.31841037]]\n"
     ]
    }
   ],
   "source": [
    "BTC_data = BTC(URL, StartDate, EndDate, Folder_base, BTC_json,\n",
    "               binance_api_key=api_key, binance_api_secret=api_secret)\n",
    "data0 = BTC_data.MarketFactor_ClosePriceFeatures(by_BinanceAPI=True,\n",
    "    FromWeb=False,close_colName='close',lags=0,window=20,DayH=2, MarketFactor=True, weekdays=7,)\n",
    "# data0[\"year\"] = (\n",
    "#     data0[\"year\"] - 2016\n",
    "# )  # 因为数据从2016年开始,减去2016,将年份变成0以上的整数,以使得Embedding词汇表范围缩小\n",
    "\n",
    "\n",
    "def dataset_Scaler(data):\n",
    "    # 将原始数据MinMax归一化,以提高特征辨识度; 注意:要小心,重复归一化,可能会导致数据变小\n",
    "    # 将close_price单独归一化,因为后面需要将close_price预测出后,还原\n",
    "    X_Scaler = MinMaxScaler()\n",
    "    y_Scaler = MinMaxScaler()\n",
    "\n",
    "    X_ds = X_Scaler.fit_transform(\n",
    "        data[data.columns[:-1]]\n",
    "    )  # 本是DataFrame的类型,经过转换,X变成了ndarray类型;\n",
    "    y_ds = y_Scaler.fit_transform(\n",
    "        data[\"close\"].to_numpy().reshape(-1, 1)\n",
    "    )  # y为series,标准化需要(-1,1)的shape\n",
    "    data = np.c_[X_ds, y_ds]\n",
    "\n",
    "    return data, X_Scaler, y_Scaler\n",
    "\n",
    "\n",
    "data, X_Scaler, y_Scaler = dataset_Scaler(data0)\n",
    "print(type(data))\n",
    "print(\"data.shape={}\".format(data.shape))\n",
    "print(\"最后两行数据:\\n{}\".format(data[-2:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c3d1c2-948d-4cb0-bf61-ab8addca5a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data共有样本:3956;train/val分隔split:3161\n",
      "dataset_train元组长度:2\n",
      "dataset_train中xs.shape:(32, 208, 24)\n",
      "dataset_train中ys.shape:(32, 7, 1)\n",
      "--------------------------------------------------\n",
      "dataset_val元组长度:2\n",
      "dataset_val中xs.shape:(32, 208, 24)\n",
      "dataset_val中ys.shape:(32, 7, 1)\n",
      "--------------------------------------------------\n",
      "dataset_predict数据.shape:(32, 208, 24),从2021-11-03 20:00:00+08:00开始,共有22组*64个数据\n"
     ]
    }
   ],
   "source": [
    "# best hyperparameter-----\n",
    "Batch_size = 32\n",
    "kernel_size = 47\n",
    "L_seq = 208\n",
    "individual = (False,)\n",
    "L_pred = 7\n",
    "out_features = 1\n",
    "in_features = data.shape[1]\n",
    "\n",
    "\n",
    "def gen(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        yield data[i, :]\n",
    "\n",
    "\n",
    "def xs_ys_dataset(\n",
    "    data,\n",
    "    L_seq,\n",
    "    L_pred,\n",
    "    Batch_size,\n",
    "    out_features=1,\n",
    "    shuffle=False,\n",
    "    buffer_size=10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    生产xs与ys合并一起的dataset,样本总长度:L_seq+L_pred.\n",
    "    args:\n",
    "        data: Dataframe格式的数据(B,in_features);\n",
    "        L_seq: Linear模型输入的时序长度;\n",
    "        L_pred: Linear模型输出的预测时序长度;\n",
    "        out_features: target输出时的特征数,缺省=1,只取收盘价格\n",
    "        Batch_size:\n",
    "        shuffle: bool ; 是否shuffle\n",
    "    out:\n",
    "        (xs,ys): (input, target) ((B,L_seq,features),(B,L_pred,out_features))\n",
    "\n",
    "    \"\"\"\n",
    "    output_signature = tf.TensorSpec((in_features), tf.float64)  # 14列\n",
    "    # gen_f = gen(data)\n",
    "    x_y = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=output_signature,\n",
    "        args=(data,),\n",
    "    )  # args用于给gen传递参数,必须是元组的形式传入参数;\n",
    "    # x_y = tf.data.Dataset.from_tensor_slices(data)\n",
    "    x_y = x_y.window(L_seq + L_pred, shift=1, drop_remainder=True)\n",
    "    x_y = x_y.flat_map(lambda w: w.batch(L_seq + L_pred, drop_remainder=True))\n",
    "    xs = x_y.map(lambda w: w[:L_seq, :])  # 时序L_seq;in_features中,不包含时间戳部分;\n",
    "    ys = x_y.map(\n",
    "        lambda w: w[L_seq:, (out_features * -1):]\n",
    "    )  # 时序L_pred部分;in_features中,不包含时间戳部分;作为Target标签,这里只取最后的收盘价格,shape(B,L,1)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((xs, ys))\n",
    "    if shuffle == True:\n",
    "        dataset.shuffle(buffer_size)\n",
    "\n",
    "    return dataset.batch(Batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "\n",
    "def xs_dataset(data, L_seq, Batch_size, shuffle=False, buffer_size=10000,):\n",
    "    \"\"\"\n",
    "    仅生产xs的dataset,不包含标签;dataset时序长度:L_seq\n",
    "    args:\n",
    "        data: Dataframe格式的数据(B,in_features);\n",
    "        L_seq: Linear模型输入的时序长度;\n",
    "        Batch_size:\n",
    "        shuffle: bool ; 是否shuffle\n",
    "    out:\n",
    "        xs: input (B,L_seq,features)\n",
    "    \"\"\"\n",
    "    output_signature = tf.TensorSpec((in_features), tf.float64)  # 14列\n",
    "    # gen_f = gen(data)\n",
    "    xs = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=output_signature,\n",
    "        args=(data,),\n",
    "    )  # args用于给gen传递参数,必须是元组的形式传入参数;\n",
    "    # xs = tf.data.Dataset.from_tensor_slices(data)\n",
    "    xs = xs.window(L_seq, shift=1, drop_remainder=True)\n",
    "    xs = xs.flat_map(lambda w: w.batch(L_seq, drop_remainder=True))\n",
    "\n",
    "    if shuffle == True:\n",
    "        xs.shuffle(buffer_size)\n",
    "\n",
    "    return xs.batch(Batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "\n",
    "# 以2022-01-01:00为起点分隔train,valid\n",
    "# split = int(data.shape[0] * 0.8)\n",
    "split = np.argwhere(data0.index==pd.Timestamp('2022-01-01',tz='UTC'))[0,0]\n",
    "# split = 2 # 为调试使用,缩短运行时间\n",
    "print(\"data共有样本:{};train/val分隔split:{}\".format(data.shape[0], split))\n",
    "\n",
    "dataset_train = xs_ys_dataset(\n",
    "    data[:split, :],\n",
    "    L_seq,\n",
    "    L_pred,\n",
    "    Batch_size,\n",
    "    out_features,\n",
    "    shuffle=True,\n",
    "    buffer_size=10000,\n",
    ")\n",
    "print(\"dataset_train元组长度:{}\".format(len(iter(dataset_train).next())))\n",
    "print(\"dataset_train中xs.shape:{}\".format(iter(dataset_train).next()[0].shape))\n",
    "print(\"dataset_train中ys.shape:{}\".format(iter(dataset_train).next()[1].shape))\n",
    "print('{}'.format(50*'-'))\n",
    "\n",
    "dataset_val = xs_ys_dataset(\n",
    "    data[split:, :],\n",
    "    L_seq,\n",
    "    L_pred,\n",
    "    Batch_size,\n",
    "    shuffle=True,\n",
    "    buffer_size=10000,\n",
    ")\n",
    "\n",
    "dataset_all = xs_ys_dataset(data, L_seq, L_pred, Batch_size,\n",
    "                            shuffle=True, buffer_size=10000,)\n",
    "print(\"dataset_val元组长度:{}\".format(len(iter(dataset_val).next())))\n",
    "print(\"dataset_val中xs.shape:{}\".format(iter(dataset_val).next()[0].shape))\n",
    "print(\"dataset_val中ys.shape:{}\".format(iter(dataset_val).next()[1].shape))\n",
    "print('{}'.format(50*'-'))\n",
    "\n",
    "# 为了batch后数据成batch不丢弃,创造n个batch(n*64),即n个batch,每个batch=Batch_size;使得总样本数在1年附近;\n",
    "# 最后一个batch时间是倒数L_seq天;第一个batch是倒数:5*Batch_size+L_seq序列开始:\n",
    "Batch_num = 365*2//Batch_size  # 1年除Batch_size,获得batch的数量;\n",
    "r_split = Batch_num*Batch_size+L_seq\n",
    "dataset_predict = xs_dataset(data[-r_split:, :], L_seq,\n",
    "                             Batch_size, shuffle=True, buffer_size=10000,)\n",
    "print('dataset_predict数据.shape:{},从{}开始,共有{}组*64个数据'.format(\n",
    "    iter(dataset_predict).next().shape, data0.index[-r_split], len(list(iter(dataset_predict)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f9dd1-abcf-4d64-b51f-37544b5d1c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaa1e1b-2a1e-44e0-a2c8-1773ef95a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型:\n",
    "DLinear = LTSF_FF_DLinear(\n",
    "    seq_len=L_seq,\n",
    "    pred_len=L_pred,\n",
    "    channels=in_features,\n",
    "    kernel_size=kernel_size,\n",
    "    \n",
    ")\n",
    "\n",
    "# 1) 自定义loss,metrics:\n",
    "#  loss:\n",
    "loss_object = tf.keras.losses.MeanAbsoluteError(\n",
    "    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    ")  # 求和\n",
    "\n",
    "#  metric\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "train_MSE = tf.keras.metrics.MeanSquaredError(name=\"train_MSE\")\n",
    "val_MSE = tf.keras.metrics.MeanSquaredError(name=\"val_MSE\")\n",
    "\n",
    "# 2) 自定义learning_rate,从而自定义optimizer:\n",
    "# 自定义 learning_rate\n",
    "def dict_compare(\n",
    "    initial, lr_dict, step\n",
    "):  # 因为If等python语句不能在Tensorflow中转化成Graph,这里用py_function包装\n",
    "    \"\"\"\n",
    "    用于从lr列表中,提取与训练step相对应的lr\n",
    "    \"\"\"\n",
    "    lr = initial\n",
    "    index = tf.where(lr_dict[:, 0] <= step)  # 找到最接近step的index\n",
    "    if index.numpy().size != 0:\n",
    "        lr = lr_dict[:, 1].numpy()[max(index).numpy()]  # ndary shape:(1,)\n",
    "        lr = lr[0]\n",
    "    return lr\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):  # 定义Schedule类\n",
    "    def __init__(self, initial):  # 不能添加**kwargs;与自定义layer层不一样;\n",
    "        \"\"\"\n",
    "        args:\n",
    "            initial: learning_rate initial value, 5e-3 for example\n",
    "        \"\"\"\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.initial = initial\n",
    "        self.lr_dict = tf.convert_to_tensor(\n",
    "            (\n",
    "                (2, initial),\n",
    "                (4, initial / 5),\n",
    "                (6, initial / 10),\n",
    "                (8, initial / 50),\n",
    "                (10, initial / 100),\n",
    "                (15, initial / 500),\n",
    "                (20, initial / 1000),\n",
    "                #                 (80, initial / 5000),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __call__(self, step):\n",
    "        lr = tf.py_function(\n",
    "            dict_compare, inp=[self.initial, self.lr_dict, step], Tout=tf.float32\n",
    "        )  # py_function包装python函数\n",
    "        return lr\n",
    "\n",
    "\n",
    "lr = CustomSchedule(5e-03)  # build lr类;\n",
    "\n",
    "# for step in [2,4,6,8,10,15,20]:\n",
    "#     print(lr(step),end=',')\n",
    "\n",
    "# optimizer:\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# 3) 自定义Callbacks:\n",
    "\n",
    "\n",
    "# 非Keras方法,直接设立checkpoint;\n",
    "checkpoint_path = \"./checkpoint\"\n",
    "ckpt = tf.train.Checkpoint(DLinear=DLinear, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt,\n",
    "    checkpoint_path,\n",
    "    max_to_keep=1,\n",
    "    checkpoint_name=\"CusomModel_F20_221219\",\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.CallbackList(\n",
    "    callbacks=[], add_history=True, add_progbar=True, model=DLinear\n",
    ")  # callbacks=[]初始为空,即在训练时,出现进度条;其它callbacks使用append方法添加;\n",
    "# customcallback = CustomCallback()\n",
    "# callbacks.append(customcallback) #测试成功,没有实际意义,关闭;\n",
    "# callbacks.append(Modelcheckpoint) #keras与custom training在这点,兼容性有问题;建议采用tf.train.checkpoint方法;\n",
    "# callbacks.append(Earlystop) #keras与custom training兼容性不方便,建议直接在cutsom traning中代码实现;\n",
    "\n",
    "\n",
    "# 3) 自定义单步训练:\n",
    "@tf.function  # 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地执行。\n",
    "def train_step(dataset):  # dataset是xs与xs_timestamp合集\n",
    "    xs, ys = dataset[0], dataset[1]\n",
    "\n",
    "    # 求导,根据导数优化变量\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_ = DLinear(xs) #(batch,l_pred,1)\n",
    "        loss_value = loss_object(ys, y_)\n",
    "    gradients = tape.gradient(loss_value, DLinear.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, DLinear.trainable_variables))\n",
    "    # 计算metric: 每个单步经过batch次运算后,共batch次 loss的平均值\n",
    "    train_loss.update_state(loss_value)\n",
    "    train_MSE.update_state(ys, y_)\n",
    "    return {\"train_loss\": train_loss.result(), \"train_MSE\": train_MSE.result()}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(dataset_val):  # validation, test 使用该单步训练\n",
    "    xs, ys = dataset_val[0], dataset_val[1]\n",
    "    y_ = DLinear(xs) #输出:(Batch,L_pred,channels)\n",
    "    loss_value = loss_object(ys, y_)\n",
    "    val_loss.update_state(loss_value)\n",
    "    val_MSE.update_state(ys, y_)\n",
    "    return {\"val_loss\": val_loss.result(), \"val_MSE\": val_MSE.result()}\n",
    "\n",
    "\n",
    "def Modelevaluation(dataset_val, training=False):  # 训练后的模型Evaluation,并输出target预测值\n",
    "    val_loss.reset_states()\n",
    "    val_MSE.reset_states()\n",
    "    target_spec = dataset_val.element_spec[1]\n",
    "    y_hat = tf.zeros(target_spec.shape, target_spec.dtype)\n",
    "    for batch_data in dataset_val:\n",
    "        xs, ys = batch_data[0], batch_data[1]\n",
    "        y_ = DLinear(xs)\n",
    "        loss_value = loss_object(ys, y_)\n",
    "        val_loss.update_state(loss_value)\n",
    "        val_MSE.update_state(ys, y_)\n",
    "        # y_hat = tf.stack([y_hat,y_.numpy()])\n",
    "        y_hat = tf.concat([y_hat, y_.numpy()], axis=0)\n",
    "    print(\n",
    "        \"training={},val_loss:{:.4f},val_MSE:{:.4f}\".format(\n",
    "            training, val_loss.result().numpy(), val_MSE.result().numpy()\n",
    "        )\n",
    "    )\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ac5fbe-2820-4054-adda-b85857fbd877",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 7s 18ms/step - train_loss: 0.2432 - train_MSE: 0.1215 - val_loss: 0.2627 - val_MSE: 0.1093\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.2277 - train_MSE: 0.1118 - val_loss: 0.2627 - val_MSE: 0.1093\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.2226 - train_MSE: 0.1072 - val_loss: 0.2572 - val_MSE: 0.1044\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.2178 - train_MSE: 0.1032 - val_loss: 0.2526 - val_MSE: 0.1002\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.2131 - train_MSE: 0.0993 - val_loss: 0.2486 - val_MSE: 0.0966\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.2084 - train_MSE: 0.0957 - val_loss: 0.2448 - val_MSE: 0.0934\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.2037 - train_MSE: 0.0922 - val_loss: 0.2412 - val_MSE: 0.0904\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1991 - train_MSE: 0.0888 - val_loss: 0.2378 - val_MSE: 0.0875\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1945 - train_MSE: 0.0855 - val_loss: 0.2344 - val_MSE: 0.0849\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.1888 - train_MSE: 0.0812 - val_loss: 0.2311 - val_MSE: 0.0823\n",
      "epoch:10/1500 - 耗时:0.03分/总0.42分; train_loss 0.2977 train_MSE 0.1768; val_loss 0.2279 val_MSE 0.0798\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1901 - train_MSE: 0.0824 - val_loss: 0.2311 - val_MSE: 0.0823\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1857 - train_MSE: 0.0793 - val_loss: 0.2279 - val_MSE: 0.0798\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1815 - train_MSE: 0.0765 - val_loss: 0.2245 - val_MSE: 0.0773\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1774 - train_MSE: 0.0737 - val_loss: 0.2212 - val_MSE: 0.0750\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1735 - train_MSE: 0.0711 - val_loss: 0.2179 - val_MSE: 0.0726\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1696 - train_MSE: 0.0685 - val_loss: 0.2145 - val_MSE: 0.0703\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1660 - train_MSE: 0.0661 - val_loss: 0.2109 - val_MSE: 0.0680\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1625 - train_MSE: 0.0637 - val_loss: 0.2072 - val_MSE: 0.0656\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1591 - train_MSE: 0.0615 - val_loss: 0.2033 - val_MSE: 0.0632\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1558 - train_MSE: 0.0593 - val_loss: 0.1993 - val_MSE: 0.0609\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.1515 - train_MSE: 0.0563 - val_loss: 0.1950 - val_MSE: 0.0585\n",
      "epoch:20/1500 - 耗时:0.03分/总0.75分; train_loss 0.2468 train_MSE 0.1262; val_loss 0.1907 val_MSE 0.0561\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1526 - train_MSE: 0.0572 - val_loss: 0.1950 - val_MSE: 0.0585\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1496 - train_MSE: 0.0551 - val_loss: 0.1907 - val_MSE: 0.0561\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1466 - train_MSE: 0.0531 - val_loss: 0.1862 - val_MSE: 0.0537\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.1438 - train_MSE: 0.0511 - val_loss: 0.1815 - val_MSE: 0.0512\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1410 - train_MSE: 0.0492 - val_loss: 0.1766 - val_MSE: 0.0488\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1383 - train_MSE: 0.0474 - val_loss: 0.1716 - val_MSE: 0.0463\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1357 - train_MSE: 0.0455 - val_loss: 0.1667 - val_MSE: 0.0439\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1330 - train_MSE: 0.0437 - val_loss: 0.1617 - val_MSE: 0.0416\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1304 - train_MSE: 0.0419 - val_loss: 0.1569 - val_MSE: 0.0394\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1277 - train_MSE: 0.0401 - val_loss: 0.1522 - val_MSE: 0.0372\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.1242 - train_MSE: 0.0379 - val_loss: 0.1476 - val_MSE: 0.0352\n",
      "epoch:30/1500 - 耗时:0.03分/总1.09分; train_loss 0.1979 train_MSE 0.0828; val_loss 0.1432 val_MSE 0.0332\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1251 - train_MSE: 0.0384 - val_loss: 0.1476 - val_MSE: 0.0352\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1223 - train_MSE: 0.0367 - val_loss: 0.1432 - val_MSE: 0.0332\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1196 - train_MSE: 0.0351 - val_loss: 0.1389 - val_MSE: 0.0314\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1168 - train_MSE: 0.0334 - val_loss: 0.1348 - val_MSE: 0.0297\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1139 - train_MSE: 0.0318 - val_loss: 0.1309 - val_MSE: 0.0281\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1111 - train_MSE: 0.0303 - val_loss: 0.1272 - val_MSE: 0.0265\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1083 - train_MSE: 0.0288 - val_loss: 0.1236 - val_MSE: 0.0251\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.1055 - train_MSE: 0.0273 - val_loss: 0.1202 - val_MSE: 0.0237\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.1027 - train_MSE: 0.0258 - val_loss: 0.1170 - val_MSE: 0.0224\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0999 - train_MSE: 0.0245 - val_loss: 0.1139 - val_MSE: 0.0212\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0965 - train_MSE: 0.0228 - val_loss: 0.1108 - val_MSE: 0.0201\n",
      "epoch:40/1500 - 耗时:0.03分/总1.43分; train_loss 0.1497 train_MSE 0.0487; val_loss 0.1079 val_MSE 0.0190\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0971 - train_MSE: 0.0231 - val_loss: 0.1108 - val_MSE: 0.0201\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0945 - train_MSE: 0.0218 - val_loss: 0.1079 - val_MSE: 0.0190\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0918 - train_MSE: 0.0205 - val_loss: 0.1052 - val_MSE: 0.0180\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0892 - train_MSE: 0.0193 - val_loss: 0.1026 - val_MSE: 0.0171\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0866 - train_MSE: 0.0182 - val_loss: 0.1001 - val_MSE: 0.0162\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0840 - train_MSE: 0.0170 - val_loss: 0.0978 - val_MSE: 0.0154\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0817 - train_MSE: 0.0160 - val_loss: 0.0957 - val_MSE: 0.0147\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0794 - train_MSE: 0.0150 - val_loss: 0.0938 - val_MSE: 0.0140\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0773 - train_MSE: 0.0141 - val_loss: 0.0920 - val_MSE: 0.0134\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0752 - train_MSE: 0.0133 - val_loss: 0.0903 - val_MSE: 0.0129\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0729 - train_MSE: 0.0123 - val_loss: 0.0887 - val_MSE: 0.0123\n",
      "epoch:50/1500 - 耗时:0.03分/总1.76分; train_loss 0.1067 train_MSE 0.0249; val_loss 0.0873 val_MSE 0.0119\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0733 - train_MSE: 0.0125 - val_loss: 0.0887 - val_MSE: 0.0123\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0715 - train_MSE: 0.0117 - val_loss: 0.0873 - val_MSE: 0.0119\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0699 - train_MSE: 0.0111 - val_loss: 0.0858 - val_MSE: 0.0114\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0684 - train_MSE: 0.0105 - val_loss: 0.0844 - val_MSE: 0.0110\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0670 - train_MSE: 0.0099 - val_loss: 0.0831 - val_MSE: 0.0106\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0658 - train_MSE: 0.0095 - val_loss: 0.0819 - val_MSE: 0.0103\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0647 - train_MSE: 0.0090 - val_loss: 0.0807 - val_MSE: 0.0099\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0638 - train_MSE: 0.0087 - val_loss: 0.0795 - val_MSE: 0.0096\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0630 - train_MSE: 0.0083 - val_loss: 0.0783 - val_MSE: 0.0093\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0622 - train_MSE: 0.0080 - val_loss: 0.0773 - val_MSE: 0.0090\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0613 - train_MSE: 0.0077 - val_loss: 0.0762 - val_MSE: 0.0088\n",
      "epoch:60/1500 - 耗时:0.03分/总2.10分; train_loss 0.0823 train_MSE 0.0139; val_loss 0.0751 val_MSE 0.0085\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0616 - train_MSE: 0.0078 - val_loss: 0.0762 - val_MSE: 0.0088\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0610 - train_MSE: 0.0075 - val_loss: 0.0751 - val_MSE: 0.0085\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0605 - train_MSE: 0.0073 - val_loss: 0.0739 - val_MSE: 0.0082\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0600 - train_MSE: 0.0071 - val_loss: 0.0727 - val_MSE: 0.0079\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0597 - train_MSE: 0.0070 - val_loss: 0.0716 - val_MSE: 0.0077\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0593 - train_MSE: 0.0068 - val_loss: 0.0705 - val_MSE: 0.0074\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0590 - train_MSE: 0.0067 - val_loss: 0.0695 - val_MSE: 0.0072\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0587 - train_MSE: 0.0066 - val_loss: 0.0685 - val_MSE: 0.0070\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0585 - train_MSE: 0.0065 - val_loss: 0.0675 - val_MSE: 0.0068\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0583 - train_MSE: 0.0064 - val_loss: 0.0666 - val_MSE: 0.0066\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0579 - train_MSE: 0.0063 - val_loss: 0.0658 - val_MSE: 0.0065\n",
      "epoch:70/1500 - 耗时:0.03分/总2.44分; train_loss 0.0726 train_MSE 0.0101; val_loss 0.0648 val_MSE 0.0063\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0581 - train_MSE: 0.0063 - val_loss: 0.0658 - val_MSE: 0.0065\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0579 - train_MSE: 0.0062 - val_loss: 0.0648 - val_MSE: 0.0063\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0577 - train_MSE: 0.0062 - val_loss: 0.0638 - val_MSE: 0.0062\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0575 - train_MSE: 0.0061 - val_loss: 0.0629 - val_MSE: 0.0060\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0573 - train_MSE: 0.0060 - val_loss: 0.0619 - val_MSE: 0.0058\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0571 - train_MSE: 0.0060 - val_loss: 0.0610 - val_MSE: 0.0057\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0570 - train_MSE: 0.0059 - val_loss: 0.0600 - val_MSE: 0.0055\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0568 - train_MSE: 0.0059 - val_loss: 0.0591 - val_MSE: 0.0054\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0566 - train_MSE: 0.0058 - val_loss: 0.0583 - val_MSE: 0.0053\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0565 - train_MSE: 0.0058 - val_loss: 0.0575 - val_MSE: 0.0051\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0562 - train_MSE: 0.0057 - val_loss: 0.0567 - val_MSE: 0.0050\n",
      "epoch:80/1500 - 耗时:0.03分/总2.77分; train_loss 0.0675 train_MSE 0.0086; val_loss 0.0558 val_MSE 0.0049\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0563 - train_MSE: 0.0057 - val_loss: 0.0567 - val_MSE: 0.0050\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0562 - train_MSE: 0.0057 - val_loss: 0.0558 - val_MSE: 0.0049\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0560 - train_MSE: 0.0056 - val_loss: 0.0550 - val_MSE: 0.0048\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0559 - train_MSE: 0.0056 - val_loss: 0.0542 - val_MSE: 0.0047\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0557 - train_MSE: 0.0056 - val_loss: 0.0535 - val_MSE: 0.0045\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0555 - train_MSE: 0.0055 - val_loss: 0.0528 - val_MSE: 0.0044\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0554 - train_MSE: 0.0055 - val_loss: 0.0520 - val_MSE: 0.0043\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0552 - train_MSE: 0.0055 - val_loss: 0.0514 - val_MSE: 0.0043\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0551 - train_MSE: 0.0054 - val_loss: 0.0508 - val_MSE: 0.0042\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0549 - train_MSE: 0.0054 - val_loss: 0.0502 - val_MSE: 0.0041\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0547 - train_MSE: 0.0053 - val_loss: 0.0496 - val_MSE: 0.0040\n",
      "epoch:90/1500 - 耗时:0.03分/总3.10分; train_loss 0.0641 train_MSE 0.0078; val_loss 0.0490 val_MSE 0.0039\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0548 - train_MSE: 0.0054 - val_loss: 0.0496 - val_MSE: 0.0040\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0547 - train_MSE: 0.0053 - val_loss: 0.0490 - val_MSE: 0.0039\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0545 - train_MSE: 0.0053 - val_loss: 0.0485 - val_MSE: 0.0039\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0544 - train_MSE: 0.0053 - val_loss: 0.0480 - val_MSE: 0.0038\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0542 - train_MSE: 0.0052 - val_loss: 0.0475 - val_MSE: 0.0037\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0541 - train_MSE: 0.0052 - val_loss: 0.0470 - val_MSE: 0.0037\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0539 - train_MSE: 0.0052 - val_loss: 0.0466 - val_MSE: 0.0036\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0538 - train_MSE: 0.0052 - val_loss: 0.0461 - val_MSE: 0.0035\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0536 - train_MSE: 0.0051 - val_loss: 0.0457 - val_MSE: 0.0035\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0535 - train_MSE: 0.0051 - val_loss: 0.0452 - val_MSE: 0.0034\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0533 - train_MSE: 0.0051 - val_loss: 0.0448 - val_MSE: 0.0034\n",
      "epoch:100/1500 - 耗时:0.03分/总3.44分; train_loss 0.0615 train_MSE 0.0072; val_loss 0.0445 val_MSE 0.0033\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0534 - train_MSE: 0.0051 - val_loss: 0.0448 - val_MSE: 0.0034\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0532 - train_MSE: 0.0051 - val_loss: 0.0445 - val_MSE: 0.0033\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0531 - train_MSE: 0.0050 - val_loss: 0.0441 - val_MSE: 0.0033\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0530 - train_MSE: 0.0050 - val_loss: 0.0438 - val_MSE: 0.0032\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0528 - train_MSE: 0.0050 - val_loss: 0.0434 - val_MSE: 0.0032\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0527 - train_MSE: 0.0050 - val_loss: 0.0431 - val_MSE: 0.0031\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0525 - train_MSE: 0.0049 - val_loss: 0.0427 - val_MSE: 0.0031\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0524 - train_MSE: 0.0049 - val_loss: 0.0424 - val_MSE: 0.0030\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0523 - train_MSE: 0.0049 - val_loss: 0.0421 - val_MSE: 0.0030\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0521 - train_MSE: 0.0049 - val_loss: 0.0419 - val_MSE: 0.0030\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0519 - train_MSE: 0.0048 - val_loss: 0.0416 - val_MSE: 0.0029\n",
      "epoch:110/1500 - 耗时:0.03分/总3.77分; train_loss 0.0593 train_MSE 0.0067; val_loss 0.0413 val_MSE 0.0029\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0520 - train_MSE: 0.0048 - val_loss: 0.0416 - val_MSE: 0.0029\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0518 - train_MSE: 0.0048 - val_loss: 0.0413 - val_MSE: 0.0029\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0517 - train_MSE: 0.0048 - val_loss: 0.0411 - val_MSE: 0.0029\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0516 - train_MSE: 0.0048 - val_loss: 0.0408 - val_MSE: 0.0028\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0514 - train_MSE: 0.0047 - val_loss: 0.0406 - val_MSE: 0.0028\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0513 - train_MSE: 0.0047 - val_loss: 0.0404 - val_MSE: 0.0028\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0512 - train_MSE: 0.0047 - val_loss: 0.0402 - val_MSE: 0.0028\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0511 - train_MSE: 0.0047 - val_loss: 0.0400 - val_MSE: 0.0027\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0509 - train_MSE: 0.0047 - val_loss: 0.0398 - val_MSE: 0.0027\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0508 - train_MSE: 0.0046 - val_loss: 0.0396 - val_MSE: 0.0027\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0506 - train_MSE: 0.0046 - val_loss: 0.0394 - val_MSE: 0.0026\n",
      "epoch:120/1500 - 耗时:0.03分/总4.11分; train_loss 0.0573 train_MSE 0.0063; val_loss 0.0392 val_MSE 0.0026\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0507 - train_MSE: 0.0046 - val_loss: 0.0394 - val_MSE: 0.0026\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0505 - train_MSE: 0.0046 - val_loss: 0.0392 - val_MSE: 0.0026\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0504 - train_MSE: 0.0046 - val_loss: 0.0391 - val_MSE: 0.0026\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0503 - train_MSE: 0.0046 - val_loss: 0.0389 - val_MSE: 0.0026\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0501 - train_MSE: 0.0045 - val_loss: 0.0387 - val_MSE: 0.0026\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0500 - train_MSE: 0.0045 - val_loss: 0.0386 - val_MSE: 0.0025\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0499 - train_MSE: 0.0045 - val_loss: 0.0384 - val_MSE: 0.0025\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0497 - train_MSE: 0.0045 - val_loss: 0.0382 - val_MSE: 0.0025\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0496 - train_MSE: 0.0044 - val_loss: 0.0381 - val_MSE: 0.0025\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0495 - train_MSE: 0.0044 - val_loss: 0.0379 - val_MSE: 0.0024\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0492 - train_MSE: 0.0044 - val_loss: 0.0378 - val_MSE: 0.0024\n",
      "epoch:130/1500 - 耗时:0.03分/总4.44分; train_loss 0.0556 train_MSE 0.0060; val_loss 0.0377 val_MSE 0.0024\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0493 - train_MSE: 0.0044 - val_loss: 0.0378 - val_MSE: 0.0024\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0492 - train_MSE: 0.0044 - val_loss: 0.0377 - val_MSE: 0.0024\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0491 - train_MSE: 0.0044 - val_loss: 0.0376 - val_MSE: 0.0024\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0490 - train_MSE: 0.0044 - val_loss: 0.0374 - val_MSE: 0.0024\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0488 - train_MSE: 0.0043 - val_loss: 0.0373 - val_MSE: 0.0024\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0487 - train_MSE: 0.0043 - val_loss: 0.0371 - val_MSE: 0.0023\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0486 - train_MSE: 0.0043 - val_loss: 0.0370 - val_MSE: 0.0023\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0484 - train_MSE: 0.0043 - val_loss: 0.0369 - val_MSE: 0.0023\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0483 - train_MSE: 0.0043 - val_loss: 0.0368 - val_MSE: 0.0023\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0482 - train_MSE: 0.0042 - val_loss: 0.0367 - val_MSE: 0.0023\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0480 - train_MSE: 0.0042 - val_loss: 0.0366 - val_MSE: 0.0023\n",
      "epoch:140/1500 - 耗时:0.03分/总4.78分; train_loss 0.0540 train_MSE 0.0057; val_loss 0.0365 val_MSE 0.0022\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0481 - train_MSE: 0.0042 - val_loss: 0.0366 - val_MSE: 0.0023\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0479 - train_MSE: 0.0042 - val_loss: 0.0365 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0478 - train_MSE: 0.0042 - val_loss: 0.0363 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0477 - train_MSE: 0.0042 - val_loss: 0.0362 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0476 - train_MSE: 0.0042 - val_loss: 0.0362 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0474 - train_MSE: 0.0041 - val_loss: 0.0361 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0473 - train_MSE: 0.0041 - val_loss: 0.0359 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0471 - train_MSE: 0.0041 - val_loss: 0.0358 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0470 - train_MSE: 0.0041 - val_loss: 0.0358 - val_MSE: 0.0022\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0469 - train_MSE: 0.0041 - val_loss: 0.0357 - val_MSE: 0.0021\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0467 - train_MSE: 0.0040 - val_loss: 0.0356 - val_MSE: 0.0021\n",
      "epoch:150/1500 - 耗时:0.03分/总5.11分; train_loss 0.0525 train_MSE 0.0055; val_loss 0.0355 val_MSE 0.0021\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0468 - train_MSE: 0.0040 - val_loss: 0.0356 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0466 - train_MSE: 0.0040 - val_loss: 0.0355 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0465 - train_MSE: 0.0040 - val_loss: 0.0354 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0464 - train_MSE: 0.0040 - val_loss: 0.0354 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0463 - train_MSE: 0.0040 - val_loss: 0.0353 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0462 - train_MSE: 0.0040 - val_loss: 0.0352 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0461 - train_MSE: 0.0039 - val_loss: 0.0351 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0460 - train_MSE: 0.0039 - val_loss: 0.0351 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0459 - train_MSE: 0.0039 - val_loss: 0.0350 - val_MSE: 0.0021\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0457 - train_MSE: 0.0039 - val_loss: 0.0349 - val_MSE: 0.0020\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0456 - train_MSE: 0.0039 - val_loss: 0.0348 - val_MSE: 0.0020\n",
      "epoch:160/1500 - 耗时:0.03分/总5.45分; train_loss 0.0511 train_MSE 0.0052; val_loss 0.0348 val_MSE 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0456 - train_MSE: 0.0039 - val_loss: 0.0348 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0455 - train_MSE: 0.0039 - val_loss: 0.0348 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0454 - train_MSE: 0.0038 - val_loss: 0.0347 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0453 - train_MSE: 0.0038 - val_loss: 0.0346 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0451 - train_MSE: 0.0038 - val_loss: 0.0346 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0450 - train_MSE: 0.0038 - val_loss: 0.0345 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0449 - train_MSE: 0.0038 - val_loss: 0.0344 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0448 - train_MSE: 0.0038 - val_loss: 0.0344 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0447 - train_MSE: 0.0038 - val_loss: 0.0343 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0446 - train_MSE: 0.0037 - val_loss: 0.0342 - val_MSE: 0.0020\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0444 - train_MSE: 0.0037 - val_loss: 0.0342 - val_MSE: 0.0020\n",
      "epoch:170/1500 - 耗时:0.03分/总5.79分; train_loss 0.0498 train_MSE 0.0050; val_loss 0.0341 val_MSE 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0444 - train_MSE: 0.0037 - val_loss: 0.0342 - val_MSE: 0.0020\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0443 - train_MSE: 0.0037 - val_loss: 0.0341 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0442 - train_MSE: 0.0037 - val_loss: 0.0340 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0441 - train_MSE: 0.0037 - val_loss: 0.0340 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0440 - train_MSE: 0.0037 - val_loss: 0.0339 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0439 - train_MSE: 0.0036 - val_loss: 0.0339 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0438 - train_MSE: 0.0036 - val_loss: 0.0338 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0437 - train_MSE: 0.0036 - val_loss: 0.0337 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0436 - train_MSE: 0.0036 - val_loss: 0.0337 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0435 - train_MSE: 0.0036 - val_loss: 0.0336 - val_MSE: 0.0019\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0433 - train_MSE: 0.0036 - val_loss: 0.0336 - val_MSE: 0.0019\n",
      "epoch:180/1500 - 耗时:0.03分/总6.12分; train_loss 0.0485 train_MSE 0.0048; val_loss 0.0335 val_MSE 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0433 - train_MSE: 0.0036 - val_loss: 0.0336 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0432 - train_MSE: 0.0036 - val_loss: 0.0335 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0431 - train_MSE: 0.0035 - val_loss: 0.0335 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0430 - train_MSE: 0.0035 - val_loss: 0.0334 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0429 - train_MSE: 0.0035 - val_loss: 0.0333 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0428 - train_MSE: 0.0035 - val_loss: 0.0333 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0427 - train_MSE: 0.0035 - val_loss: 0.0333 - val_MSE: 0.0019\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0426 - train_MSE: 0.0035 - val_loss: 0.0332 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0425 - train_MSE: 0.0035 - val_loss: 0.0332 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0424 - train_MSE: 0.0034 - val_loss: 0.0331 - val_MSE: 0.0018\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0422 - train_MSE: 0.0034 - val_loss: 0.0331 - val_MSE: 0.0018\n",
      "epoch:190/1500 - 耗时:0.03分/总6.46分; train_loss 0.0473 train_MSE 0.0046; val_loss 0.0331 val_MSE 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0423 - train_MSE: 0.0034 - val_loss: 0.0331 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0422 - train_MSE: 0.0034 - val_loss: 0.0331 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0421 - train_MSE: 0.0034 - val_loss: 0.0330 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0420 - train_MSE: 0.0034 - val_loss: 0.0330 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0419 - train_MSE: 0.0034 - val_loss: 0.0329 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0418 - train_MSE: 0.0034 - val_loss: 0.0329 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0416 - train_MSE: 0.0033 - val_loss: 0.0328 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0415 - train_MSE: 0.0033 - val_loss: 0.0328 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0414 - train_MSE: 0.0033 - val_loss: 0.0327 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0413 - train_MSE: 0.0033 - val_loss: 0.0327 - val_MSE: 0.0018\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0412 - train_MSE: 0.0033 - val_loss: 0.0327 - val_MSE: 0.0018\n",
      "epoch:200/1500 - 耗时:0.03分/总6.79分; train_loss 0.0461 train_MSE 0.0044; val_loss 0.0326 val_MSE 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0412 - train_MSE: 0.0033 - val_loss: 0.0327 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0411 - train_MSE: 0.0033 - val_loss: 0.0326 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0410 - train_MSE: 0.0033 - val_loss: 0.0326 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0409 - train_MSE: 0.0032 - val_loss: 0.0325 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0408 - train_MSE: 0.0032 - val_loss: 0.0325 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0407 - train_MSE: 0.0032 - val_loss: 0.0325 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0406 - train_MSE: 0.0032 - val_loss: 0.0324 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0405 - train_MSE: 0.0032 - val_loss: 0.0324 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0403 - train_MSE: 0.0032 - val_loss: 0.0323 - val_MSE: 0.0018\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0402 - train_MSE: 0.0032 - val_loss: 0.0323 - val_MSE: 0.0018\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0401 - train_MSE: 0.0031 - val_loss: 0.0323 - val_MSE: 0.0017\n",
      "epoch:210/1500 - 耗时:0.03分/总7.13分; train_loss 0.0449 train_MSE 0.0042; val_loss 0.0322 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0401 - train_MSE: 0.0031 - val_loss: 0.0323 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0400 - train_MSE: 0.0031 - val_loss: 0.0322 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0399 - train_MSE: 0.0031 - val_loss: 0.0322 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0398 - train_MSE: 0.0031 - val_loss: 0.0322 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0397 - train_MSE: 0.0031 - val_loss: 0.0322 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0396 - train_MSE: 0.0031 - val_loss: 0.0321 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0395 - train_MSE: 0.0031 - val_loss: 0.0321 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0394 - train_MSE: 0.0031 - val_loss: 0.0321 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0393 - train_MSE: 0.0030 - val_loss: 0.0321 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0392 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0391 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "epoch:220/1500 - 耗时:0.03分/总7.46分; train_loss 0.0438 train_MSE 0.0041; val_loss 0.0320 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0391 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0390 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0389 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0388 - train_MSE: 0.0030 - val_loss: 0.0320 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0387 - train_MSE: 0.0030 - val_loss: 0.0319 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0386 - train_MSE: 0.0029 - val_loss: 0.0319 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0385 - train_MSE: 0.0029 - val_loss: 0.0319 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0384 - train_MSE: 0.0029 - val_loss: 0.0319 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0383 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 3s 27ms/step - train_loss: 0.0382 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0381 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "epoch:230/1500 - 耗时:0.03分/总7.81分; train_loss 0.0427 train_MSE 0.0039; val_loss 0.0318 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0381 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0380 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0379 - train_MSE: 0.0029 - val_loss: 0.0318 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0378 - train_MSE: 0.0029 - val_loss: 0.0317 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0377 - train_MSE: 0.0028 - val_loss: 0.0317 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0376 - train_MSE: 0.0028 - val_loss: 0.0317 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0375 - train_MSE: 0.0028 - val_loss: 0.0317 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0374 - train_MSE: 0.0028 - val_loss: 0.0317 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0373 - train_MSE: 0.0028 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0372 - train_MSE: 0.0028 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0371 - train_MSE: 0.0028 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "epoch:240/1500 - 耗时:0.03分/总8.15分; train_loss 0.0417 train_MSE 0.0037; val_loss 0.0316 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0372 - train_MSE: 0.0028 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0370 - train_MSE: 0.0028 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0370 - train_MSE: 0.0027 - val_loss: 0.0316 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0368 - train_MSE: 0.0027 - val_loss: 0.0315 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0367 - train_MSE: 0.0027 - val_loss: 0.0315 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0366 - train_MSE: 0.0027 - val_loss: 0.0315 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0366 - train_MSE: 0.0027 - val_loss: 0.0315 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0364 - train_MSE: 0.0027 - val_loss: 0.0315 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0363 - train_MSE: 0.0027 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0362 - train_MSE: 0.0027 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0361 - train_MSE: 0.0026 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "epoch:250/1500 - 耗时:0.03分/总8.48分; train_loss 0.0407 train_MSE 0.0036; val_loss 0.0314 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0361 - train_MSE: 0.0026 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0360 - train_MSE: 0.0026 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0359 - train_MSE: 0.0026 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0358 - train_MSE: 0.0026 - val_loss: 0.0314 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0357 - train_MSE: 0.0026 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0356 - train_MSE: 0.0026 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0355 - train_MSE: 0.0026 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0354 - train_MSE: 0.0026 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0353 - train_MSE: 0.0025 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0352 - train_MSE: 0.0025 - val_loss: 0.0313 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0351 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "epoch:260/1500 - 耗时:0.03分/总8.81分; train_loss 0.0397 train_MSE 0.0035; val_loss 0.0312 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0351 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0350 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0349 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0348 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0347 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0346 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0346 - train_MSE: 0.0025 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0345 - train_MSE: 0.0024 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0344 - train_MSE: 0.0024 - val_loss: 0.0312 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0343 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0342 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "epoch:270/1500 - 耗时:0.03分/总9.13分; train_loss 0.0388 train_MSE 0.0033; val_loss 0.0311 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0342 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0341 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0340 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0339 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0338 - train_MSE: 0.0024 - val_loss: 0.0311 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0337 - train_MSE: 0.0024 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0336 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0335 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0334 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0333 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0331 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "epoch:280/1500 - 耗时:0.03分/总9.45分; train_loss 0.0379 train_MSE 0.0032; val_loss 0.0310 val_MSE 0.0017\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0332 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0331 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0330 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0329 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0017\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0328 - train_MSE: 0.0023 - val_loss: 0.0310 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0326 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0325 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0324 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0323 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0322 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0321 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "epoch:290/1500 - 耗时:0.03分/总9.78分; train_loss 0.0370 train_MSE 0.0031; val_loss 0.0309 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0321 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0320 - train_MSE: 0.0022 - val_loss: 0.0309 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0319 - train_MSE: 0.0022 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0318 - train_MSE: 0.0022 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0317 - train_MSE: 0.0021 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0316 - train_MSE: 0.0021 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0315 - train_MSE: 0.0021 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0314 - train_MSE: 0.0021 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0313 - train_MSE: 0.0021 - val_loss: 0.0308 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0312 - train_MSE: 0.0021 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0310 - train_MSE: 0.0021 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "epoch:300/1500 - 耗时:0.03分/总10.11分; train_loss 0.0361 train_MSE 0.0030; val_loss 0.0307 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0311 - train_MSE: 0.0021 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0310 - train_MSE: 0.0021 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0309 - train_MSE: 0.0021 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0308 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0307 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0306 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0305 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0304 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0303 - train_MSE: 0.0020 - val_loss: 0.0307 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0302 - train_MSE: 0.0020 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0300 - train_MSE: 0.0020 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "epoch:310/1500 - 耗时:0.03分/总10.43分; train_loss 0.0353 train_MSE 0.0029; val_loss 0.0306 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0301 - train_MSE: 0.0020 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0300 - train_MSE: 0.0020 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0299 - train_MSE: 0.0020 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0298 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0297 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0296 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0295 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0294 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0293 - train_MSE: 0.0019 - val_loss: 0.0306 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0292 - train_MSE: 0.0019 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0290 - train_MSE: 0.0019 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "epoch:320/1500 - 耗时:0.03分/总10.75分; train_loss 0.0346 train_MSE 0.0028; val_loss 0.0305 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0291 - train_MSE: 0.0019 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0290 - train_MSE: 0.0019 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0289 - train_MSE: 0.0019 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0288 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0287 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0286 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0285 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0284 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0283 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0282 - train_MSE: 0.0018 - val_loss: 0.0305 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0280 - train_MSE: 0.0018 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "epoch:330/1500 - 耗时:0.03分/总11.07分; train_loss 0.0338 train_MSE 0.0027; val_loss 0.0304 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0281 - train_MSE: 0.0018 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0280 - train_MSE: 0.0018 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0279 - train_MSE: 0.0018 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0278 - train_MSE: 0.0018 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0277 - train_MSE: 0.0017 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0276 - train_MSE: 0.0017 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0276 - train_MSE: 0.0017 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0275 - train_MSE: 0.0017 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0274 - train_MSE: 0.0017 - val_loss: 0.0304 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0273 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0271 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "epoch:340/1500 - 耗时:0.03分/总11.40分; train_loss 0.0331 train_MSE 0.0026; val_loss 0.0303 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0272 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0271 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0270 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0269 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0268 - train_MSE: 0.0017 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0267 - train_MSE: 0.0016 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0266 - train_MSE: 0.0016 - val_loss: 0.0303 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0265 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0264 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0263 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0261 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "epoch:350/1500 - 耗时:0.03分/总11.73分; train_loss 0.0324 train_MSE 0.0025; val_loss 0.0302 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0262 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0262 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0261 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0260 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0259 - train_MSE: 0.0016 - val_loss: 0.0302 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0258 - train_MSE: 0.0016 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0257 - train_MSE: 0.0016 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0256 - train_MSE: 0.0016 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0256 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0255 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0253 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "epoch:360/1500 - 耗时:0.03分/总12.05分; train_loss 0.0317 train_MSE 0.0024; val_loss 0.0301 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0254 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0253 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0252 - train_MSE: 0.0015 - val_loss: 0.0301 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0251 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0250 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0250 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0249 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0248 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0247 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0246 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0245 - train_MSE: 0.0014 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "epoch:370/1500 - 耗时:0.03分/总12.37分; train_loss 0.0311 train_MSE 0.0024; val_loss 0.0300 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0246 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0245 - train_MSE: 0.0015 - val_loss: 0.0300 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0244 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0243 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0242 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0242 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0241 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0240 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0239 - train_MSE: 0.0014 - val_loss: 0.0299 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0239 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0237 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "epoch:380/1500 - 耗时:0.03分/总12.71分; train_loss 0.0305 train_MSE 0.0023; val_loss 0.0298 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0238 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0237 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0236 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0236 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0235 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0234 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0234 - train_MSE: 0.0014 - val_loss: 0.0298 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0233 - train_MSE: 0.0014 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0232 - train_MSE: 0.0013 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0231 - train_MSE: 0.0013 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0229 - train_MSE: 0.0013 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "epoch:390/1500 - 耗时:0.03分/总13.04分; train_loss 0.0299 train_MSE 0.0022; val_loss 0.0297 val_MSE 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0230 - train_MSE: 0.0013 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0230 - train_MSE: 0.0013 - val_loss: 0.0297 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0229 - train_MSE: 0.0013 - val_loss: 0.0296 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0228 - train_MSE: 0.0013 - val_loss: 0.0296 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0227 - train_MSE: 0.0013 - val_loss: 0.0296 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0226 - train_MSE: 0.0013 - val_loss: 0.0296 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0226 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0225 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0224 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0016\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0224 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0222 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "epoch:400/1500 - 耗时:0.03分/总13.37分; train_loss 0.0293 train_MSE 0.0022; val_loss 0.0295 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0223 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 3s 27ms/step - train_loss: 0.0222 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0222 - train_MSE: 0.0013 - val_loss: 0.0295 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0221 - train_MSE: 0.0013 - val_loss: 0.0294 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0220 - train_MSE: 0.0013 - val_loss: 0.0294 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0219 - train_MSE: 0.0012 - val_loss: 0.0294 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0219 - train_MSE: 0.0012 - val_loss: 0.0294 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0218 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0218 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0217 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0215 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "epoch:410/1500 - 耗时:0.03分/总13.72分; train_loss 0.0288 train_MSE 0.0021; val_loss 0.0293 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0216 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0216 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0215 - train_MSE: 0.0012 - val_loss: 0.0293 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0214 - train_MSE: 0.0012 - val_loss: 0.0292 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0214 - train_MSE: 0.0012 - val_loss: 0.0292 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0213 - train_MSE: 0.0012 - val_loss: 0.0292 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0213 - train_MSE: 0.0012 - val_loss: 0.0292 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0212 - train_MSE: 0.0012 - val_loss: 0.0292 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0211 - train_MSE: 0.0012 - val_loss: 0.0291 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0211 - train_MSE: 0.0012 - val_loss: 0.0291 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0209 - train_MSE: 0.0012 - val_loss: 0.0291 - val_MSE: 0.0015\n",
      "epoch:420/1500 - 耗时:0.03分/总14.05分; train_loss 0.0282 train_MSE 0.0021; val_loss 0.0291 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0210 - train_MSE: 0.0012 - val_loss: 0.0291 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0210 - train_MSE: 0.0012 - val_loss: 0.0291 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0209 - train_MSE: 0.0012 - val_loss: 0.0290 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0209 - train_MSE: 0.0012 - val_loss: 0.0290 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0208 - train_MSE: 0.0012 - val_loss: 0.0290 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0207 - train_MSE: 0.0012 - val_loss: 0.0290 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0207 - train_MSE: 0.0012 - val_loss: 0.0290 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0206 - train_MSE: 0.0011 - val_loss: 0.0289 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0206 - train_MSE: 0.0011 - val_loss: 0.0289 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0205 - train_MSE: 0.0011 - val_loss: 0.0289 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0204 - train_MSE: 0.0011 - val_loss: 0.0289 - val_MSE: 0.0015\n",
      "epoch:430/1500 - 耗时:0.03分/总14.39分; train_loss 0.0277 train_MSE 0.0020; val_loss 0.0288 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0205 - train_MSE: 0.0011 - val_loss: 0.0289 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0204 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0204 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0203 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0203 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0202 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0202 - train_MSE: 0.0011 - val_loss: 0.0288 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0201 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0200 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0200 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0199 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "epoch:440/1500 - 耗时:0.03分/总14.72分; train_loss 0.0273 train_MSE 0.0019; val_loss 0.0287 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0200 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0199 - train_MSE: 0.0011 - val_loss: 0.0287 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0199 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0198 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0198 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0197 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0197 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0196 - train_MSE: 0.0011 - val_loss: 0.0286 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0196 - train_MSE: 0.0011 - val_loss: 0.0285 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0195 - train_MSE: 0.0011 - val_loss: 0.0285 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0194 - train_MSE: 0.0010 - val_loss: 0.0285 - val_MSE: 0.0015\n",
      "epoch:450/1500 - 耗时:0.03分/总15.05分; train_loss 0.0268 train_MSE 0.0019; val_loss 0.0284 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0195 - train_MSE: 0.0011 - val_loss: 0.0285 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0194 - train_MSE: 0.0011 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0194 - train_MSE: 0.0011 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0194 - train_MSE: 0.0011 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0193 - train_MSE: 0.0010 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0193 - train_MSE: 0.0010 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0192 - train_MSE: 0.0010 - val_loss: 0.0284 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0192 - train_MSE: 0.0010 - val_loss: 0.0283 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0191 - train_MSE: 0.0010 - val_loss: 0.0283 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0191 - train_MSE: 0.0010 - val_loss: 0.0283 - val_MSE: 0.0015\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0189 - train_MSE: 0.0010 - val_loss: 0.0283 - val_MSE: 0.0015    \n",
      "epoch:460/1500 - 耗时:0.03分/总15.38分; train_loss 0.0264 train_MSE 0.0018; val_loss 0.0282 val_MSE 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0190 - train_MSE: 0.0010 - val_loss: 0.0283 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0190 - train_MSE: 0.0010 - val_loss: 0.0282 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0189 - train_MSE: 0.0010 - val_loss: 0.0282 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0189 - train_MSE: 0.0010 - val_loss: 0.0282 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0188 - train_MSE: 0.0010 - val_loss: 0.0282 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0188 - train_MSE: 0.0010 - val_loss: 0.0281 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0188 - train_MSE: 0.0010 - val_loss: 0.0281 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0187 - train_MSE: 0.0010 - val_loss: 0.0281 - val_MSE: 0.0015\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0187 - train_MSE: 9.9836e-04 - val_loss: 0.0281 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0186 - train_MSE: 9.9649e-04 - val_loss: 0.0281 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0185 - train_MSE: 9.8428e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "epoch:470/1500 - 耗时:0.03分/总15.71分; train_loss 0.0259 train_MSE 0.0018; val_loss 0.0280 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0186 - train_MSE: 9.9399e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0186 - train_MSE: 9.9091e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0185 - train_MSE: 9.8782e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0185 - train_MSE: 9.8374e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0184 - train_MSE: 9.8187e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0184 - train_MSE: 9.7968e-04 - val_loss: 0.0280 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0184 - train_MSE: 9.7593e-04 - val_loss: 0.0279 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0183 - train_MSE: 9.7274e-04 - val_loss: 0.0279 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0183 - train_MSE: 9.6919e-04 - val_loss: 0.0279 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0182 - train_MSE: 9.6607e-04 - val_loss: 0.0279 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0181 - train_MSE: 9.5350e-04 - val_loss: 0.0278 - val_MSE: 0.0014\n",
      "epoch:480/1500 - 耗时:0.03分/总16.04分; train_loss 0.0255 train_MSE 0.0018; val_loss 0.0278 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0182 - train_MSE: 9.6304e-04 - val_loss: 0.0278 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0181 - train_MSE: 9.6009e-04 - val_loss: 0.0278 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0181 - train_MSE: 9.5610e-04 - val_loss: 0.0277 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0180 - train_MSE: 9.5207e-04 - val_loss: 0.0277 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0180 - train_MSE: 9.4960e-04 - val_loss: 0.0277 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0180 - train_MSE: 9.4629e-04 - val_loss: 0.0277 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0179 - train_MSE: 9.4412e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0179 - train_MSE: 9.4188e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0179 - train_MSE: 9.3906e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0178 - train_MSE: 9.3685e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0177 - train_MSE: 9.2518e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "epoch:490/1500 - 耗时:0.03分/总16.36分; train_loss 0.0251 train_MSE 0.0017; val_loss 0.0276 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0178 - train_MSE: 9.3452e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0178 - train_MSE: 9.3200e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0178 - train_MSE: 9.2934e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0177 - train_MSE: 9.2658e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0177 - train_MSE: 9.2442e-04 - val_loss: 0.0276 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0176 - train_MSE: 9.2079e-04 - val_loss: 0.0275 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0176 - train_MSE: 9.1750e-04 - val_loss: 0.0274 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0175 - train_MSE: 9.1334e-04 - val_loss: 0.0274 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0175 - train_MSE: 9.1021e-04 - val_loss: 0.0274 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0174 - train_MSE: 9.0739e-04 - val_loss: 0.0273 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0173 - train_MSE: 8.9476e-04 - val_loss: 0.0273 - val_MSE: 0.0014\n",
      "epoch:500/1500 - 耗时:0.03分/总16.69分; train_loss 0.0247 train_MSE 0.0017; val_loss 0.0273 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0174 - train_MSE: 9.0396e-04 - val_loss: 0.0273 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0173 - train_MSE: 9.0105e-04 - val_loss: 0.0273 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0173 - train_MSE: 8.9822e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0173 - train_MSE: 8.9556e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0172 - train_MSE: 8.9366e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0172 - train_MSE: 8.9036e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0172 - train_MSE: 8.8796e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0171 - train_MSE: 8.8619e-04 - val_loss: 0.0272 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0171 - train_MSE: 8.8462e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0171 - train_MSE: 8.8222e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0170 - train_MSE: 8.7358e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "epoch:510/1500 - 耗时:0.03分/总17.02分; train_loss 0.0243 train_MSE 0.0016; val_loss 0.0271 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0171 - train_MSE: 8.7956e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0170 - train_MSE: 8.7775e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0170 - train_MSE: 8.7505e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0170 - train_MSE: 8.7225e-04 - val_loss: 0.0271 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0169 - train_MSE: 8.7135e-04 - val_loss: 0.0270 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0169 - train_MSE: 8.6806e-04 - val_loss: 0.0270 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0169 - train_MSE: 8.6494e-04 - val_loss: 0.0270 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0168 - train_MSE: 8.6297e-04 - val_loss: 0.0270 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0168 - train_MSE: 8.5993e-04 - val_loss: 0.0269 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0167 - train_MSE: 8.5744e-04 - val_loss: 0.0269 - val_MSE: 0.0014\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0166 - train_MSE: 8.4423e-04 - val_loss: 0.0268 - val_MSE: 0.0014\n",
      "epoch:520/1500 - 耗时:0.03分/总17.35分; train_loss 0.0239 train_MSE 0.0016; val_loss 0.0268 val_MSE 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0167 - train_MSE: 8.5311e-04 - val_loss: 0.0268 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0166 - train_MSE: 8.4994e-04 - val_loss: 0.0268 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0166 - train_MSE: 8.4748e-04 - val_loss: 0.0268 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0166 - train_MSE: 8.4571e-04 - val_loss: 0.0267 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0165 - train_MSE: 8.4337e-04 - val_loss: 0.0267 - val_MSE: 0.0014\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0165 - train_MSE: 8.4004e-04 - val_loss: 0.0267 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0165 - train_MSE: 8.3748e-04 - val_loss: 0.0267 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0164 - train_MSE: 8.3619e-04 - val_loss: 0.0267 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0164 - train_MSE: 8.3329e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0164 - train_MSE: 8.3091e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0163 - train_MSE: 8.2541e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "epoch:530/1500 - 耗时:0.03分/总17.67分; train_loss 0.0236 train_MSE 0.0015; val_loss 0.0266 val_MSE 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0164 - train_MSE: 8.3117e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0164 - train_MSE: 8.2962e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0163 - train_MSE: 8.2650e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0163 - train_MSE: 8.2447e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0163 - train_MSE: 8.2165e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0162 - train_MSE: 8.2001e-04 - val_loss: 0.0266 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0162 - train_MSE: 8.1845e-04 - val_loss: 0.0265 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0162 - train_MSE: 8.1642e-04 - val_loss: 0.0265 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0161 - train_MSE: 8.1294e-04 - val_loss: 0.0264 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0161 - train_MSE: 8.0973e-04 - val_loss: 0.0264 - val_MSE: 0.0013\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0159 - train_MSE: 7.9831e-04 - val_loss: 0.0263 - val_MSE: 0.0013\n",
      "epoch:540/1500 - 耗时:0.04分/总18.00分; train_loss 0.0232 train_MSE 0.0015; val_loss 0.0263 val_MSE 0.0013\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0160 - train_MSE: 8.0683e-04 - val_loss: 0.0263 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0160 - train_MSE: 8.0428e-04 - val_loss: 0.0263 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0159 - train_MSE: 8.0056e-04 - val_loss: 0.0262 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0159 - train_MSE: 7.9813e-04 - val_loss: 0.0262 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0159 - train_MSE: 7.9570e-04 - val_loss: 0.0262 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0158 - train_MSE: 7.9376e-04 - val_loss: 0.0261 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0158 - train_MSE: 7.9171e-04 - val_loss: 0.0261 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0158 - train_MSE: 7.8868e-04 - val_loss: 0.0261 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0157 - train_MSE: 7.8539e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0157 - train_MSE: 7.8243e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0155 - train_MSE: 7.6892e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "epoch:550/1500 - 耗时:0.03分/总18.33分; train_loss 0.0228 train_MSE 0.0015; val_loss 0.0260 val_MSE 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0156 - train_MSE: 7.8015e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0156 - train_MSE: 7.7894e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0156 - train_MSE: 7.7766e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0156 - train_MSE: 7.7608e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0156 - train_MSE: 7.7538e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0156 - train_MSE: 7.7282e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0156 - train_MSE: 7.7178e-04 - val_loss: 0.0260 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0156 - train_MSE: 7.7000e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0155 - train_MSE: 7.6791e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0155 - train_MSE: 7.6675e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0154 - train_MSE: 7.5954e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "epoch:560/1500 - 耗时:0.03分/总18.65分; train_loss 0.0226 train_MSE 0.0014; val_loss 0.0259 val_MSE 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0155 - train_MSE: 7.6497e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0155 - train_MSE: 7.6333e-04 - val_loss: 0.0259 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0154 - train_MSE: 7.6056e-04 - val_loss: 0.0258 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0154 - train_MSE: 7.5820e-04 - val_loss: 0.0258 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0153 - train_MSE: 7.5536e-04 - val_loss: 0.0257 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0153 - train_MSE: 7.5221e-04 - val_loss: 0.0256 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0152 - train_MSE: 7.4907e-04 - val_loss: 0.0256 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0152 - train_MSE: 7.4609e-04 - val_loss: 0.0255 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0151 - train_MSE: 7.4367e-04 - val_loss: 0.0255 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0151 - train_MSE: 7.4127e-04 - val_loss: 0.0254 - val_MSE: 0.0013\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0150 - train_MSE: 7.3062e-04 - val_loss: 0.0254 - val_MSE: 0.0013\n",
      "epoch:570/1500 - 耗时:0.03分/总18.99分; train_loss 0.0222 train_MSE 0.0014; val_loss 0.0254 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0150 - train_MSE: 7.3870e-04 - val_loss: 0.0254 - val_MSE: 0.0013\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0150 - train_MSE: 7.3671e-04 - val_loss: 0.0254 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0150 - train_MSE: 7.3460e-04 - val_loss: 0.0253 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0150 - train_MSE: 7.3270e-04 - val_loss: 0.0253 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0149 - train_MSE: 7.3078e-04 - val_loss: 0.0253 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0149 - train_MSE: 7.2934e-04 - val_loss: 0.0253 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0149 - train_MSE: 7.2763e-04 - val_loss: 0.0253 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0149 - train_MSE: 7.2565e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0148 - train_MSE: 7.2371e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0148 - train_MSE: 7.2170e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0147 - train_MSE: 7.1261e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "epoch:580/1500 - 耗时:0.03分/总19.32分; train_loss 0.0219 train_MSE 0.0014; val_loss 0.0252 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0148 - train_MSE: 7.2052e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0148 - train_MSE: 7.1868e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0148 - train_MSE: 7.1744e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0147 - train_MSE: 7.1421e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0147 - train_MSE: 7.1239e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0147 - train_MSE: 7.1178e-04 - val_loss: 0.0252 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0147 - train_MSE: 7.1131e-04 - val_loss: 0.0251 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0147 - train_MSE: 7.0951e-04 - val_loss: 0.0251 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0147 - train_MSE: 7.0875e-04 - val_loss: 0.0251 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0147 - train_MSE: 7.0704e-04 - val_loss: 0.0250 - val_MSE: 0.0012\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0145 - train_MSE: 6.9678e-04 - val_loss: 0.0250 - val_MSE: 0.0012\n",
      "epoch:590/1500 - 耗时:0.03分/总19.64分; train_loss 0.0216 train_MSE 0.0013; val_loss 0.0249 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0146 - train_MSE: 7.0452e-04 - val_loss: 0.0250 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0146 - train_MSE: 7.0192e-04 - val_loss: 0.0249 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0145 - train_MSE: 6.9877e-04 - val_loss: 0.0248 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0145 - train_MSE: 6.9571e-04 - val_loss: 0.0248 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0144 - train_MSE: 6.9238e-04 - val_loss: 0.0247 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0144 - train_MSE: 6.8967e-04 - val_loss: 0.0247 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0143 - train_MSE: 6.8665e-04 - val_loss: 0.0246 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0143 - train_MSE: 6.8457e-04 - val_loss: 0.0246 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0142 - train_MSE: 6.8299e-04 - val_loss: 0.0246 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0142 - train_MSE: 6.8096e-04 - val_loss: 0.0246 - val_MSE: 0.0012\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0141 - train_MSE: 6.7199e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "epoch:600/1500 - 耗时:0.03分/总19.98分; train_loss 0.0212 train_MSE 0.0013; val_loss 0.0245 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0142 - train_MSE: 6.7962e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0142 - train_MSE: 6.7822e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0142 - train_MSE: 6.7695e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0142 - train_MSE: 6.7602e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0141 - train_MSE: 6.7398e-04 - val_loss: 0.0245 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0141 - train_MSE: 6.7160e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0141 - train_MSE: 6.6895e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0140 - train_MSE: 6.6648e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0140 - train_MSE: 6.6599e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0140 - train_MSE: 6.6654e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0140 - train_MSE: 6.5800e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "epoch:610/1500 - 耗时:0.03分/总20.31分; train_loss 0.0209 train_MSE 0.0013; val_loss 0.0243 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0141 - train_MSE: 6.6546e-04 - val_loss: 0.0244 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0140 - train_MSE: 6.6389e-04 - val_loss: 0.0243 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0140 - train_MSE: 6.6192e-04 - val_loss: 0.0243 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0140 - train_MSE: 6.5882e-04 - val_loss: 0.0243 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0139 - train_MSE: 6.5606e-04 - val_loss: 0.0242 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0139 - train_MSE: 6.5545e-04 - val_loss: 0.0242 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0139 - train_MSE: 6.5426e-04 - val_loss: 0.0242 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0139 - train_MSE: 6.5325e-04 - val_loss: 0.0242 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0139 - train_MSE: 6.5186e-04 - val_loss: 0.0241 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0138 - train_MSE: 6.4927e-04 - val_loss: 0.0241 - val_MSE: 0.0012\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0138 - train_MSE: 6.4473e-04 - val_loss: 0.0241 - val_MSE: 0.0012\n",
      "epoch:620/1500 - 耗时:0.04分/总20.65分; train_loss 0.0206 train_MSE 0.0013; val_loss 0.0240 val_MSE 0.0012\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0138 - train_MSE: 6.4716e-04 - val_loss: 0.0241 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0138 - train_MSE: 6.4626e-04 - val_loss: 0.0240 - val_MSE: 0.0012\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0137 - train_MSE: 6.4392e-04 - val_loss: 0.0240 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0137 - train_MSE: 6.4180e-04 - val_loss: 0.0239 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0137 - train_MSE: 6.3934e-04 - val_loss: 0.0238 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0136 - train_MSE: 6.3623e-04 - val_loss: 0.0238 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0135 - train_MSE: 6.3347e-04 - val_loss: 0.0238 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0135 - train_MSE: 6.3071e-04 - val_loss: 0.0237 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0135 - train_MSE: 6.2899e-04 - val_loss: 0.0237 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0134 - train_MSE: 6.2707e-04 - val_loss: 0.0237 - val_MSE: 0.0011\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0132 - train_MSE: 6.1122e-04 - val_loss: 0.0236 - val_MSE: 0.0011\n",
      "epoch:630/1500 - 耗时:0.03分/总20.98分; train_loss 0.0203 train_MSE 0.0012; val_loss 0.0236 val_MSE 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0134 - train_MSE: 6.2589e-04 - val_loss: 0.0236 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0134 - train_MSE: 6.2460e-04 - val_loss: 0.0236 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0134 - train_MSE: 6.2298e-04 - val_loss: 0.0236 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0134 - train_MSE: 6.2176e-04 - val_loss: 0.0236 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0133 - train_MSE: 6.1954e-04 - val_loss: 0.0235 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0133 - train_MSE: 6.1638e-04 - val_loss: 0.0235 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0133 - train_MSE: 6.1340e-04 - val_loss: 0.0235 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0132 - train_MSE: 6.1221e-04 - val_loss: 0.0235 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0132 - train_MSE: 6.1173e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0132 - train_MSE: 6.1213e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0132 - train_MSE: 6.0588e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "epoch:640/1500 - 耗时:0.03分/总21.31分; train_loss 0.0201 train_MSE 0.0012; val_loss 0.0234 val_MSE 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0133 - train_MSE: 6.1295e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0133 - train_MSE: 6.1172e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0132 - train_MSE: 6.0887e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0132 - train_MSE: 6.0603e-04 - val_loss: 0.0234 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 6.0265e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0131 - train_MSE: 6.0003e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 6.0005e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 5.9981e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 6.0060e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0132 - train_MSE: 6.0061e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0131 - train_MSE: 5.9289e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "epoch:650/1500 - 耗时:0.03分/总21.64分; train_loss 0.0199 train_MSE 0.0012; val_loss 0.0232 val_MSE 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0132 - train_MSE: 5.9981e-04 - val_loss: 0.0233 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0132 - train_MSE: 5.9806e-04 - val_loss: 0.0232 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 5.9680e-04 - val_loss: 0.0232 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0131 - train_MSE: 5.9466e-04 - val_loss: 0.0231 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0131 - train_MSE: 5.9276e-04 - val_loss: 0.0231 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0130 - train_MSE: 5.9109e-04 - val_loss: 0.0230 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0130 - train_MSE: 5.8831e-04 - val_loss: 0.0229 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0129 - train_MSE: 5.8611e-04 - val_loss: 0.0229 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0129 - train_MSE: 5.8384e-04 - val_loss: 0.0228 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0128 - train_MSE: 5.8124e-04 - val_loss: 0.0228 - val_MSE: 0.0011\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0127 - train_MSE: 5.7096e-04 - val_loss: 0.0227 - val_MSE: 0.0011\n",
      "epoch:660/1500 - 耗时:0.03分/总21.98分; train_loss 0.0195 train_MSE 0.0011; val_loss 0.0227 val_MSE 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0128 - train_MSE: 5.7781e-04 - val_loss: 0.0227 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0127 - train_MSE: 5.7522e-04 - val_loss: 0.0227 - val_MSE: 0.0011\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0127 - train_MSE: 5.7318e-04 - val_loss: 0.0226 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0126 - train_MSE: 5.7138e-04 - val_loss: 0.0226 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0126 - train_MSE: 5.6950e-04 - val_loss: 0.0226 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0126 - train_MSE: 5.6779e-04 - val_loss: 0.0225 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0125 - train_MSE: 5.6701e-04 - val_loss: 0.0225 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0126 - train_MSE: 5.6685e-04 - val_loss: 0.0225 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0126 - train_MSE: 5.6537e-04 - val_loss: 0.0225 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0125 - train_MSE: 5.6276e-04 - val_loss: 0.0225 - val_MSE: 0.0010\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0124 - train_MSE: 5.5208e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "epoch:670/1500 - 耗时:0.03分/总22.31分; train_loss 0.0192 train_MSE 0.0011; val_loss 0.0224 val_MSE 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0125 - train_MSE: 5.5882e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0124 - train_MSE: 5.5609e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0124 - train_MSE: 5.5541e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0124 - train_MSE: 5.5486e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0124 - train_MSE: 5.5555e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0125 - train_MSE: 5.5668e-04 - val_loss: 0.0224 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0125 - train_MSE: 5.5570e-04 - val_loss: 0.0223 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0125 - train_MSE: 5.5338e-04 - val_loss: 0.0223 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0124 - train_MSE: 5.5049e-04 - val_loss: 0.0223 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0123 - train_MSE: 5.4693e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0122 - train_MSE: 5.3827e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "epoch:680/1500 - 耗时:0.03分/总22.65分; train_loss 0.0189 train_MSE 0.0011; val_loss 0.0222 val_MSE 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0123 - train_MSE: 5.4483e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0123 - train_MSE: 5.4404e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0123 - train_MSE: 5.4370e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0123 - train_MSE: 5.4354e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0123 - train_MSE: 5.4300e-04 - val_loss: 0.0222 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0123 - train_MSE: 5.4211e-04 - val_loss: 0.0221 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0123 - train_MSE: 5.4063e-04 - val_loss: 0.0221 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0122 - train_MSE: 5.3877e-04 - val_loss: 0.0221 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0122 - train_MSE: 5.3758e-04 - val_loss: 0.0220 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0122 - train_MSE: 5.3687e-04 - val_loss: 0.0220 - val_MSE: 0.0010\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0121 - train_MSE: 5.2963e-04 - val_loss: 0.0219 - val_MSE: 0.0010\n",
      "epoch:690/1500 - 耗时:0.03分/总22.99分; train_loss 0.0187 train_MSE 0.0011; val_loss 0.0219 val_MSE 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0122 - train_MSE: 5.3607e-04 - val_loss: 0.0219 - val_MSE: 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0122 - train_MSE: 5.3489e-04 - val_loss: 0.0219 - val_MSE: 9.9552e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0122 - train_MSE: 5.3237e-04 - val_loss: 0.0218 - val_MSE: 9.9083e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0121 - train_MSE: 5.3068e-04 - val_loss: 0.0217 - val_MSE: 9.8654e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0120 - train_MSE: 5.2680e-04 - val_loss: 0.0217 - val_MSE: 9.8310e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0120 - train_MSE: 5.2449e-04 - val_loss: 0.0217 - val_MSE: 9.7952e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0119 - train_MSE: 5.2263e-04 - val_loss: 0.0216 - val_MSE: 9.7524e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0119 - train_MSE: 5.2076e-04 - val_loss: 0.0216 - val_MSE: 9.7247e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0118 - train_MSE: 5.1882e-04 - val_loss: 0.0215 - val_MSE: 9.6750e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0118 - train_MSE: 5.1836e-04 - val_loss: 0.0215 - val_MSE: 9.6644e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0117 - train_MSE: 5.1038e-04 - val_loss: 0.0215 - val_MSE: 9.6217e-04\n",
      "epoch:700/1500 - 耗时:0.03分/总23.32分; train_loss 0.0183 train_MSE 0.0010; val_loss 0.0214 val_MSE 0.0010\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0118 - train_MSE: 5.1672e-04 - val_loss: 0.0215 - val_MSE: 9.6215e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0118 - train_MSE: 5.1416e-04 - val_loss: 0.0214 - val_MSE: 9.5803e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0118 - train_MSE: 5.1228e-04 - val_loss: 0.0214 - val_MSE: 9.5555e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0117 - train_MSE: 5.0873e-04 - val_loss: 0.0214 - val_MSE: 9.5498e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0117 - train_MSE: 5.0712e-04 - val_loss: 0.0214 - val_MSE: 9.5420e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0116 - train_MSE: 5.0607e-04 - val_loss: 0.0213 - val_MSE: 9.5309e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0116 - train_MSE: 5.0497e-04 - val_loss: 0.0213 - val_MSE: 9.5151e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0116 - train_MSE: 5.0449e-04 - val_loss: 0.0213 - val_MSE: 9.4947e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0116 - train_MSE: 5.0335e-04 - val_loss: 0.0213 - val_MSE: 9.4989e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0116 - train_MSE: 5.0369e-04 - val_loss: 0.0213 - val_MSE: 9.4693e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0116 - train_MSE: 4.9671e-04 - val_loss: 0.0212 - val_MSE: 9.4499e-04\n",
      "epoch:710/1500 - 耗时:0.03分/总23.66分; train_loss 0.0181 train_MSE 0.0010; val_loss 0.0212 val_MSE 0.0009\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0116 - train_MSE: 5.0293e-04 - val_loss: 0.0212 - val_MSE: 9.4499e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0116 - train_MSE: 5.0113e-04 - val_loss: 0.0212 - val_MSE: 9.4357e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0116 - train_MSE: 4.9776e-04 - val_loss: 0.0212 - val_MSE: 9.4249e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0115 - train_MSE: 4.9426e-04 - val_loss: 0.0212 - val_MSE: 9.4332e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0115 - train_MSE: 4.9253e-04 - val_loss: 0.0212 - val_MSE: 9.4282e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.9185e-04 - val_loss: 0.0212 - val_MSE: 9.4284e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.9224e-04 - val_loss: 0.0212 - val_MSE: 9.4247e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.9218e-04 - val_loss: 0.0211 - val_MSE: 9.4008e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0115 - train_MSE: 4.9206e-04 - val_loss: 0.0211 - val_MSE: 9.3880e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.9102e-04 - val_loss: 0.0211 - val_MSE: 9.3881e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0114 - train_MSE: 4.8378e-04 - val_loss: 0.0211 - val_MSE: 9.3723e-04\n",
      "epoch:720/1500 - 耗时:0.03分/总23.98分; train_loss 0.0179 train_MSE 0.0010; val_loss 0.0211 val_MSE 0.0009\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.8984e-04 - val_loss: 0.0211 - val_MSE: 9.3723e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.8805e-04 - val_loss: 0.0211 - val_MSE: 9.3592e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0114 - train_MSE: 4.8568e-04 - val_loss: 0.0210 - val_MSE: 9.3406e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0114 - train_MSE: 4.8382e-04 - val_loss: 0.0210 - val_MSE: 9.3468e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0114 - train_MSE: 4.8250e-04 - val_loss: 0.0210 - val_MSE: 9.3429e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0113 - train_MSE: 4.8169e-04 - val_loss: 0.0210 - val_MSE: 9.3358e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0113 - train_MSE: 4.8140e-04 - val_loss: 0.0210 - val_MSE: 9.3182e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0114 - train_MSE: 4.8277e-04 - val_loss: 0.0209 - val_MSE: 9.3078e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0114 - train_MSE: 4.8278e-04 - val_loss: 0.0209 - val_MSE: 9.2754e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.8109e-04 - val_loss: 0.0208 - val_MSE: 9.2339e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0114 - train_MSE: 4.7380e-04 - val_loss: 0.0208 - val_MSE: 9.1971e-04\n",
      "epoch:730/1500 - 耗时:0.03分/总24.31分; train_loss 0.0177 train_MSE 0.0010; val_loss 0.0208 val_MSE 0.0009\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0115 - train_MSE: 4.7973e-04 - val_loss: 0.0208 - val_MSE: 9.1970e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0114 - train_MSE: 4.7809e-04 - val_loss: 0.0208 - val_MSE: 9.1786e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0114 - train_MSE: 4.7716e-04 - val_loss: 0.0207 - val_MSE: 9.1386e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0113 - train_MSE: 4.7625e-04 - val_loss: 0.0207 - val_MSE: 9.1006e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0113 - train_MSE: 4.7554e-04 - val_loss: 0.0206 - val_MSE: 9.0503e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0113 - train_MSE: 4.7353e-04 - val_loss: 0.0205 - val_MSE: 8.9901e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0112 - train_MSE: 4.7092e-04 - val_loss: 0.0204 - val_MSE: 8.9278e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0112 - train_MSE: 4.6816e-04 - val_loss: 0.0204 - val_MSE: 8.8913e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0111 - train_MSE: 4.6598e-04 - val_loss: 0.0204 - val_MSE: 8.8579e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0110 - train_MSE: 4.6346e-04 - val_loss: 0.0203 - val_MSE: 8.8196e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0109 - train_MSE: 4.5517e-04 - val_loss: 0.0203 - val_MSE: 8.7861e-04\n",
      "epoch:740/1500 - 耗时:0.03分/总24.65分; train_loss 0.0172 train_MSE 0.0009; val_loss 0.0202 val_MSE 0.0009\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0110 - train_MSE: 4.6106e-04 - val_loss: 0.0203 - val_MSE: 8.7859e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0109 - train_MSE: 4.5943e-04 - val_loss: 0.0202 - val_MSE: 8.7477e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0109 - train_MSE: 4.5773e-04 - val_loss: 0.0202 - val_MSE: 8.7277e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0109 - train_MSE: 4.5614e-04 - val_loss: 0.0202 - val_MSE: 8.7019e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0108 - train_MSE: 4.5441e-04 - val_loss: 0.0201 - val_MSE: 8.6728e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0108 - train_MSE: 4.5355e-04 - val_loss: 0.0201 - val_MSE: 8.6399e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0108 - train_MSE: 4.5185e-04 - val_loss: 0.0201 - val_MSE: 8.6200e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0108 - train_MSE: 4.5011e-04 - val_loss: 0.0201 - val_MSE: 8.5998e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0108 - train_MSE: 4.4835e-04 - val_loss: 0.0201 - val_MSE: 8.5871e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4611e-04 - val_loss: 0.0201 - val_MSE: 8.5768e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0106 - train_MSE: 4.3838e-04 - val_loss: 0.0200 - val_MSE: 8.5710e-04\n",
      "epoch:750/1500 - 耗时:0.03分/总24.98分; train_loss 0.0170 train_MSE 0.0009; val_loss 0.0200 val_MSE 0.0009\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4415e-04 - val_loss: 0.0200 - val_MSE: 8.5710e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4308e-04 - val_loss: 0.0200 - val_MSE: 8.5634e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4273e-04 - val_loss: 0.0200 - val_MSE: 8.5504e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0106 - train_MSE: 4.4221e-04 - val_loss: 0.0200 - val_MSE: 8.5522e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4266e-04 - val_loss: 0.0200 - val_MSE: 8.5431e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0107 - train_MSE: 4.4242e-04 - val_loss: 0.0200 - val_MSE: 8.5398e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.4207e-04 - val_loss: 0.0199 - val_MSE: 8.5140e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0108 - train_MSE: 4.4193e-04 - val_loss: 0.0199 - val_MSE: 8.4939e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0108 - train_MSE: 4.3975e-04 - val_loss: 0.0199 - val_MSE: 8.4533e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.3584e-04 - val_loss: 0.0199 - val_MSE: 8.4565e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0105 - train_MSE: 4.2724e-04 - val_loss: 0.0199 - val_MSE: 8.4534e-04\n",
      "epoch:760/1500 - 耗时:0.03分/总25.32分; train_loss 0.0168 train_MSE 0.0009; val_loss 0.0199 val_MSE 0.0008\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0106 - train_MSE: 4.3286e-04 - val_loss: 0.0199 - val_MSE: 8.4534e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0106 - train_MSE: 4.3152e-04 - val_loss: 0.0199 - val_MSE: 8.4557e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0105 - train_MSE: 4.3044e-04 - val_loss: 0.0199 - val_MSE: 8.4930e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.3056e-04 - val_loss: 0.0199 - val_MSE: 8.5013e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0105 - train_MSE: 4.3115e-04 - val_loss: 0.0198 - val_MSE: 8.4767e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0106 - train_MSE: 4.3214e-04 - val_loss: 0.0198 - val_MSE: 8.4653e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0106 - train_MSE: 4.3237e-04 - val_loss: 0.0198 - val_MSE: 8.4328e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.3216e-04 - val_loss: 0.0197 - val_MSE: 8.4027e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0107 - train_MSE: 4.3094e-04 - val_loss: 0.0197 - val_MSE: 8.3906e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0106 - train_MSE: 4.2891e-04 - val_loss: 0.0197 - val_MSE: 8.3783e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0105 - train_MSE: 4.2146e-04 - val_loss: 0.0196 - val_MSE: 8.3664e-04\n",
      "epoch:770/1500 - 耗时:0.03分/总25.64分; train_loss 0.0166 train_MSE 0.0009; val_loss 0.0196 val_MSE 0.0008\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.2696e-04 - val_loss: 0.0196 - val_MSE: 8.3663e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0105 - train_MSE: 4.2632e-04 - val_loss: 0.0196 - val_MSE: 8.3269e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.2618e-04 - val_loss: 0.0195 - val_MSE: 8.3069e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.2478e-04 - val_loss: 0.0195 - val_MSE: 8.2763e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.2374e-04 - val_loss: 0.0194 - val_MSE: 8.2329e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0105 - train_MSE: 4.2273e-04 - val_loss: 0.0194 - val_MSE: 8.2005e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0105 - train_MSE: 4.2094e-04 - val_loss: 0.0193 - val_MSE: 8.1618e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0104 - train_MSE: 4.1841e-04 - val_loss: 0.0193 - val_MSE: 8.1320e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0104 - train_MSE: 4.1709e-04 - val_loss: 0.0192 - val_MSE: 8.0822e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0103 - train_MSE: 4.1507e-04 - val_loss: 0.0192 - val_MSE: 8.0484e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0102 - train_MSE: 4.0770e-04 - val_loss: 0.0192 - val_MSE: 8.0212e-04\n",
      "epoch:780/1500 - 耗时:0.03分/总25.97分; train_loss 0.0163 train_MSE 0.0009; val_loss 0.0191 val_MSE 0.0008\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0103 - train_MSE: 4.1310e-04 - val_loss: 0.0192 - val_MSE: 8.0210e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0102 - train_MSE: 4.1184e-04 - val_loss: 0.0191 - val_MSE: 7.9866e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0102 - train_MSE: 4.1046e-04 - val_loss: 0.0191 - val_MSE: 7.9604e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0102 - train_MSE: 4.0944e-04 - val_loss: 0.0190 - val_MSE: 7.9118e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0102 - train_MSE: 4.0966e-04 - val_loss: 0.0190 - val_MSE: 7.9017e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0102 - train_MSE: 4.0885e-04 - val_loss: 0.0189 - val_MSE: 7.8492e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0102 - train_MSE: 4.0813e-04 - val_loss: 0.0189 - val_MSE: 7.8089e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0102 - train_MSE: 4.0531e-04 - val_loss: 0.0188 - val_MSE: 7.7676e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0101 - train_MSE: 4.0153e-04 - val_loss: 0.0188 - val_MSE: 7.7532e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0100 - train_MSE: 3.9964e-04 - val_loss: 0.0188 - val_MSE: 7.7463e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0099 - train_MSE: 3.9369e-04 - val_loss: 0.0188 - val_MSE: 7.7287e-04\n",
      "epoch:790/1500 - 耗时:0.03分/总26.31分; train_loss 0.0160 train_MSE 0.0008; val_loss 0.0188 val_MSE 0.0008\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0100 - train_MSE: 3.9903e-04 - val_loss: 0.0188 - val_MSE: 7.7287e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0099 - train_MSE: 3.9770e-04 - val_loss: 0.0188 - val_MSE: 7.7216e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0099 - train_MSE: 3.9679e-04 - val_loss: 0.0188 - val_MSE: 7.7010e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0099 - train_MSE: 3.9596e-04 - val_loss: 0.0187 - val_MSE: 7.6825e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0099 - train_MSE: 3.9555e-04 - val_loss: 0.0187 - val_MSE: 7.6769e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0098 - train_MSE: 3.9457e-04 - val_loss: 0.0187 - val_MSE: 7.6715e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0098 - train_MSE: 3.9339e-04 - val_loss: 0.0187 - val_MSE: 7.6428e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0099 - train_MSE: 3.9345e-04 - val_loss: 0.0186 - val_MSE: 7.6261e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0099 - train_MSE: 3.9298e-04 - val_loss: 0.0186 - val_MSE: 7.6047e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0099 - train_MSE: 3.9192e-04 - val_loss: 0.0186 - val_MSE: 7.5753e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0098 - train_MSE: 3.8390e-04 - val_loss: 0.0186 - val_MSE: 7.5517e-04\n",
      "epoch:800/1500 - 耗时:0.03分/总26.64分; train_loss 0.0158 train_MSE 0.0008; val_loss 0.0186 val_MSE 0.0008\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0099 - train_MSE: 3.8911e-04 - val_loss: 0.0186 - val_MSE: 7.5517e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0098 - train_MSE: 3.8570e-04 - val_loss: 0.0186 - val_MSE: 7.5500e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0098 - train_MSE: 3.8308e-04 - val_loss: 0.0186 - val_MSE: 7.5574e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0098 - train_MSE: 3.8209e-04 - val_loss: 0.0186 - val_MSE: 7.5650e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.8167e-04 - val_loss: 0.0186 - val_MSE: 7.5677e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.8141e-04 - val_loss: 0.0186 - val_MSE: 7.5491e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0096 - train_MSE: 3.8101e-04 - val_loss: 0.0185 - val_MSE: 7.5366e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0097 - train_MSE: 3.8108e-04 - val_loss: 0.0185 - val_MSE: 7.5237e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0097 - train_MSE: 3.8100e-04 - val_loss: 0.0184 - val_MSE: 7.4940e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.8110e-04 - val_loss: 0.0184 - val_MSE: 7.4818e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0097 - train_MSE: 3.7516e-04 - val_loss: 0.0184 - val_MSE: 7.4691e-04\n",
      "epoch:810/1500 - 耗时:0.03分/总26.97分; train_loss 0.0156 train_MSE 0.0008; val_loss 0.0184 val_MSE 0.0007\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.8019e-04 - val_loss: 0.0184 - val_MSE: 7.4690e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0097 - train_MSE: 3.7854e-04 - val_loss: 0.0184 - val_MSE: 7.4375e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0098 - train_MSE: 3.7807e-04 - val_loss: 0.0184 - val_MSE: 7.4239e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0097 - train_MSE: 3.7625e-04 - val_loss: 0.0184 - val_MSE: 7.3927e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.7335e-04 - val_loss: 0.0183 - val_MSE: 7.3906e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0096 - train_MSE: 3.7137e-04 - val_loss: 0.0183 - val_MSE: 7.4029e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0096 - train_MSE: 3.7073e-04 - val_loss: 0.0183 - val_MSE: 7.4314e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0096 - train_MSE: 3.7098e-04 - val_loss: 0.0183 - val_MSE: 7.4689e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0096 - train_MSE: 3.7196e-04 - val_loss: 0.0183 - val_MSE: 7.4566e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0096 - train_MSE: 3.7304e-04 - val_loss: 0.0182 - val_MSE: 7.4213e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0095 - train_MSE: 3.6857e-04 - val_loss: 0.0182 - val_MSE: 7.3843e-04\n",
      "epoch:820/1500 - 耗时:0.03分/总27.30分; train_loss 0.0154 train_MSE 0.0008; val_loss 0.0181 val_MSE 0.0007\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0096 - train_MSE: 3.7351e-04 - val_loss: 0.0182 - val_MSE: 7.3841e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0098 - train_MSE: 3.7484e-04 - val_loss: 0.0181 - val_MSE: 7.3353e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0098 - train_MSE: 3.7382e-04 - val_loss: 0.0181 - val_MSE: 7.2920e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0097 - train_MSE: 3.7175e-04 - val_loss: 0.0180 - val_MSE: 7.2539e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0096 - train_MSE: 3.6948e-04 - val_loss: 0.0180 - val_MSE: 7.2335e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0096 - train_MSE: 3.6748e-04 - val_loss: 0.0179 - val_MSE: 7.1974e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0095 - train_MSE: 3.6573e-04 - val_loss: 0.0179 - val_MSE: 7.1568e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0094 - train_MSE: 3.6399e-04 - val_loss: 0.0178 - val_MSE: 7.1376e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0094 - train_MSE: 3.6240e-04 - val_loss: 0.0178 - val_MSE: 7.1207e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0093 - train_MSE: 3.6040e-04 - val_loss: 0.0178 - val_MSE: 7.1045e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0092 - train_MSE: 3.5293e-04 - val_loss: 0.0177 - val_MSE: 7.0877e-04\n",
      "epoch:830/1500 - 耗时:0.03分/总27.64分; train_loss 0.0151 train_MSE 0.0008; val_loss 0.0177 val_MSE 0.0007\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0093 - train_MSE: 3.5947e-04 - val_loss: 0.0177 - val_MSE: 7.0876e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0093 - train_MSE: 3.5867e-04 - val_loss: 0.0177 - val_MSE: 7.0703e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0093 - train_MSE: 3.5830e-04 - val_loss: 0.0177 - val_MSE: 7.0485e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0093 - train_MSE: 3.5923e-04 - val_loss: 0.0176 - val_MSE: 7.0024e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0094 - train_MSE: 3.5889e-04 - val_loss: 0.0175 - val_MSE: 6.9467e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0094 - train_MSE: 3.5683e-04 - val_loss: 0.0175 - val_MSE: 6.8937e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0093 - train_MSE: 3.5436e-04 - val_loss: 0.0175 - val_MSE: 6.8687e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0093 - train_MSE: 3.5202e-04 - val_loss: 0.0174 - val_MSE: 6.8569e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0092 - train_MSE: 3.5059e-04 - val_loss: 0.0174 - val_MSE: 6.8525e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0092 - train_MSE: 3.4955e-04 - val_loss: 0.0174 - val_MSE: 6.8408e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0090 - train_MSE: 3.4386e-04 - val_loss: 0.0174 - val_MSE: 6.8228e-04\n",
      "epoch:840/1500 - 耗时:0.03分/总27.98分; train_loss 0.0148 train_MSE 0.0007; val_loss 0.0173 val_MSE 0.0007\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0091 - train_MSE: 3.4866e-04 - val_loss: 0.0174 - val_MSE: 6.8228e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0091 - train_MSE: 3.4790e-04 - val_loss: 0.0173 - val_MSE: 6.8205e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0091 - train_MSE: 3.4782e-04 - val_loss: 0.0173 - val_MSE: 6.7996e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0091 - train_MSE: 3.4773e-04 - val_loss: 0.0173 - val_MSE: 6.7794e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0091 - train_MSE: 3.4788e-04 - val_loss: 0.0172 - val_MSE: 6.7544e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0091 - train_MSE: 3.4727e-04 - val_loss: 0.0172 - val_MSE: 6.7286e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0091 - train_MSE: 3.4661e-04 - val_loss: 0.0171 - val_MSE: 6.6848e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0092 - train_MSE: 3.4548e-04 - val_loss: 0.0171 - val_MSE: 6.6380e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0091 - train_MSE: 3.4318e-04 - val_loss: 0.0171 - val_MSE: 6.6084e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0090 - train_MSE: 3.3978e-04 - val_loss: 0.0171 - val_MSE: 6.6012e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0089 - train_MSE: 3.3278e-04 - val_loss: 0.0171 - val_MSE: 6.6156e-04\n",
      "epoch:850/1500 - 耗时:0.03分/总28.31分; train_loss 0.0146 train_MSE 0.0007; val_loss 0.0171 val_MSE 0.0007\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0090 - train_MSE: 3.3748e-04 - val_loss: 0.0171 - val_MSE: 6.6156e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0090 - train_MSE: 3.3688e-04 - val_loss: 0.0171 - val_MSE: 6.6099e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0090 - train_MSE: 3.3619e-04 - val_loss: 0.0171 - val_MSE: 6.6166e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0090 - train_MSE: 3.3543e-04 - val_loss: 0.0171 - val_MSE: 6.6220e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0089 - train_MSE: 3.3462e-04 - val_loss: 0.0171 - val_MSE: 6.6067e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0089 - train_MSE: 3.3358e-04 - val_loss: 0.0171 - val_MSE: 6.5954e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.3243e-04 - val_loss: 0.0170 - val_MSE: 6.5732e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.3207e-04 - val_loss: 0.0170 - val_MSE: 6.5623e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0089 - train_MSE: 3.3182e-04 - val_loss: 0.0170 - val_MSE: 6.5456e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0089 - train_MSE: 3.3143e-04 - val_loss: 0.0170 - val_MSE: 6.5213e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0089 - train_MSE: 3.2622e-04 - val_loss: 0.0170 - val_MSE: 6.5047e-04\n",
      "epoch:860/1500 - 耗时:0.03分/总28.63分; train_loss 0.0145 train_MSE 0.0007; val_loss 0.0170 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0089 - train_MSE: 3.3074e-04 - val_loss: 0.0170 - val_MSE: 6.5047e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0089 - train_MSE: 3.2862e-04 - val_loss: 0.0170 - val_MSE: 6.4886e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0088 - train_MSE: 3.2537e-04 - val_loss: 0.0171 - val_MSE: 6.4913e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0089 - train_MSE: 3.2337e-04 - val_loss: 0.0170 - val_MSE: 6.4978e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0088 - train_MSE: 3.2261e-04 - val_loss: 0.0170 - val_MSE: 6.5235e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.2273e-04 - val_loss: 0.0170 - val_MSE: 6.5470e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.2337e-04 - val_loss: 0.0170 - val_MSE: 6.5504e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0087 - train_MSE: 3.2343e-04 - val_loss: 0.0169 - val_MSE: 6.5277e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0087 - train_MSE: 3.2416e-04 - val_loss: 0.0169 - val_MSE: 6.4931e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0088 - train_MSE: 3.2480e-04 - val_loss: 0.0168 - val_MSE: 6.4778e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0088 - train_MSE: 3.2088e-04 - val_loss: 0.0168 - val_MSE: 6.4555e-04\n",
      "epoch:870/1500 - 耗时:0.03分/总28.95分; train_loss 0.0142 train_MSE 0.0007; val_loss 0.0167 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.2531e-04 - val_loss: 0.0168 - val_MSE: 6.4553e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0090 - train_MSE: 3.2542e-04 - val_loss: 0.0167 - val_MSE: 6.4127e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0089 - train_MSE: 3.2376e-04 - val_loss: 0.0167 - val_MSE: 6.3732e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0090 - train_MSE: 3.2373e-04 - val_loss: 0.0166 - val_MSE: 6.3497e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0089 - train_MSE: 3.2203e-04 - val_loss: 0.0166 - val_MSE: 6.3444e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0088 - train_MSE: 3.1960e-04 - val_loss: 0.0166 - val_MSE: 6.3451e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0087 - train_MSE: 3.1798e-04 - val_loss: 0.0166 - val_MSE: 6.3553e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0086 - train_MSE: 3.1701e-04 - val_loss: 0.0166 - val_MSE: 6.3672e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0086 - train_MSE: 3.1685e-04 - val_loss: 0.0166 - val_MSE: 6.3594e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0086 - train_MSE: 3.1666e-04 - val_loss: 0.0166 - val_MSE: 6.3244e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0085 - train_MSE: 3.1241e-04 - val_loss: 0.0165 - val_MSE: 6.2937e-04\n",
      "epoch:880/1500 - 耗时:0.03分/总29.27分; train_loss 0.0140 train_MSE 0.0007; val_loss 0.0164 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0086 - train_MSE: 3.1676e-04 - val_loss: 0.0165 - val_MSE: 6.2936e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0087 - train_MSE: 3.1831e-04 - val_loss: 0.0164 - val_MSE: 6.2557e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.1842e-04 - val_loss: 0.0164 - val_MSE: 6.2130e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.1711e-04 - val_loss: 0.0163 - val_MSE: 6.1703e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0088 - train_MSE: 3.1489e-04 - val_loss: 0.0162 - val_MSE: 6.1388e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0087 - train_MSE: 3.1166e-04 - val_loss: 0.0162 - val_MSE: 6.1161e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0086 - train_MSE: 3.0973e-04 - val_loss: 0.0162 - val_MSE: 6.0993e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0085 - train_MSE: 3.0803e-04 - val_loss: 0.0162 - val_MSE: 6.0931e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0085 - train_MSE: 3.0712e-04 - val_loss: 0.0162 - val_MSE: 6.0727e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0085 - train_MSE: 3.0648e-04 - val_loss: 0.0161 - val_MSE: 6.0575e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0084 - train_MSE: 3.0231e-04 - val_loss: 0.0161 - val_MSE: 6.0594e-04\n",
      "epoch:890/1500 - 耗时:0.03分/总29.61分; train_loss 0.0138 train_MSE 0.0007; val_loss 0.0161 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0085 - train_MSE: 3.0661e-04 - val_loss: 0.0161 - val_MSE: 6.0595e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0085 - train_MSE: 3.0649e-04 - val_loss: 0.0161 - val_MSE: 6.0797e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0085 - train_MSE: 3.0678e-04 - val_loss: 0.0161 - val_MSE: 6.0577e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0086 - train_MSE: 3.0804e-04 - val_loss: 0.0160 - val_MSE: 6.0050e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0087 - train_MSE: 3.0886e-04 - val_loss: 0.0159 - val_MSE: 5.9303e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0086 - train_MSE: 3.0705e-04 - val_loss: 0.0158 - val_MSE: 5.8775e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0085 - train_MSE: 3.0441e-04 - val_loss: 0.0158 - val_MSE: 5.8509e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0084 - train_MSE: 3.0253e-04 - val_loss: 0.0158 - val_MSE: 5.8561e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0083 - train_MSE: 3.0112e-04 - val_loss: 0.0158 - val_MSE: 5.8576e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0083 - train_MSE: 3.0055e-04 - val_loss: 0.0158 - val_MSE: 5.8472e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0082 - train_MSE: 2.9580e-04 - val_loss: 0.0158 - val_MSE: 5.8403e-04\n",
      "epoch:900/1500 - 耗时:0.04分/总29.94分; train_loss 0.0136 train_MSE 0.0007; val_loss 0.0157 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0083 - train_MSE: 3.0005e-04 - val_loss: 0.0158 - val_MSE: 5.8402e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9968e-04 - val_loss: 0.0157 - val_MSE: 5.8162e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0082 - train_MSE: 2.9928e-04 - val_loss: 0.0157 - val_MSE: 5.7977e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9910e-04 - val_loss: 0.0157 - val_MSE: 5.7875e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0082 - train_MSE: 2.9758e-04 - val_loss: 0.0156 - val_MSE: 5.7499e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9679e-04 - val_loss: 0.0156 - val_MSE: 5.7312e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9617e-04 - val_loss: 0.0155 - val_MSE: 5.6973e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9520e-04 - val_loss: 0.0155 - val_MSE: 5.6751e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.9479e-04 - val_loss: 0.0155 - val_MSE: 5.6550e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0082 - train_MSE: 2.9317e-04 - val_loss: 0.0154 - val_MSE: 5.6250e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0081 - train_MSE: 2.8629e-04 - val_loss: 0.0155 - val_MSE: 5.6122e-04\n",
      "epoch:910/1500 - 耗时:0.03分/总30.27分; train_loss 0.0135 train_MSE 0.0006; val_loss 0.0155 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0082 - train_MSE: 2.9042e-04 - val_loss: 0.0155 - val_MSE: 5.6122e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0081 - train_MSE: 2.8871e-04 - val_loss: 0.0155 - val_MSE: 5.6145e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0083 - train_MSE: 2.8869e-04 - val_loss: 0.0155 - val_MSE: 5.6033e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0084 - train_MSE: 2.9033e-04 - val_loss: 0.0155 - val_MSE: 5.6032e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0084 - train_MSE: 2.8980e-04 - val_loss: 0.0155 - val_MSE: 5.5979e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0083 - train_MSE: 2.8794e-04 - val_loss: 0.0155 - val_MSE: 5.5940e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0082 - train_MSE: 2.8534e-04 - val_loss: 0.0155 - val_MSE: 5.5948e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.8263e-04 - val_loss: 0.0155 - val_MSE: 5.5873e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.8107e-04 - val_loss: 0.0155 - val_MSE: 5.5686e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.8023e-04 - val_loss: 0.0155 - val_MSE: 5.5575e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0079 - train_MSE: 2.7557e-04 - val_loss: 0.0154 - val_MSE: 5.5584e-04\n",
      "epoch:920/1500 - 耗时:0.03分/总30.55分; train_loss 0.0132 train_MSE 0.0006; val_loss 0.0154 val_MSE 0.0006\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.7957e-04 - val_loss: 0.0154 - val_MSE: 5.5584e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0080 - train_MSE: 2.8011e-04 - val_loss: 0.0154 - val_MSE: 5.5545e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0080 - train_MSE: 2.8073e-04 - val_loss: 0.0154 - val_MSE: 5.5420e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0079 - train_MSE: 2.7953e-04 - val_loss: 0.0154 - val_MSE: 5.5225e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0079 - train_MSE: 2.7883e-04 - val_loss: 0.0153 - val_MSE: 5.5129e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0079 - train_MSE: 2.7846e-04 - val_loss: 0.0153 - val_MSE: 5.4945e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0080 - train_MSE: 2.7890e-04 - val_loss: 0.0152 - val_MSE: 5.4840e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0081 - train_MSE: 2.7939e-04 - val_loss: 0.0152 - val_MSE: 5.4633e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0083 - train_MSE: 2.7973e-04 - val_loss: 0.0152 - val_MSE: 5.4437e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0083 - train_MSE: 2.7813e-04 - val_loss: 0.0153 - val_MSE: 5.4233e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0080 - train_MSE: 2.7084e-04 - val_loss: 0.0153 - val_MSE: 5.4212e-04\n",
      "epoch:930/1500 - 耗时:0.03分/总30.88分; train_loss 0.0132 train_MSE 0.0006; val_loss 0.0153 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0081 - train_MSE: 2.7472e-04 - val_loss: 0.0153 - val_MSE: 5.4212e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.7248e-04 - val_loss: 0.0153 - val_MSE: 5.4320e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.7197e-04 - val_loss: 0.0153 - val_MSE: 5.4435e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0081 - train_MSE: 2.7226e-04 - val_loss: 0.0153 - val_MSE: 5.4628e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0081 - train_MSE: 2.7195e-04 - val_loss: 0.0153 - val_MSE: 5.4977e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.7275e-04 - val_loss: 0.0152 - val_MSE: 5.4752e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.7240e-04 - val_loss: 0.0151 - val_MSE: 5.4251e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0080 - train_MSE: 2.7212e-04 - val_loss: 0.0151 - val_MSE: 5.4027e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.7264e-04 - val_loss: 0.0150 - val_MSE: 5.3930e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0081 - train_MSE: 2.7261e-04 - val_loss: 0.0150 - val_MSE: 5.3888e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0081 - train_MSE: 2.7079e-04 - val_loss: 0.0150 - val_MSE: 5.3717e-04\n",
      "epoch:940/1500 - 耗时:0.03分/总31.20分; train_loss 0.0130 train_MSE 0.0006; val_loss 0.0150 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0081 - train_MSE: 2.7205e-04 - val_loss: 0.0150 - val_MSE: 5.3717e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0081 - train_MSE: 2.7184e-04 - val_loss: 0.0150 - val_MSE: 5.3718e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0081 - train_MSE: 2.7092e-04 - val_loss: 0.0150 - val_MSE: 5.3726e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0080 - train_MSE: 2.7017e-04 - val_loss: 0.0150 - val_MSE: 5.3838e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.7010e-04 - val_loss: 0.0150 - val_MSE: 5.3731e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0081 - train_MSE: 2.7112e-04 - val_loss: 0.0150 - val_MSE: 5.3494e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.7128e-04 - val_loss: 0.0149 - val_MSE: 5.3181e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0082 - train_MSE: 2.7134e-04 - val_loss: 0.0149 - val_MSE: 5.2962e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0082 - train_MSE: 2.7024e-04 - val_loss: 0.0148 - val_MSE: 5.2704e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0081 - train_MSE: 2.6905e-04 - val_loss: 0.0148 - val_MSE: 5.2461e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0080 - train_MSE: 2.6708e-04 - val_loss: 0.0148 - val_MSE: 5.2292e-04\n",
      "epoch:950/1500 - 耗时:0.03分/总31.51分; train_loss 0.0129 train_MSE 0.0006; val_loss 0.0148 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.6832e-04 - val_loss: 0.0148 - val_MSE: 5.2291e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0080 - train_MSE: 2.6868e-04 - val_loss: 0.0148 - val_MSE: 5.2066e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0081 - train_MSE: 2.6833e-04 - val_loss: 0.0147 - val_MSE: 5.1691e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0080 - train_MSE: 2.6702e-04 - val_loss: 0.0146 - val_MSE: 5.1276e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0079 - train_MSE: 2.6578e-04 - val_loss: 0.0145 - val_MSE: 5.0963e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0079 - train_MSE: 2.6468e-04 - val_loss: 0.0145 - val_MSE: 5.0724e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0078 - train_MSE: 2.6317e-04 - val_loss: 0.0144 - val_MSE: 5.0466e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0077 - train_MSE: 2.6181e-04 - val_loss: 0.0144 - val_MSE: 5.0330e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0077 - train_MSE: 2.6094e-04 - val_loss: 0.0144 - val_MSE: 5.0282e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0077 - train_MSE: 2.6023e-04 - val_loss: 0.0144 - val_MSE: 5.0138e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0075 - train_MSE: 2.5175e-04 - val_loss: 0.0144 - val_MSE: 4.9986e-04\n",
      "epoch:960/1500 - 耗时:0.03分/总31.85分; train_loss 0.0125 train_MSE 0.0006; val_loss 0.0143 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5931e-04 - val_loss: 0.0144 - val_MSE: 4.9985e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5877e-04 - val_loss: 0.0143 - val_MSE: 4.9788e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5812e-04 - val_loss: 0.0143 - val_MSE: 4.9623e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5762e-04 - val_loss: 0.0143 - val_MSE: 4.9441e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5686e-04 - val_loss: 0.0142 - val_MSE: 4.9111e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0076 - train_MSE: 2.5609e-04 - val_loss: 0.0141 - val_MSE: 4.8817e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5496e-04 - val_loss: 0.0141 - val_MSE: 4.8560e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.5324e-04 - val_loss: 0.0141 - val_MSE: 4.8399e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.5177e-04 - val_loss: 0.0141 - val_MSE: 4.8408e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.5171e-04 - val_loss: 0.0141 - val_MSE: 4.8356e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0076 - train_MSE: 2.4874e-04 - val_loss: 0.0141 - val_MSE: 4.8312e-04\n",
      "epoch:970/1500 - 耗时:0.03分/总32.18分; train_loss 0.0124 train_MSE 0.0006; val_loss 0.0141 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0077 - train_MSE: 2.5237e-04 - val_loss: 0.0141 - val_MSE: 4.8312e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5144e-04 - val_loss: 0.0141 - val_MSE: 4.8217e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.5120e-04 - val_loss: 0.0141 - val_MSE: 4.8109e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.5085e-04 - val_loss: 0.0141 - val_MSE: 4.8007e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.4958e-04 - val_loss: 0.0140 - val_MSE: 4.7870e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.4919e-04 - val_loss: 0.0140 - val_MSE: 4.7795e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0075 - train_MSE: 2.4839e-04 - val_loss: 0.0140 - val_MSE: 4.7630e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.4659e-04 - val_loss: 0.0139 - val_MSE: 4.7521e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.4557e-04 - val_loss: 0.0140 - val_MSE: 4.7424e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.4475e-04 - val_loss: 0.0140 - val_MSE: 4.7337e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0075 - train_MSE: 2.4301e-04 - val_loss: 0.0141 - val_MSE: 4.7327e-04\n",
      "epoch:980/1500 - 耗时:0.03分/总32.49分; train_loss 0.0123 train_MSE 0.0005; val_loss 0.0141 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0075 - train_MSE: 2.4418e-04 - val_loss: 0.0141 - val_MSE: 4.7327e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.4258e-04 - val_loss: 0.0141 - val_MSE: 4.7286e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.4061e-04 - val_loss: 0.0141 - val_MSE: 4.7242e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0075 - train_MSE: 2.4060e-04 - val_loss: 0.0141 - val_MSE: 4.7267e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0077 - train_MSE: 2.4236e-04 - val_loss: 0.0140 - val_MSE: 4.7249e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.4238e-04 - val_loss: 0.0140 - val_MSE: 4.7236e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.4059e-04 - val_loss: 0.0140 - val_MSE: 4.7194e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0074 - train_MSE: 2.3914e-04 - val_loss: 0.0139 - val_MSE: 4.7030e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0073 - train_MSE: 2.3806e-04 - val_loss: 0.0139 - val_MSE: 4.6844e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0073 - train_MSE: 2.3760e-04 - val_loss: 0.0139 - val_MSE: 4.6758e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0072 - train_MSE: 2.3114e-04 - val_loss: 0.0139 - val_MSE: 4.6657e-04\n",
      "epoch:990/1500 - 耗时:0.03分/总32.79分; train_loss 0.0120 train_MSE 0.0005; val_loss 0.0139 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0073 - train_MSE: 2.3686e-04 - val_loss: 0.0139 - val_MSE: 4.6656e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0073 - train_MSE: 2.3628e-04 - val_loss: 0.0139 - val_MSE: 4.6598e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3609e-04 - val_loss: 0.0139 - val_MSE: 4.6661e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3529e-04 - val_loss: 0.0140 - val_MSE: 4.6623e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3481e-04 - val_loss: 0.0140 - val_MSE: 4.6676e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3352e-04 - val_loss: 0.0140 - val_MSE: 4.6624e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3364e-04 - val_loss: 0.0140 - val_MSE: 4.6615e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0076 - train_MSE: 2.3522e-04 - val_loss: 0.0139 - val_MSE: 4.6717e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.3466e-04 - val_loss: 0.0138 - val_MSE: 4.7042e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.3528e-04 - val_loss: 0.0138 - val_MSE: 4.6973e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0073 - train_MSE: 2.2821e-04 - val_loss: 0.0137 - val_MSE: 4.6656e-04\n",
      "epoch:1000/1500 - 耗时:0.03分/总33.10分; train_loss 0.0120 train_MSE 0.0005; val_loss 0.0137 val_MSE 0.0005\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0074 - train_MSE: 2.3492e-04 - val_loss: 0.0137 - val_MSE: 4.6656e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3496e-04 - val_loss: 0.0137 - val_MSE: 4.6491e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.3579e-04 - val_loss: 0.0137 - val_MSE: 4.6484e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0075 - train_MSE: 2.3704e-04 - val_loss: 0.0138 - val_MSE: 4.6514e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0076 - train_MSE: 2.3790e-04 - val_loss: 0.0138 - val_MSE: 4.6668e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.3855e-04 - val_loss: 0.0139 - val_MSE: 4.6617e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0076 - train_MSE: 2.3952e-04 - val_loss: 0.0139 - val_MSE: 4.6407e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0077 - train_MSE: 2.4021e-04 - val_loss: 0.0138 - val_MSE: 4.6059e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0077 - train_MSE: 2.3895e-04 - val_loss: 0.0137 - val_MSE: 4.5736e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0075 - train_MSE: 2.3658e-04 - val_loss: 0.0136 - val_MSE: 4.5308e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0074 - train_MSE: 2.3407e-04 - val_loss: 0.0135 - val_MSE: 4.4980e-04\n",
      "epoch:1010/1500 - 耗时:0.04分/总33.40分; train_loss 0.0119 train_MSE 0.0005; val_loss 0.0135 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0075 - train_MSE: 2.3517e-04 - val_loss: 0.0135 - val_MSE: 4.4979e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0074 - train_MSE: 2.3326e-04 - val_loss: 0.0135 - val_MSE: 4.4724e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0073 - train_MSE: 2.3094e-04 - val_loss: 0.0134 - val_MSE: 4.4554e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0072 - train_MSE: 2.2940e-04 - val_loss: 0.0134 - val_MSE: 4.4408e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0071 - train_MSE: 2.2821e-04 - val_loss: 0.0134 - val_MSE: 4.4280e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0071 - train_MSE: 2.2719e-04 - val_loss: 0.0133 - val_MSE: 4.4129e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2597e-04 - val_loss: 0.0133 - val_MSE: 4.4041e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2525e-04 - val_loss: 0.0133 - val_MSE: 4.3938e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2448e-04 - val_loss: 0.0132 - val_MSE: 4.3770e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2361e-04 - val_loss: 0.0132 - val_MSE: 4.3663e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0069 - train_MSE: 2.1987e-04 - val_loss: 0.0132 - val_MSE: 4.3444e-04\n",
      "epoch:1020/1500 - 耗时:0.03分/总33.74分; train_loss 0.0115 train_MSE 0.0005; val_loss 0.0131 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2313e-04 - val_loss: 0.0132 - val_MSE: 4.3444e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2337e-04 - val_loss: 0.0131 - val_MSE: 4.3359e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2285e-04 - val_loss: 0.0131 - val_MSE: 4.3099e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2209e-04 - val_loss: 0.0131 - val_MSE: 4.2927e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2117e-04 - val_loss: 0.0131 - val_MSE: 4.2894e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2035e-04 - val_loss: 0.0130 - val_MSE: 4.2845e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2101e-04 - val_loss: 0.0130 - val_MSE: 4.2847e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 2.2209e-04 - val_loss: 0.0130 - val_MSE: 4.2767e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.2246e-04 - val_loss: 0.0130 - val_MSE: 4.2735e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.2238e-04 - val_loss: 0.0130 - val_MSE: 4.2696e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0069 - train_MSE: 2.1678e-04 - val_loss: 0.0131 - val_MSE: 4.2678e-04\n",
      "epoch:1030/1500 - 耗时:0.03分/总34.06分; train_loss 0.0115 train_MSE 0.0005; val_loss 0.0130 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.2229e-04 - val_loss: 0.0131 - val_MSE: 4.2677e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0069 - train_MSE: 2.2173e-04 - val_loss: 0.0130 - val_MSE: 4.2511e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2200e-04 - val_loss: 0.0130 - val_MSE: 4.2272e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2114e-04 - val_loss: 0.0130 - val_MSE: 4.2070e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.2063e-04 - val_loss: 0.0129 - val_MSE: 4.1754e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0071 - train_MSE: 2.2010e-04 - val_loss: 0.0128 - val_MSE: 4.1548e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.1762e-04 - val_loss: 0.0129 - val_MSE: 4.1397e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 2.1522e-04 - val_loss: 0.0129 - val_MSE: 4.1403e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0069 - train_MSE: 2.1392e-04 - val_loss: 0.0128 - val_MSE: 4.1485e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0071 - train_MSE: 2.1524e-04 - val_loss: 0.0128 - val_MSE: 4.1447e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0071 - train_MSE: 2.1341e-04 - val_loss: 0.0128 - val_MSE: 4.1466e-04\n",
      "epoch:1040/1500 - 耗时:0.03分/总34.38分; train_loss 0.0114 train_MSE 0.0005; val_loss 0.0128 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0072 - train_MSE: 2.1654e-04 - val_loss: 0.0128 - val_MSE: 4.1466e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 2.1452e-04 - val_loss: 0.0128 - val_MSE: 4.1508e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 2.1261e-04 - val_loss: 0.0128 - val_MSE: 4.1442e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 2.1117e-04 - val_loss: 0.0128 - val_MSE: 4.1385e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0068 - train_MSE: 2.0926e-04 - val_loss: 0.0129 - val_MSE: 4.1330e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0067 - train_MSE: 2.0774e-04 - val_loss: 0.0129 - val_MSE: 4.1275e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0661e-04 - val_loss: 0.0129 - val_MSE: 4.1207e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0599e-04 - val_loss: 0.0129 - val_MSE: 4.1254e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0067 - train_MSE: 2.0592e-04 - val_loss: 0.0130 - val_MSE: 4.1280e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0494e-04 - val_loss: 0.0129 - val_MSE: 4.0970e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0065 - train_MSE: 2.0312e-04 - val_loss: 0.0130 - val_MSE: 4.1046e-04\n",
      "epoch:1050/1500 - 耗时:0.03分/总34.66分; train_loss 0.0110 train_MSE 0.0005; val_loss 0.0130 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0412e-04 - val_loss: 0.0130 - val_MSE: 4.1046e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0423e-04 - val_loss: 0.0130 - val_MSE: 4.1123e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0356e-04 - val_loss: 0.0128 - val_MSE: 4.0829e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 2.0237e-04 - val_loss: 0.0129 - val_MSE: 4.0911e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0068 - train_MSE: 2.0408e-04 - val_loss: 0.0131 - val_MSE: 4.1091e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0264e-04 - val_loss: 0.0128 - val_MSE: 4.0655e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0198e-04 - val_loss: 0.0129 - val_MSE: 4.0716e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0066 - train_MSE: 2.0228e-04 - val_loss: 0.0128 - val_MSE: 4.0514e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 2.0178e-04 - val_loss: 0.0126 - val_MSE: 4.0348e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0066 - train_MSE: 2.0130e-04 - val_loss: 0.0127 - val_MSE: 4.0408e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0066 - train_MSE: 2.0097e-04 - val_loss: 0.0126 - val_MSE: 4.0328e-04\n",
      "epoch:1060/1500 - 耗时:0.03分/总34.97分; train_loss 0.0110 train_MSE 0.0004; val_loss 0.0125 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0066 - train_MSE: 2.0195e-04 - val_loss: 0.0126 - val_MSE: 4.0328e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0066 - train_MSE: 2.0167e-04 - val_loss: 0.0125 - val_MSE: 4.0209e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0067 - train_MSE: 2.0235e-04 - val_loss: 0.0125 - val_MSE: 4.0110e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0067 - train_MSE: 2.0276e-04 - val_loss: 0.0125 - val_MSE: 4.0121e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0067 - train_MSE: 2.0290e-04 - val_loss: 0.0125 - val_MSE: 3.9917e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0070 - train_MSE: 2.0430e-04 - val_loss: 0.0124 - val_MSE: 3.9807e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0072 - train_MSE: 2.0606e-04 - val_loss: 0.0124 - val_MSE: 3.9742e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0071 - train_MSE: 2.0421e-04 - val_loss: 0.0124 - val_MSE: 3.9800e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 2.0272e-04 - val_loss: 0.0124 - val_MSE: 3.9715e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 2.0433e-04 - val_loss: 0.0124 - val_MSE: 3.9785e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0072 - train_MSE: 2.0205e-04 - val_loss: 0.0126 - val_MSE: 4.0109e-04\n",
      "epoch:1070/1500 - 耗时:0.03分/总35.29分; train_loss 0.0113 train_MSE 0.0004; val_loss 0.0127 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0072 - train_MSE: 2.0493e-04 - val_loss: 0.0126 - val_MSE: 4.0110e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 2.0331e-04 - val_loss: 0.0127 - val_MSE: 4.0319e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.0286e-04 - val_loss: 0.0127 - val_MSE: 4.0330e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 2.0407e-04 - val_loss: 0.0127 - val_MSE: 4.0308e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0072 - train_MSE: 2.0639e-04 - val_loss: 0.0127 - val_MSE: 4.0377e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.0844e-04 - val_loss: 0.0127 - val_MSE: 4.0071e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.0908e-04 - val_loss: 0.0127 - val_MSE: 3.9853e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0074 - train_MSE: 2.0900e-04 - val_loss: 0.0127 - val_MSE: 3.9681e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0073 - train_MSE: 2.0781e-04 - val_loss: 0.0126 - val_MSE: 3.9215e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0072 - train_MSE: 2.0647e-04 - val_loss: 0.0125 - val_MSE: 3.8704e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0070 - train_MSE: 2.0015e-04 - val_loss: 0.0124 - val_MSE: 3.8340e-04\n",
      "epoch:1080/1500 - 耗时:0.03分/总35.58分; train_loss 0.0112 train_MSE 0.0004; val_loss 0.0123 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0071 - train_MSE: 2.0408e-04 - val_loss: 0.0124 - val_MSE: 3.8339e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0069 - train_MSE: 2.0141e-04 - val_loss: 0.0123 - val_MSE: 3.8212e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0068 - train_MSE: 1.9907e-04 - val_loss: 0.0123 - val_MSE: 3.8108e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0066 - train_MSE: 1.9703e-04 - val_loss: 0.0123 - val_MSE: 3.8048e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0066 - train_MSE: 1.9598e-04 - val_loss: 0.0123 - val_MSE: 3.8041e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0065 - train_MSE: 1.9520e-04 - val_loss: 0.0122 - val_MSE: 3.7992e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0065 - train_MSE: 1.9441e-04 - val_loss: 0.0122 - val_MSE: 3.7909e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0064 - train_MSE: 1.9385e-04 - val_loss: 0.0122 - val_MSE: 3.7828e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0064 - train_MSE: 1.9330e-04 - val_loss: 0.0122 - val_MSE: 3.7745e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0064 - train_MSE: 1.9286e-04 - val_loss: 0.0122 - val_MSE: 3.7635e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0063 - train_MSE: 1.8889e-04 - val_loss: 0.0122 - val_MSE: 3.7629e-04\n",
      "epoch:1090/1500 - 耗时:0.03分/总35.91分; train_loss 0.0106 train_MSE 0.0004; val_loss 0.0121 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0063 - train_MSE: 1.9177e-04 - val_loss: 0.0122 - val_MSE: 3.7629e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0063 - train_MSE: 1.9106e-04 - val_loss: 0.0121 - val_MSE: 3.7411e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0063 - train_MSE: 1.9058e-04 - val_loss: 0.0120 - val_MSE: 3.7246e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0063 - train_MSE: 1.9016e-04 - val_loss: 0.0120 - val_MSE: 3.7178e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0063 - train_MSE: 1.8962e-04 - val_loss: 0.0120 - val_MSE: 3.7018e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.8887e-04 - val_loss: 0.0120 - val_MSE: 3.6974e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.8717e-04 - val_loss: 0.0120 - val_MSE: 3.7049e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.8668e-04 - val_loss: 0.0120 - val_MSE: 3.7000e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.8752e-04 - val_loss: 0.0120 - val_MSE: 3.6967e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0067 - train_MSE: 1.8946e-04 - val_loss: 0.0120 - val_MSE: 3.6918e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0066 - train_MSE: 1.8691e-04 - val_loss: 0.0120 - val_MSE: 3.6899e-04\n",
      "epoch:1100/1500 - 耗时:0.03分/总36.22分; train_loss 0.0106 train_MSE 0.0004; val_loss 0.0119 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0066 - train_MSE: 1.8970e-04 - val_loss: 0.0120 - val_MSE: 3.6898e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0066 - train_MSE: 1.9001e-04 - val_loss: 0.0119 - val_MSE: 3.6762e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0065 - train_MSE: 1.8933e-04 - val_loss: 0.0119 - val_MSE: 3.6628e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0065 - train_MSE: 1.8891e-04 - val_loss: 0.0118 - val_MSE: 3.6481e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.8964e-04 - val_loss: 0.0118 - val_MSE: 3.6306e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0065 - train_MSE: 1.8850e-04 - val_loss: 0.0118 - val_MSE: 3.6237e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0064 - train_MSE: 1.8698e-04 - val_loss: 0.0118 - val_MSE: 3.6078e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0066 - train_MSE: 1.8753e-04 - val_loss: 0.0117 - val_MSE: 3.5914e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 1.8904e-04 - val_loss: 0.0117 - val_MSE: 3.5651e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 1.8737e-04 - val_loss: 0.0120 - val_MSE: 3.5753e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0064 - train_MSE: 1.7938e-04 - val_loss: 0.0120 - val_MSE: 3.5808e-04\n",
      "epoch:1110/1500 - 耗时:0.03分/总36.53分; train_loss 0.0106 train_MSE 0.0004; val_loss 0.0120 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.8211e-04 - val_loss: 0.0120 - val_MSE: 3.5808e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.8020e-04 - val_loss: 0.0120 - val_MSE: 3.5901e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7901e-04 - val_loss: 0.0120 - val_MSE: 3.6063e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7864e-04 - val_loss: 0.0120 - val_MSE: 3.6074e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7888e-04 - val_loss: 0.0120 - val_MSE: 3.6011e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7871e-04 - val_loss: 0.0119 - val_MSE: 3.5978e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7798e-04 - val_loss: 0.0118 - val_MSE: 3.5850e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.7737e-04 - val_loss: 0.0119 - val_MSE: 3.5772e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.7689e-04 - val_loss: 0.0118 - val_MSE: 3.5634e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.7671e-04 - val_loss: 0.0118 - val_MSE: 3.5579e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0061 - train_MSE: 1.7391e-04 - val_loss: 0.0118 - val_MSE: 3.5546e-04\n",
      "epoch:1120/1500 - 耗时:0.03分/总36.81分; train_loss 0.0101 train_MSE 0.0004; val_loss 0.0119 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.7653e-04 - val_loss: 0.0118 - val_MSE: 3.5547e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7693e-04 - val_loss: 0.0119 - val_MSE: 3.5597e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7799e-04 - val_loss: 0.0120 - val_MSE: 3.5673e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7704e-04 - val_loss: 0.0120 - val_MSE: 3.5733e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7599e-04 - val_loss: 0.0122 - val_MSE: 3.5995e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7520e-04 - val_loss: 0.0121 - val_MSE: 3.5804e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7558e-04 - val_loss: 0.0120 - val_MSE: 3.5618e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.7653e-04 - val_loss: 0.0118 - val_MSE: 3.5675e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0065 - train_MSE: 1.7651e-04 - val_loss: 0.0117 - val_MSE: 3.5802e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.7675e-04 - val_loss: 0.0116 - val_MSE: 3.5765e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0063 - train_MSE: 1.7396e-04 - val_loss: 0.0117 - val_MSE: 3.5680e-04\n",
      "epoch:1130/1500 - 耗时:0.03分/总37.10分; train_loss 0.0102 train_MSE 0.0004; val_loss 0.0117 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7652e-04 - val_loss: 0.0117 - val_MSE: 3.5680e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7610e-04 - val_loss: 0.0117 - val_MSE: 3.5602e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7643e-04 - val_loss: 0.0117 - val_MSE: 3.5574e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0065 - train_MSE: 1.7864e-04 - val_loss: 0.0117 - val_MSE: 3.5481e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0066 - train_MSE: 1.8023e-04 - val_loss: 0.0118 - val_MSE: 3.5552e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0066 - train_MSE: 1.8067e-04 - val_loss: 0.0120 - val_MSE: 3.5937e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7933e-04 - val_loss: 0.0120 - val_MSE: 3.6054e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.7970e-04 - val_loss: 0.0120 - val_MSE: 3.5967e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 1.8112e-04 - val_loss: 0.0121 - val_MSE: 3.6074e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 1.8093e-04 - val_loss: 0.0120 - val_MSE: 3.5813e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0066 - train_MSE: 1.7873e-04 - val_loss: 0.0121 - val_MSE: 3.5841e-04\n",
      "epoch:1140/1500 - 耗时:0.03分/总37.39分; train_loss 0.0104 train_MSE 0.0004; val_loss 0.0120 val_MSE 0.0004\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 1.8129e-04 - val_loss: 0.0121 - val_MSE: 3.5840e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0066 - train_MSE: 1.7998e-04 - val_loss: 0.0120 - val_MSE: 3.5694e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7791e-04 - val_loss: 0.0119 - val_MSE: 3.5258e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7694e-04 - val_loss: 0.0118 - val_MSE: 3.5024e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.7528e-04 - val_loss: 0.0118 - val_MSE: 3.4900e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.7351e-04 - val_loss: 0.0117 - val_MSE: 3.4603e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0062 - train_MSE: 1.7231e-04 - val_loss: 0.0116 - val_MSE: 3.4257e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.7210e-04 - val_loss: 0.0115 - val_MSE: 3.4153e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0061 - train_MSE: 1.7039e-04 - val_loss: 0.0114 - val_MSE: 3.3797e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0061 - train_MSE: 1.6973e-04 - val_loss: 0.0114 - val_MSE: 3.3748e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0062 - train_MSE: 1.6924e-04 - val_loss: 0.0114 - val_MSE: 3.3667e-04\n",
      "epoch:1150/1500 - 耗时:0.03分/总37.71分; train_loss 0.0100 train_MSE 0.0004; val_loss 0.0113 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.7007e-04 - val_loss: 0.0114 - val_MSE: 3.3666e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0061 - train_MSE: 1.6872e-04 - val_loss: 0.0113 - val_MSE: 3.3515e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0060 - train_MSE: 1.6803e-04 - val_loss: 0.0113 - val_MSE: 3.3462e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0061 - train_MSE: 1.6850e-04 - val_loss: 0.0112 - val_MSE: 3.3329e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0061 - train_MSE: 1.6842e-04 - val_loss: 0.0112 - val_MSE: 3.3272e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.6867e-04 - val_loss: 0.0112 - val_MSE: 3.3262e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.6995e-04 - val_loss: 0.0112 - val_MSE: 3.3263e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7167e-04 - val_loss: 0.0113 - val_MSE: 3.3367e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7225e-04 - val_loss: 0.0114 - val_MSE: 3.3478e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.7339e-04 - val_loss: 0.0115 - val_MSE: 3.3517e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0064 - train_MSE: 1.7247e-04 - val_loss: 0.0114 - val_MSE: 3.3343e-04\n",
      "epoch:1160/1500 - 耗时:0.03分/总38.02分; train_loss 0.0105 train_MSE 0.0004; val_loss 0.0113 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.7506e-04 - val_loss: 0.0114 - val_MSE: 3.3341e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0064 - train_MSE: 1.7369e-04 - val_loss: 0.0113 - val_MSE: 3.2816e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0063 - train_MSE: 1.7155e-04 - val_loss: 0.0112 - val_MSE: 3.2474e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0061 - train_MSE: 1.6914e-04 - val_loss: 0.0111 - val_MSE: 3.2301e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0060 - train_MSE: 1.6732e-04 - val_loss: 0.0111 - val_MSE: 3.2248e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.6574e-04 - val_loss: 0.0110 - val_MSE: 3.2217e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0059 - train_MSE: 1.6425e-04 - val_loss: 0.0110 - val_MSE: 3.2243e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.6346e-04 - val_loss: 0.0110 - val_MSE: 3.2218e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0059 - train_MSE: 1.6311e-04 - val_loss: 0.0111 - val_MSE: 3.2304e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.6256e-04 - val_loss: 0.0111 - val_MSE: 3.2331e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0060 - train_MSE: 1.6088e-04 - val_loss: 0.0111 - val_MSE: 3.2274e-04\n",
      "epoch:1170/1500 - 耗时:0.03分/总38.33分; train_loss 0.0097 train_MSE 0.0004; val_loss 0.0111 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.6249e-04 - val_loss: 0.0111 - val_MSE: 3.2275e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.6280e-04 - val_loss: 0.0111 - val_MSE: 3.2318e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.6125e-04 - val_loss: 0.0111 - val_MSE: 3.2219e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5993e-04 - val_loss: 0.0110 - val_MSE: 3.2079e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0058 - train_MSE: 1.5956e-04 - val_loss: 0.0111 - val_MSE: 3.2027e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0057 - train_MSE: 1.5811e-04 - val_loss: 0.0110 - val_MSE: 3.1924e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.5734e-04 - val_loss: 0.0110 - val_MSE: 3.1873e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.5688e-04 - val_loss: 0.0111 - val_MSE: 3.1829e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5709e-04 - val_loss: 0.0111 - val_MSE: 3.1803e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.5909e-04 - val_loss: 0.0113 - val_MSE: 3.1939e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0063 - train_MSE: 1.5614e-04 - val_loss: 0.0117 - val_MSE: 3.2370e-04\n",
      "epoch:1180/1500 - 耗时:0.03分/总38.63分; train_loss 0.0102 train_MSE 0.0004; val_loss 0.0117 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.6097e-04 - val_loss: 0.0117 - val_MSE: 3.2369e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.5690e-04 - val_loss: 0.0117 - val_MSE: 3.2281e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.5504e-04 - val_loss: 0.0116 - val_MSE: 3.2307e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.5646e-04 - val_loss: 0.0115 - val_MSE: 3.2200e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.5747e-04 - val_loss: 0.0113 - val_MSE: 3.1993e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.5529e-04 - val_loss: 0.0113 - val_MSE: 3.1891e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5394e-04 - val_loss: 0.0112 - val_MSE: 3.1654e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5327e-04 - val_loss: 0.0111 - val_MSE: 3.1444e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5322e-04 - val_loss: 0.0111 - val_MSE: 3.1427e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5289e-04 - val_loss: 0.0111 - val_MSE: 3.1349e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0058 - train_MSE: 1.5204e-04 - val_loss: 0.0112 - val_MSE: 3.1477e-04\n",
      "epoch:1190/1500 - 耗时:0.03分/总38.91分; train_loss 0.0095 train_MSE 0.0003; val_loss 0.0113 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.5280e-04 - val_loss: 0.0112 - val_MSE: 3.1477e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.5317e-04 - val_loss: 0.0113 - val_MSE: 3.1550e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.5323e-04 - val_loss: 0.0114 - val_MSE: 3.1740e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.5387e-04 - val_loss: 0.0114 - val_MSE: 3.1790e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.5295e-04 - val_loss: 0.0113 - val_MSE: 3.1512e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.5363e-04 - val_loss: 0.0112 - val_MSE: 3.1452e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0061 - train_MSE: 1.5320e-04 - val_loss: 0.0110 - val_MSE: 3.1384e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.5506e-04 - val_loss: 0.0109 - val_MSE: 3.1364e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.5534e-04 - val_loss: 0.0108 - val_MSE: 3.1231e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0062 - train_MSE: 1.5617e-04 - val_loss: 0.0108 - val_MSE: 3.1178e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0063 - train_MSE: 1.5751e-04 - val_loss: 0.0107 - val_MSE: 3.1214e-04\n",
      "epoch:1200/1500 - 耗时:0.03分/总39.22分; train_loss 0.0097 train_MSE 0.0003; val_loss 0.0109 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.5824e-04 - val_loss: 0.0107 - val_MSE: 3.1215e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.5805e-04 - val_loss: 0.0109 - val_MSE: 3.1464e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.5762e-04 - val_loss: 0.0111 - val_MSE: 3.1865e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.5846e-04 - val_loss: 0.0113 - val_MSE: 3.2302e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.5967e-04 - val_loss: 0.0114 - val_MSE: 3.2468e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0065 - train_MSE: 1.6279e-04 - val_loss: 0.0116 - val_MSE: 3.2679e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0069 - train_MSE: 1.6890e-04 - val_loss: 0.0116 - val_MSE: 3.2633e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0070 - train_MSE: 1.7108e-04 - val_loss: 0.0116 - val_MSE: 3.2588e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 1.7184e-04 - val_loss: 0.0114 - val_MSE: 3.1990e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0071 - train_MSE: 1.7032e-04 - val_loss: 0.0112 - val_MSE: 3.1345e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0068 - train_MSE: 1.6565e-04 - val_loss: 0.0111 - val_MSE: 3.0858e-04\n",
      "epoch:1210/1500 - 耗时:0.03分/总39.51分; train_loss 0.0101 train_MSE 0.0004; val_loss 0.0110 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0068 - train_MSE: 1.6640e-04 - val_loss: 0.0111 - val_MSE: 3.0857e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0064 - train_MSE: 1.5968e-04 - val_loss: 0.0110 - val_MSE: 3.0465e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0062 - train_MSE: 1.5605e-04 - val_loss: 0.0108 - val_MSE: 3.0069e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0059 - train_MSE: 1.5270e-04 - val_loss: 0.0107 - val_MSE: 3.0044e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0057 - train_MSE: 1.5037e-04 - val_loss: 0.0106 - val_MSE: 2.9929e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4894e-04 - val_loss: 0.0106 - val_MSE: 2.9837e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4800e-04 - val_loss: 0.0105 - val_MSE: 2.9785e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.4676e-04 - val_loss: 0.0105 - val_MSE: 2.9682e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0055 - train_MSE: 1.4636e-04 - val_loss: 0.0105 - val_MSE: 2.9756e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.4616e-04 - val_loss: 0.0104 - val_MSE: 2.9681e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0054 - train_MSE: 1.4337e-04 - val_loss: 0.0104 - val_MSE: 2.9620e-04\n",
      "epoch:1220/1500 - 耗时:0.03分/总39.83分; train_loss 0.0090 train_MSE 0.0003; val_loss 0.0104 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.4559e-04 - val_loss: 0.0104 - val_MSE: 2.9620e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.4583e-04 - val_loss: 0.0104 - val_MSE: 2.9657e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.4534e-04 - val_loss: 0.0104 - val_MSE: 2.9603e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.4633e-04 - val_loss: 0.0105 - val_MSE: 2.9807e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.4592e-04 - val_loss: 0.0105 - val_MSE: 2.9771e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.4531e-04 - val_loss: 0.0106 - val_MSE: 2.9726e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.4546e-04 - val_loss: 0.0106 - val_MSE: 2.9586e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4600e-04 - val_loss: 0.0105 - val_MSE: 2.9350e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0059 - train_MSE: 1.4763e-04 - val_loss: 0.0104 - val_MSE: 2.9032e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.4935e-04 - val_loss: 0.0103 - val_MSE: 2.8762e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0061 - train_MSE: 1.4544e-04 - val_loss: 0.0108 - val_MSE: 2.9154e-04\n",
      "epoch:1230/1500 - 耗时:0.03分/总40.11分; train_loss 0.0094 train_MSE 0.0003; val_loss 0.0110 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.4686e-04 - val_loss: 0.0108 - val_MSE: 2.9155e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.4177e-04 - val_loss: 0.0110 - val_MSE: 2.9450e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3963e-04 - val_loss: 0.0109 - val_MSE: 2.9464e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3924e-04 - val_loss: 0.0107 - val_MSE: 2.9335e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3926e-04 - val_loss: 0.0106 - val_MSE: 2.9161e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4004e-04 - val_loss: 0.0104 - val_MSE: 2.8974e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0057 - train_MSE: 1.4160e-04 - val_loss: 0.0103 - val_MSE: 2.8855e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4037e-04 - val_loss: 0.0102 - val_MSE: 2.8818e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.3979e-04 - val_loss: 0.0102 - val_MSE: 2.8694e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0053 - train_MSE: 1.3973e-04 - val_loss: 0.0102 - val_MSE: 2.8603e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0054 - train_MSE: 1.3868e-04 - val_loss: 0.0101 - val_MSE: 2.8400e-04\n",
      "epoch:1240/1500 - 耗时:0.03分/总40.42分; train_loss 0.0089 train_MSE 0.0003; val_loss 0.0101 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.4009e-04 - val_loss: 0.0101 - val_MSE: 2.8400e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.4098e-04 - val_loss: 0.0101 - val_MSE: 2.8261e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.4298e-04 - val_loss: 0.0101 - val_MSE: 2.8065e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.4456e-04 - val_loss: 0.0103 - val_MSE: 2.8046e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.4324e-04 - val_loss: 0.0109 - val_MSE: 2.8622e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3689e-04 - val_loss: 0.0109 - val_MSE: 2.8637e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3551e-04 - val_loss: 0.0106 - val_MSE: 2.8497e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3483e-04 - val_loss: 0.0106 - val_MSE: 2.8578e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3475e-04 - val_loss: 0.0104 - val_MSE: 2.8293e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3453e-04 - val_loss: 0.0103 - val_MSE: 2.8201e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0052 - train_MSE: 1.3229e-04 - val_loss: 0.0102 - val_MSE: 2.8151e-04\n",
      "epoch:1250/1500 - 耗时:0.03分/总40.70分; train_loss 0.0087 train_MSE 0.0003; val_loss 0.0101 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3435e-04 - val_loss: 0.0102 - val_MSE: 2.8150e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3439e-04 - val_loss: 0.0101 - val_MSE: 2.8060e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.3420e-04 - val_loss: 0.0101 - val_MSE: 2.7960e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0053 - train_MSE: 1.3500e-04 - val_loss: 0.0102 - val_MSE: 2.7867e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3580e-04 - val_loss: 0.0102 - val_MSE: 2.7747e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3548e-04 - val_loss: 0.0102 - val_MSE: 2.7691e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3448e-04 - val_loss: 0.0102 - val_MSE: 2.7727e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3290e-04 - val_loss: 0.0105 - val_MSE: 2.8001e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3365e-04 - val_loss: 0.0107 - val_MSE: 2.8362e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3299e-04 - val_loss: 0.0109 - val_MSE: 2.8538e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0054 - train_MSE: 1.3069e-04 - val_loss: 0.0111 - val_MSE: 2.8789e-04\n",
      "epoch:1260/1500 - 耗时:0.03分/总40.98分; train_loss 0.0089 train_MSE 0.0003; val_loss 0.0111 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3267e-04 - val_loss: 0.0111 - val_MSE: 2.8789e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.3309e-04 - val_loss: 0.0111 - val_MSE: 2.8759e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.3500e-04 - val_loss: 0.0108 - val_MSE: 2.8431e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.3531e-04 - val_loss: 0.0104 - val_MSE: 2.8225e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0057 - train_MSE: 1.3409e-04 - val_loss: 0.0102 - val_MSE: 2.8068e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.3378e-04 - val_loss: 0.0100 - val_MSE: 2.7986e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0055 - train_MSE: 1.3293e-04 - val_loss: 0.0100 - val_MSE: 2.7848e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.3236e-04 - val_loss: 0.0100 - val_MSE: 2.7599e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0055 - train_MSE: 1.3292e-04 - val_loss: 0.0099 - val_MSE: 2.7419e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3292e-04 - val_loss: 0.0099 - val_MSE: 2.7336e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0055 - train_MSE: 1.3181e-04 - val_loss: 0.0099 - val_MSE: 2.7260e-04\n",
      "epoch:1270/1500 - 耗时:0.03分/总41.30分; train_loss 0.0087 train_MSE 0.0003; val_loss 0.0099 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0055 - train_MSE: 1.3309e-04 - val_loss: 0.0099 - val_MSE: 2.7260e-04\n",
      "116/116 [==============================] - 2s 18ms/step - train_loss: 0.0056 - train_MSE: 1.3365e-04 - val_loss: 0.0099 - val_MSE: 2.7254e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.3402e-04 - val_loss: 0.0099 - val_MSE: 2.7244e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0055 - train_MSE: 1.3248e-04 - val_loss: 0.0099 - val_MSE: 2.7244e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0054 - train_MSE: 1.3202e-04 - val_loss: 0.0099 - val_MSE: 2.7257e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.3204e-04 - val_loss: 0.0099 - val_MSE: 2.7265e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.3124e-04 - val_loss: 0.0098 - val_MSE: 2.7236e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3197e-04 - val_loss: 0.0100 - val_MSE: 2.7631e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3277e-04 - val_loss: 0.0104 - val_MSE: 2.8441e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3309e-04 - val_loss: 0.0109 - val_MSE: 2.9502e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0054 - train_MSE: 1.3267e-04 - val_loss: 0.0108 - val_MSE: 2.9257e-04\n",
      "epoch:1280/1500 - 耗时:0.03分/总41.61分; train_loss 0.0088 train_MSE 0.0003; val_loss 0.0107 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.3332e-04 - val_loss: 0.0108 - val_MSE: 2.9255e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.3502e-04 - val_loss: 0.0107 - val_MSE: 2.8747e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0062 - train_MSE: 1.4014e-04 - val_loss: 0.0105 - val_MSE: 2.8114e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0060 - train_MSE: 1.3943e-04 - val_loss: 0.0106 - val_MSE: 2.8207e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.4035e-04 - val_loss: 0.0105 - val_MSE: 2.8021e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.3888e-04 - val_loss: 0.0105 - val_MSE: 2.8124e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.3864e-04 - val_loss: 0.0106 - val_MSE: 2.8253e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3669e-04 - val_loss: 0.0108 - val_MSE: 2.8720e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.3609e-04 - val_loss: 0.0106 - val_MSE: 2.8112e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3682e-04 - val_loss: 0.0104 - val_MSE: 2.7632e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0057 - train_MSE: 1.3491e-04 - val_loss: 0.0103 - val_MSE: 2.7428e-04\n",
      "epoch:1290/1500 - 耗时:0.03分/总41.89分; train_loss 0.0089 train_MSE 0.0003; val_loss 0.0102 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3689e-04 - val_loss: 0.0103 - val_MSE: 2.7426e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3671e-04 - val_loss: 0.0102 - val_MSE: 2.7088e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3657e-04 - val_loss: 0.0102 - val_MSE: 2.7146e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.3658e-04 - val_loss: 0.0102 - val_MSE: 2.7172e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.3629e-04 - val_loss: 0.0103 - val_MSE: 2.7329e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.3384e-04 - val_loss: 0.0102 - val_MSE: 2.6950e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.3143e-04 - val_loss: 0.0100 - val_MSE: 2.6537e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0054 - train_MSE: 1.3027e-04 - val_loss: 0.0099 - val_MSE: 2.6186e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0054 - train_MSE: 1.2954e-04 - val_loss: 0.0097 - val_MSE: 2.5909e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0055 - train_MSE: 1.2936e-04 - val_loss: 0.0097 - val_MSE: 2.5787e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0054 - train_MSE: 1.2723e-04 - val_loss: 0.0096 - val_MSE: 2.5813e-04\n",
      "epoch:1300/1500 - 耗时:0.03分/总42.19分; train_loss 0.0086 train_MSE 0.0003; val_loss 0.0097 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.2913e-04 - val_loss: 0.0096 - val_MSE: 2.5813e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.2998e-04 - val_loss: 0.0097 - val_MSE: 2.5811e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.3048e-04 - val_loss: 0.0097 - val_MSE: 2.5835e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.2982e-04 - val_loss: 0.0098 - val_MSE: 2.5951e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.2812e-04 - val_loss: 0.0101 - val_MSE: 2.6320e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2490e-04 - val_loss: 0.0105 - val_MSE: 2.6819e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2368e-04 - val_loss: 0.0105 - val_MSE: 2.6772e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.2503e-04 - val_loss: 0.0107 - val_MSE: 2.7035e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.2704e-04 - val_loss: 0.0106 - val_MSE: 2.6931e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.2866e-04 - val_loss: 0.0106 - val_MSE: 2.6886e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0057 - train_MSE: 1.2469e-04 - val_loss: 0.0107 - val_MSE: 2.7090e-04\n",
      "epoch:1310/1500 - 耗时:0.03分/总42.47分; train_loss 0.0087 train_MSE 0.0003; val_loss 0.0108 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.2647e-04 - val_loss: 0.0107 - val_MSE: 2.7090e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.2412e-04 - val_loss: 0.0108 - val_MSE: 2.7090e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.2136e-04 - val_loss: 0.0107 - val_MSE: 2.6855e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1974e-04 - val_loss: 0.0104 - val_MSE: 2.6329e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1879e-04 - val_loss: 0.0104 - val_MSE: 2.6168e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.1741e-04 - val_loss: 0.0102 - val_MSE: 2.5809e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.1723e-04 - val_loss: 0.0101 - val_MSE: 2.5546e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.1718e-04 - val_loss: 0.0099 - val_MSE: 2.5253e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0049 - train_MSE: 1.1703e-04 - val_loss: 0.0097 - val_MSE: 2.5023e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0049 - train_MSE: 1.1721e-04 - val_loss: 0.0096 - val_MSE: 2.4943e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0050 - train_MSE: 1.1684e-04 - val_loss: 0.0096 - val_MSE: 2.4812e-04\n",
      "epoch:1320/1500 - 耗时:0.03分/总42.76分; train_loss 0.0082 train_MSE 0.0003; val_loss 0.0094 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0050 - train_MSE: 1.1803e-04 - val_loss: 0.0096 - val_MSE: 2.4811e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.1742e-04 - val_loss: 0.0094 - val_MSE: 2.4688e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1854e-04 - val_loss: 0.0097 - val_MSE: 2.4849e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2048e-04 - val_loss: 0.0098 - val_MSE: 2.4905e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2011e-04 - val_loss: 0.0100 - val_MSE: 2.5032e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2022e-04 - val_loss: 0.0103 - val_MSE: 2.5411e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2062e-04 - val_loss: 0.0103 - val_MSE: 2.5343e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.2419e-04 - val_loss: 0.0101 - val_MSE: 2.5265e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.2167e-04 - val_loss: 0.0099 - val_MSE: 2.5406e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.1907e-04 - val_loss: 0.0098 - val_MSE: 2.5370e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0053 - train_MSE: 1.1690e-04 - val_loss: 0.0096 - val_MSE: 2.5130e-04\n",
      "epoch:1330/1500 - 耗时:0.03分/总43.04分; train_loss 0.0083 train_MSE 0.0003; val_loss 0.0097 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.1804e-04 - val_loss: 0.0096 - val_MSE: 2.5130e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1641e-04 - val_loss: 0.0097 - val_MSE: 2.5033e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1571e-04 - val_loss: 0.0096 - val_MSE: 2.4913e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.1476e-04 - val_loss: 0.0096 - val_MSE: 2.4829e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.1521e-04 - val_loss: 0.0095 - val_MSE: 2.4750e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.1451e-04 - val_loss: 0.0096 - val_MSE: 2.4742e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1635e-04 - val_loss: 0.0095 - val_MSE: 2.4601e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1561e-04 - val_loss: 0.0095 - val_MSE: 2.4671e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1549e-04 - val_loss: 0.0095 - val_MSE: 2.4592e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1681e-04 - val_loss: 0.0095 - val_MSE: 2.4575e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0052 - train_MSE: 1.1569e-04 - val_loss: 0.0095 - val_MSE: 2.4655e-04\n",
      "epoch:1340/1500 - 耗时:0.03分/总43.32分; train_loss 0.0082 train_MSE 0.0003; val_loss 0.0095 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1680e-04 - val_loss: 0.0095 - val_MSE: 2.4655e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1693e-04 - val_loss: 0.0095 - val_MSE: 2.4613e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0052 - train_MSE: 1.1701e-04 - val_loss: 0.0094 - val_MSE: 2.4686e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.1920e-04 - val_loss: 0.0093 - val_MSE: 2.4633e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.1965e-04 - val_loss: 0.0096 - val_MSE: 2.5547e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.1968e-04 - val_loss: 0.0104 - val_MSE: 2.6958e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.1980e-04 - val_loss: 0.0105 - val_MSE: 2.7317e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.2076e-04 - val_loss: 0.0104 - val_MSE: 2.6839e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.2451e-04 - val_loss: 0.0104 - val_MSE: 2.6937e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0063 - train_MSE: 1.3219e-04 - val_loss: 0.0103 - val_MSE: 2.6459e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0061 - train_MSE: 1.2890e-04 - val_loss: 0.0106 - val_MSE: 2.7029e-04\n",
      "epoch:1350/1500 - 耗时:0.03分/总43.61分; train_loss 0.0088 train_MSE 0.0003; val_loss 0.0102 val_MSE 0.0003\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0061 - train_MSE: 1.3004e-04 - val_loss: 0.0106 - val_MSE: 2.7025e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0064 - train_MSE: 1.3406e-04 - val_loss: 0.0102 - val_MSE: 2.6183e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.3030e-04 - val_loss: 0.0104 - val_MSE: 2.6364e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0061 - train_MSE: 1.3045e-04 - val_loss: 0.0102 - val_MSE: 2.6002e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0059 - train_MSE: 1.2789e-04 - val_loss: 0.0101 - val_MSE: 2.5598e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.2657e-04 - val_loss: 0.0100 - val_MSE: 2.5385e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.2444e-04 - val_loss: 0.0100 - val_MSE: 2.5259e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.2269e-04 - val_loss: 0.0099 - val_MSE: 2.4915e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.2179e-04 - val_loss: 0.0098 - val_MSE: 2.4693e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.2050e-04 - val_loss: 0.0097 - val_MSE: 2.4384e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0053 - train_MSE: 1.1812e-04 - val_loss: 0.0095 - val_MSE: 2.4078e-04\n",
      "epoch:1360/1500 - 耗时:0.03分/总43.90分; train_loss 0.0082 train_MSE 0.0003; val_loss 0.0094 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.1926e-04 - val_loss: 0.0095 - val_MSE: 2.4077e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0053 - train_MSE: 1.1824e-04 - val_loss: 0.0094 - val_MSE: 2.3828e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0053 - train_MSE: 1.1772e-04 - val_loss: 0.0093 - val_MSE: 2.3626e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0053 - train_MSE: 1.1731e-04 - val_loss: 0.0092 - val_MSE: 2.3452e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0053 - train_MSE: 1.1662e-04 - val_loss: 0.0091 - val_MSE: 2.3348e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0053 - train_MSE: 1.1595e-04 - val_loss: 0.0091 - val_MSE: 2.3263e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0052 - train_MSE: 1.1498e-04 - val_loss: 0.0090 - val_MSE: 2.3237e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.1379e-04 - val_loss: 0.0090 - val_MSE: 2.3200e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1274e-04 - val_loss: 0.0091 - val_MSE: 2.3198e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.1141e-04 - val_loss: 0.0092 - val_MSE: 2.3353e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0049 - train_MSE: 1.0866e-04 - val_loss: 0.0093 - val_MSE: 2.3337e-04\n",
      "epoch:1370/1500 - 耗时:0.03分/总44.22分; train_loss 0.0078 train_MSE 0.0002; val_loss 0.0094 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0919e-04 - val_loss: 0.0093 - val_MSE: 2.3338e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0880e-04 - val_loss: 0.0094 - val_MSE: 2.3470e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 1.0803e-04 - val_loss: 0.0095 - val_MSE: 2.3460e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 1.0712e-04 - val_loss: 0.0094 - val_MSE: 2.3340e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0833e-04 - val_loss: 0.0094 - val_MSE: 2.3335e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0941e-04 - val_loss: 0.0094 - val_MSE: 2.3341e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.1340e-04 - val_loss: 0.0092 - val_MSE: 2.3076e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.1424e-04 - val_loss: 0.0092 - val_MSE: 2.3103e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.1065e-04 - val_loss: 0.0094 - val_MSE: 2.3154e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0690e-04 - val_loss: 0.0095 - val_MSE: 2.3120e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0047 - train_MSE: 1.0455e-04 - val_loss: 0.0094 - val_MSE: 2.2875e-04\n",
      "epoch:1380/1500 - 耗时:0.03分/总44.51分; train_loss 0.0076 train_MSE 0.0002; val_loss 0.0093 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 1.0508e-04 - val_loss: 0.0094 - val_MSE: 2.2875e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 1.0480e-04 - val_loss: 0.0093 - val_MSE: 2.2838e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 1.0410e-04 - val_loss: 0.0091 - val_MSE: 2.2626e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 1.0399e-04 - val_loss: 0.0093 - val_MSE: 2.2754e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 1.0414e-04 - val_loss: 0.0094 - val_MSE: 2.2942e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 1.0341e-04 - val_loss: 0.0092 - val_MSE: 2.2699e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 1.0317e-04 - val_loss: 0.0090 - val_MSE: 2.2542e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0561e-04 - val_loss: 0.0094 - val_MSE: 2.2957e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0437e-04 - val_loss: 0.0094 - val_MSE: 2.2881e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0412e-04 - val_loss: 0.0097 - val_MSE: 2.3060e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0047 - train_MSE: 1.0132e-04 - val_loss: 0.0099 - val_MSE: 2.3302e-04\n",
      "epoch:1390/1500 - 耗时:0.03分/总44.79分; train_loss 0.0076 train_MSE 0.0002; val_loss 0.0100 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 1.0288e-04 - val_loss: 0.0099 - val_MSE: 2.3303e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 1.0220e-04 - val_loss: 0.0100 - val_MSE: 2.3449e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 1.0175e-04 - val_loss: 0.0104 - val_MSE: 2.4015e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 1.0209e-04 - val_loss: 0.0102 - val_MSE: 2.3712e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0525e-04 - val_loss: 0.0103 - val_MSE: 2.3837e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0963e-04 - val_loss: 0.0101 - val_MSE: 2.3667e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0059 - train_MSE: 1.1375e-04 - val_loss: 0.0093 - val_MSE: 2.2867e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0056 - train_MSE: 1.1014e-04 - val_loss: 0.0090 - val_MSE: 2.2667e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0053 - train_MSE: 1.0678e-04 - val_loss: 0.0088 - val_MSE: 2.2638e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0049 - train_MSE: 1.0380e-04 - val_loss: 0.0088 - val_MSE: 2.2441e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0048 - train_MSE: 1.0190e-04 - val_loss: 0.0087 - val_MSE: 2.2297e-04\n",
      "epoch:1400/1500 - 耗时:0.03分/总45.09分; train_loss 0.0076 train_MSE 0.0002; val_loss 0.0087 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 1.0240e-04 - val_loss: 0.0087 - val_MSE: 2.2297e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0314e-04 - val_loss: 0.0087 - val_MSE: 2.2089e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0332e-04 - val_loss: 0.0088 - val_MSE: 2.2044e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0458e-04 - val_loss: 0.0088 - val_MSE: 2.2056e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0058 - train_MSE: 1.1158e-04 - val_loss: 0.0091 - val_MSE: 2.2095e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0820e-04 - val_loss: 0.0091 - val_MSE: 2.2135e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0872e-04 - val_loss: 0.0092 - val_MSE: 2.2287e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0865e-04 - val_loss: 0.0090 - val_MSE: 2.2246e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0812e-04 - val_loss: 0.0088 - val_MSE: 2.2445e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0661e-04 - val_loss: 0.0088 - val_MSE: 2.2753e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0050 - train_MSE: 1.0376e-04 - val_loss: 0.0092 - val_MSE: 2.3429e-04\n",
      "epoch:1410/1500 - 耗时:0.03分/总45.37分; train_loss 0.0079 train_MSE 0.0002; val_loss 0.0093 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0528e-04 - val_loss: 0.0092 - val_MSE: 2.3429e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0412e-04 - val_loss: 0.0093 - val_MSE: 2.3561e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 1.0387e-04 - val_loss: 0.0096 - val_MSE: 2.3997e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0606e-04 - val_loss: 0.0095 - val_MSE: 2.3885e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0658e-04 - val_loss: 0.0099 - val_MSE: 2.4512e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.1196e-04 - val_loss: 0.0096 - val_MSE: 2.3958e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0936e-04 - val_loss: 0.0098 - val_MSE: 2.4298e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.1059e-04 - val_loss: 0.0097 - val_MSE: 2.4077e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0750e-04 - val_loss: 0.0099 - val_MSE: 2.4485e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0648e-04 - val_loss: 0.0099 - val_MSE: 2.4392e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0049 - train_MSE: 1.0445e-04 - val_loss: 0.0099 - val_MSE: 2.4456e-04\n",
      "epoch:1420/1500 - 耗时:0.03分/总45.65分; train_loss 0.0078 train_MSE 0.0002; val_loss 0.0099 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0600e-04 - val_loss: 0.0099 - val_MSE: 2.4456e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0532e-04 - val_loss: 0.0099 - val_MSE: 2.4330e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0551e-04 - val_loss: 0.0099 - val_MSE: 2.4327e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0512e-04 - val_loss: 0.0098 - val_MSE: 2.4216e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 1.0535e-04 - val_loss: 0.0099 - val_MSE: 2.4334e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0553e-04 - val_loss: 0.0101 - val_MSE: 2.4659e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0539e-04 - val_loss: 0.0100 - val_MSE: 2.4422e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0622e-04 - val_loss: 0.0099 - val_MSE: 2.4046e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0640e-04 - val_loss: 0.0097 - val_MSE: 2.3535e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0689e-04 - val_loss: 0.0094 - val_MSE: 2.2870e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0052 - train_MSE: 1.0398e-04 - val_loss: 0.0093 - val_MSE: 2.2539e-04\n",
      "epoch:1430/1500 - 耗时:0.03分/总45.93分; train_loss 0.0080 train_MSE 0.0002; val_loss 0.0094 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0759e-04 - val_loss: 0.0093 - val_MSE: 2.2540e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0865e-04 - val_loss: 0.0094 - val_MSE: 2.2677e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0923e-04 - val_loss: 0.0094 - val_MSE: 2.2617e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0721e-04 - val_loss: 0.0092 - val_MSE: 2.2338e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0571e-04 - val_loss: 0.0091 - val_MSE: 2.2094e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0490e-04 - val_loss: 0.0091 - val_MSE: 2.1927e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0450e-04 - val_loss: 0.0089 - val_MSE: 2.1535e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0051 - train_MSE: 1.0424e-04 - val_loss: 0.0088 - val_MSE: 2.1293e-04\n",
      "116/116 [==============================] - 2s 17ms/step - train_loss: 0.0050 - train_MSE: 1.0364e-04 - val_loss: 0.0086 - val_MSE: 2.1091e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0051 - train_MSE: 1.0460e-04 - val_loss: 0.0085 - val_MSE: 2.0934e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0052 - train_MSE: 1.0481e-04 - val_loss: 0.0085 - val_MSE: 2.0850e-04\n",
      "epoch:1440/1500 - 耗时:0.03分/总46.23分; train_loss 0.0077 train_MSE 0.0002; val_loss 0.0085 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0577e-04 - val_loss: 0.0085 - val_MSE: 2.0850e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0673e-04 - val_loss: 0.0085 - val_MSE: 2.0800e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0605e-04 - val_loss: 0.0087 - val_MSE: 2.0873e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0425e-04 - val_loss: 0.0089 - val_MSE: 2.1101e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0261e-04 - val_loss: 0.0091 - val_MSE: 2.1427e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0217e-04 - val_loss: 0.0095 - val_MSE: 2.1918e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0156e-04 - val_loss: 0.0096 - val_MSE: 2.1955e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0453e-04 - val_loss: 0.0098 - val_MSE: 2.2209e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0057 - train_MSE: 1.0650e-04 - val_loss: 0.0097 - val_MSE: 2.2144e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.0780e-04 - val_loss: 0.0098 - val_MSE: 2.2224e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0058 - train_MSE: 1.0752e-04 - val_loss: 0.0099 - val_MSE: 2.2405e-04\n",
      "epoch:1450/1500 - 耗时:0.03分/总46.50分; train_loss 0.0081 train_MSE 0.0002; val_loss 0.0100 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0058 - train_MSE: 1.0841e-04 - val_loss: 0.0099 - val_MSE: 2.2405e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0055 - train_MSE: 1.0400e-04 - val_loss: 0.0100 - val_MSE: 2.2588e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0079e-04 - val_loss: 0.0096 - val_MSE: 2.1940e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0054 - train_MSE: 1.0228e-04 - val_loss: 0.0093 - val_MSE: 2.1460e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 9.9468e-05 - val_loss: 0.0090 - val_MSE: 2.1207e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 9.6529e-05 - val_loss: 0.0089 - val_MSE: 2.0933e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.4323e-05 - val_loss: 0.0087 - val_MSE: 2.0662e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 9.4176e-05 - val_loss: 0.0086 - val_MSE: 2.0511e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0046 - train_MSE: 9.3533e-05 - val_loss: 0.0085 - val_MSE: 2.0369e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0046 - train_MSE: 9.3681e-05 - val_loss: 0.0084 - val_MSE: 2.0282e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0046 - train_MSE: 9.2601e-05 - val_loss: 0.0083 - val_MSE: 2.0225e-04\n",
      "epoch:1460/1500 - 耗时:0.03分/总46.80分; train_loss 0.0073 train_MSE 0.0002; val_loss 0.0082 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 9.3976e-05 - val_loss: 0.0083 - val_MSE: 2.0225e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 9.3709e-05 - val_loss: 0.0082 - val_MSE: 2.0256e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 9.4042e-05 - val_loss: 0.0082 - val_MSE: 2.0224e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 9.4562e-05 - val_loss: 0.0082 - val_MSE: 2.0226e-04\n",
      "116/116 [==============================] - 2s 16ms/step - train_loss: 0.0047 - train_MSE: 9.4826e-05 - val_loss: 0.0082 - val_MSE: 2.0283e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.5353e-05 - val_loss: 0.0082 - val_MSE: 2.0261e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 9.6115e-05 - val_loss: 0.0082 - val_MSE: 2.0432e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 9.8815e-05 - val_loss: 0.0084 - val_MSE: 2.0689e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 1.0123e-04 - val_loss: 0.0085 - val_MSE: 2.0995e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0052 - train_MSE: 1.0314e-04 - val_loss: 0.0092 - val_MSE: 2.2326e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0051 - train_MSE: 1.0077e-04 - val_loss: 0.0095 - val_MSE: 2.2928e-04\n",
      "epoch:1470/1500 - 耗时:0.03分/总47.09分; train_loss 0.0079 train_MSE 0.0002; val_loss 0.0097 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0051 - train_MSE: 1.0218e-04 - val_loss: 0.0095 - val_MSE: 2.2929e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 9.9806e-05 - val_loss: 0.0097 - val_MSE: 2.3239e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 9.9478e-05 - val_loss: 0.0095 - val_MSE: 2.2815e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 9.9051e-05 - val_loss: 0.0095 - val_MSE: 2.2796e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 9.7302e-05 - val_loss: 0.0093 - val_MSE: 2.2323e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 9.7914e-05 - val_loss: 0.0091 - val_MSE: 2.1759e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 9.5918e-05 - val_loss: 0.0090 - val_MSE: 2.1724e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 9.5127e-05 - val_loss: 0.0090 - val_MSE: 2.1707e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0046 - train_MSE: 9.5435e-05 - val_loss: 0.0091 - val_MSE: 2.1989e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.7251e-05 - val_loss: 0.0095 - val_MSE: 2.2689e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0048 - train_MSE: 9.7464e-05 - val_loss: 0.0099 - val_MSE: 2.3599e-04\n",
      "epoch:1480/1500 - 耗时:0.03分/总47.37分; train_loss 0.0077 train_MSE 0.0002; val_loss 0.0101 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 9.8915e-05 - val_loss: 0.0099 - val_MSE: 2.3601e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 9.8635e-05 - val_loss: 0.0101 - val_MSE: 2.4019e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.7589e-05 - val_loss: 0.0099 - val_MSE: 2.3719e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.6900e-05 - val_loss: 0.0098 - val_MSE: 2.3295e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0048 - train_MSE: 9.6309e-05 - val_loss: 0.0095 - val_MSE: 2.2476e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.5634e-05 - val_loss: 0.0093 - val_MSE: 2.1771e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.4835e-05 - val_loss: 0.0092 - val_MSE: 2.1644e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.4802e-05 - val_loss: 0.0092 - val_MSE: 2.1709e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.5457e-05 - val_loss: 0.0093 - val_MSE: 2.1927e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.6558e-05 - val_loss: 0.0094 - val_MSE: 2.2076e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0048 - train_MSE: 9.7267e-05 - val_loss: 0.0094 - val_MSE: 2.2155e-04\n",
      "epoch:1490/1500 - 耗时:0.03分/总47.65分; train_loss 0.0075 train_MSE 0.0002; val_loss 0.0095 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 9.8217e-05 - val_loss: 0.0094 - val_MSE: 2.2156e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0049 - train_MSE: 9.8992e-05 - val_loss: 0.0095 - val_MSE: 2.2230e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0050 - train_MSE: 9.8985e-05 - val_loss: 0.0093 - val_MSE: 2.1752e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.5922e-05 - val_loss: 0.0092 - val_MSE: 2.1322e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0047 - train_MSE: 9.4973e-05 - val_loss: 0.0090 - val_MSE: 2.0744e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0048 - train_MSE: 9.4776e-05 - val_loss: 0.0087 - val_MSE: 2.0149e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0049 - train_MSE: 9.5839e-05 - val_loss: 0.0085 - val_MSE: 1.9854e-04\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0050 - train_MSE: 9.7368e-05 - val_loss: 0.0084 - val_MSE: 1.9634e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0053 - train_MSE: 1.0105e-04 - val_loss: 0.0083 - val_MSE: 1.9510e-04\n",
      "116/116 [==============================] - 2s 14ms/step - train_loss: 0.0056 - train_MSE: 1.0459e-04 - val_loss: 0.0083 - val_MSE: 1.9426e-04\n",
      "116/116 [==============================] - ETA: 0s - train_loss: 0.0057 - train_MSE: 1.0347e-04 - val_loss: 0.0085 - val_MSE: 1.9529e-04\n",
      "epoch:1500/1500 - 耗时:0.03分/总47.94分; train_loss 0.0078 train_MSE 0.0002; val_loss 0.0088 val_MSE 0.0002\n",
      "116/116 [==============================] - 2s 15ms/step - train_loss: 0.0057 - train_MSE: 1.0601e-04 - val_loss: 0.0085 - val_MSE: 1.9531e-04\n",
      "\n",
      "epoch:1500/1500 - 共耗时:47.94分,正常结束\n"
     ]
    }
   ],
   "source": [
    "# 4) epoch循环训练:\n",
    "EPOCHS = 1500\n",
    "train_loss_results = []\n",
    "logs = {}  # 字典\n",
    "wait = 0\n",
    "best = np.infty  # 先设置一个无穷大的数字;\n",
    "patience = 80\n",
    "\n",
    "train_start = time.time()\n",
    "callbacks.on_train_begin(logs=logs)\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    callbacks.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_MSE.reset_states()\n",
    "\n",
    "    for batch, batch_data in enumerate(dataset_all): #以全部的数据来做训练\n",
    "        callbacks.on_batch_begin(batch, logs=logs)\n",
    "        callbacks.on_train_batch_begin(batch, logs=logs)\n",
    "        train_dict = train_step(batch_data)\n",
    "        logs[\"train_loss\"] = train_dict[\"train_loss\"]\n",
    "        logs[\"train_MSE\"] = train_dict[\"train_MSE\"]\n",
    "\n",
    "        # if batch % 10 == 0:  # 每n次batch,打印\n",
    "        #     print(\n",
    "        #         \"Epoch {} Batch {} train_loss {:.4f} train_MSE {:.4f} \".format(\n",
    "        #             epoch + 1, batch, train_loss.result(), train_MSE.result()\n",
    "        #         )\n",
    "        #     )\n",
    "        callbacks.on_train_batch_end(batch, logs=logs)\n",
    "        callbacks.on_batch_end(batch, logs=logs)\n",
    "    # End epoch 每个epoch\n",
    "    train_loss_results.append(train_loss.result())\n",
    "\n",
    "    val_loss.reset_states()\n",
    "    val_MSE.reset_states()\n",
    "    for batch_data in dataset_val:\n",
    "        callbacks.on_batch_begin(batch, logs=logs)\n",
    "        callbacks.on_test_batch_begin(batch, logs=logs)\n",
    "        test_step(batch_data)\n",
    "        callbacks.on_test_batch_end(batch, logs=logs)\n",
    "        callbacks.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    logs[\"val_loss\"] = val_loss.result()\n",
    "    logs[\"val_MSE\"] = val_MSE.result()\n",
    "    # --------------------\n",
    "    # The early stopping strategy: stop the training if `val_loss` does not\n",
    "    # decrease over a certain number of epochs.\n",
    "    wait += 1\n",
    "    if (\n",
    "        val_loss.result() < best\n",
    "    ):  # 当loss变小,在改进时,计数器恢复为0,存储模型,实现总是存储最佳模型; 当n次loss不再变小,即触发\n",
    "        best = val_loss.result()\n",
    "        wait = 0\n",
    "\n",
    "        ckpt_save_path = ckpt_manager.save()  # 存weight\n",
    "        # print(\"Saving checkpoint for epoch {} at {}\".format(epoch + 1, ckpt_save_path))\n",
    "    if wait >= patience:\n",
    "        print(\n",
    "            \"\\nepoch:{}/{} - 共耗时:{:.2f}分,历{}次训练未见val_loss减少,故提前中止\".format(\n",
    "                epoch + 1, EPOCHS, (time.time() - train_start) / 60, patience\n",
    "            )\n",
    "        )\n",
    "        break\n",
    "    # --------------------\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:  # 每n次epoch,打印\n",
    "\n",
    "        print(\n",
    "            \"\\nepoch:{}/{} - 耗时:{:.2f}分/总{:.2f}分; train_loss {:.4f} train_MSE {:.4f}; val_loss {:.4f} val_MSE {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                EPOCHS,\n",
    "                (time.time() - epoch_start) / 60,\n",
    "                (time.time() - train_start) / 60,\n",
    "                train_loss.result(),\n",
    "                train_MSE.result(),\n",
    "                val_loss.result(),\n",
    "                val_MSE.result(),\n",
    "            )\n",
    "        )\n",
    "    # print(\"Time taken for 1 epoch: {} mins\\n\".format((time.time() - start) / 60))\n",
    "    callbacks.on_epoch_end(epoch, logs=logs)\n",
    "callbacks.on_train_end(logs=logs)\n",
    "print(\n",
    "    \"\\nepoch:{}/{} - 共耗时:{:.2f}分,正常结束\".format(\n",
    "        epoch + 1, EPOCHS, (time.time() - train_start) / 60\n",
    "    )\n",
    ")\n",
    "\n",
    "# 在Docker上运行时,用于JupyterLab上自动关闭租用连接:\n",
    "# import os\n",
    "# 若释放前要保存环境并命名为 SnapName\n",
    "# os.system(\"export $(cat /proc/1/environ |tr '\\\\0' '\\\\n' | grep MATCLOUD_CANCELTOKEN)&&/public/script/matncli node cancel -url https://matpool.com/api/public/node -save -name snapName\")\n",
    "# 若释放前不需要保存环境\n",
    "# os.system(\"export $(cat /proc/1/environ |tr '\\\\0' '\\\\n' | grep MATCLOUD_CANCELTOKEN)&&/public/script/matncli node cancel -url https://matpool.com/api/public/node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40319ac7-f285-4704-8d3b-af815f7ba67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLinear = LTSF_DLinear(\n",
    "#     seq_len=L_seq,\n",
    "#     pred_len=L_pred,\n",
    "#     channels=14,\n",
    "#     kernel_size=25,\n",
    "#     individual=False,\n",
    "# )\n",
    "# DLinear(iter(dataset_train).next()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484efed-0435-4465-a3d5-5b9487d4e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ckpt_path = ckpt_manager.latest_checkpoint  # 提取checkpoint目录中存储的最新的checkpoint\n",
    "print(\"最近存储的checkpoint: {}\".format(saved_ckpt_path))\n",
    "ckpt.restore(\"./checkpoint/BTC_F20_221202-161\").expect_partial()\n",
    "# ckpt.restore(saved_ckpt_path).assert_consumed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4e453ea-f084-445b-aedc-c7aaa513a006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training=False,val_loss:0.0088,val_MSE:0.0002\n",
      "y_hat.shape:(704, 7, 1)\n",
      "y_true.shape:(3956, 1)\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 717 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#用验证集评估模型,并获得验证集的预测值,以及y_Scaler缩放之前的值:\n",
    "Modelevaluation(dataset_val)\n",
    "#获取向前创建的dataset_predict数据的预测;dataset_preict包含5*Batch_size组数据,预测出(5*Batch_size,7,1)个数据来\n",
    "y_hat = np.zeros((Batch_size,L_pred,out_features))\n",
    "# print(y_hat.shape)\n",
    "for data_hat in dataset_predict:\n",
    "    y_hat_ = DLinear(data_hat)\n",
    "    # print(y_hat_.shape)\n",
    "    y_hat_inverse = y_Scaler.inverse_transform(y_hat_[:,:,0])\n",
    "    y_hat = np.concatenate([y_hat,np.expand_dims(y_hat_inverse,-1)],axis=0)\n",
    "y_hat = y_hat[Batch_size:] #初始是Batch_size个0;抛去;\n",
    "\n",
    "# y_hat_end = y_hat_.shape[0]\n",
    "print('y_hat.shape:{}'.format(y_hat.shape))\n",
    "\n",
    "# 真实值:\n",
    "y_true = y_Scaler.inverse_transform(data[:,-1:])\n",
    "print('y_true.shape:{}'.format(y_true.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bba06-b553-4afa-a284-0a22b49c531f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9237e3-fdb8-4886-870d-91154a4bbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly.io.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c12694c-470f-4311-8944-122cb509e720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "真实值",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2022-02-15T20:00:00+08:00",
          "2022-02-16T08:00:00+08:00",
          "2022-02-16T20:00:00+08:00",
          "2022-02-17T08:00:00+08:00",
          "2022-02-17T20:00:00+08:00",
          "2022-02-18T08:00:00+08:00",
          "2022-02-18T20:00:00+08:00",
          "2022-02-19T08:00:00+08:00",
          "2022-02-19T20:00:00+08:00",
          "2022-02-20T08:00:00+08:00",
          "2022-02-20T20:00:00+08:00",
          "2022-02-21T08:00:00+08:00",
          "2022-02-21T20:00:00+08:00",
          "2022-02-22T08:00:00+08:00",
          "2022-02-22T20:00:00+08:00",
          "2022-02-23T08:00:00+08:00",
          "2022-02-23T20:00:00+08:00",
          "2022-02-24T08:00:00+08:00",
          "2022-02-24T20:00:00+08:00",
          "2022-02-25T08:00:00+08:00",
          "2022-02-25T20:00:00+08:00",
          "2022-02-26T08:00:00+08:00",
          "2022-02-26T20:00:00+08:00",
          "2022-02-27T08:00:00+08:00",
          "2022-02-27T20:00:00+08:00",
          "2022-02-28T08:00:00+08:00",
          "2022-02-28T20:00:00+08:00",
          "2022-03-01T08:00:00+08:00",
          "2022-03-01T20:00:00+08:00",
          "2022-03-02T08:00:00+08:00",
          "2022-03-02T20:00:00+08:00",
          "2022-03-03T08:00:00+08:00",
          "2022-03-03T20:00:00+08:00",
          "2022-03-04T08:00:00+08:00",
          "2022-03-04T20:00:00+08:00",
          "2022-03-05T08:00:00+08:00",
          "2022-03-05T20:00:00+08:00",
          "2022-03-06T08:00:00+08:00",
          "2022-03-06T20:00:00+08:00",
          "2022-03-07T08:00:00+08:00",
          "2022-03-07T20:00:00+08:00",
          "2022-03-08T08:00:00+08:00",
          "2022-03-08T20:00:00+08:00",
          "2022-03-09T08:00:00+08:00",
          "2022-03-09T20:00:00+08:00",
          "2022-03-10T08:00:00+08:00",
          "2022-03-10T20:00:00+08:00",
          "2022-03-11T08:00:00+08:00",
          "2022-03-11T20:00:00+08:00",
          "2022-03-12T08:00:00+08:00",
          "2022-03-12T20:00:00+08:00",
          "2022-03-13T08:00:00+08:00",
          "2022-03-13T20:00:00+08:00",
          "2022-03-14T08:00:00+08:00",
          "2022-03-14T20:00:00+08:00",
          "2022-03-15T08:00:00+08:00",
          "2022-03-15T20:00:00+08:00",
          "2022-03-16T08:00:00+08:00",
          "2022-03-16T20:00:00+08:00",
          "2022-03-17T08:00:00+08:00",
          "2022-03-17T20:00:00+08:00",
          "2022-03-18T08:00:00+08:00",
          "2022-03-18T20:00:00+08:00",
          "2022-03-19T08:00:00+08:00",
          "2022-03-19T20:00:00+08:00",
          "2022-03-20T08:00:00+08:00",
          "2022-03-20T20:00:00+08:00",
          "2022-03-21T08:00:00+08:00",
          "2022-03-21T20:00:00+08:00",
          "2022-03-22T08:00:00+08:00",
          "2022-03-22T20:00:00+08:00",
          "2022-03-23T08:00:00+08:00",
          "2022-03-23T20:00:00+08:00",
          "2022-03-24T08:00:00+08:00",
          "2022-03-24T20:00:00+08:00",
          "2022-03-25T08:00:00+08:00",
          "2022-03-25T20:00:00+08:00",
          "2022-03-26T08:00:00+08:00",
          "2022-03-26T20:00:00+08:00",
          "2022-03-27T08:00:00+08:00",
          "2022-03-27T20:00:00+08:00",
          "2022-03-28T08:00:00+08:00",
          "2022-03-28T20:00:00+08:00",
          "2022-03-29T08:00:00+08:00",
          "2022-03-29T20:00:00+08:00",
          "2022-03-30T08:00:00+08:00",
          "2022-03-30T20:00:00+08:00",
          "2022-03-31T08:00:00+08:00",
          "2022-03-31T20:00:00+08:00",
          "2022-04-01T08:00:00+08:00",
          "2022-04-01T20:00:00+08:00",
          "2022-04-02T08:00:00+08:00",
          "2022-04-02T20:00:00+08:00",
          "2022-04-03T08:00:00+08:00",
          "2022-04-03T20:00:00+08:00",
          "2022-04-04T08:00:00+08:00",
          "2022-04-04T20:00:00+08:00",
          "2022-04-05T08:00:00+08:00",
          "2022-04-05T20:00:00+08:00",
          "2022-04-06T08:00:00+08:00",
          "2022-04-06T20:00:00+08:00",
          "2022-04-07T08:00:00+08:00",
          "2022-04-07T20:00:00+08:00",
          "2022-04-08T08:00:00+08:00",
          "2022-04-08T20:00:00+08:00",
          "2022-04-09T08:00:00+08:00",
          "2022-04-09T20:00:00+08:00",
          "2022-04-10T08:00:00+08:00",
          "2022-04-10T20:00:00+08:00",
          "2022-04-11T08:00:00+08:00",
          "2022-04-11T20:00:00+08:00",
          "2022-04-12T08:00:00+08:00",
          "2022-04-12T20:00:00+08:00",
          "2022-04-13T08:00:00+08:00",
          "2022-04-13T20:00:00+08:00",
          "2022-04-14T08:00:00+08:00",
          "2022-04-14T20:00:00+08:00",
          "2022-04-15T08:00:00+08:00",
          "2022-04-15T20:00:00+08:00",
          "2022-04-16T08:00:00+08:00",
          "2022-04-16T20:00:00+08:00",
          "2022-04-17T08:00:00+08:00",
          "2022-04-17T20:00:00+08:00",
          "2022-04-18T08:00:00+08:00",
          "2022-04-18T20:00:00+08:00",
          "2022-04-19T08:00:00+08:00",
          "2022-04-19T20:00:00+08:00",
          "2022-04-20T08:00:00+08:00",
          "2022-04-20T20:00:00+08:00",
          "2022-04-21T08:00:00+08:00",
          "2022-04-21T20:00:00+08:00",
          "2022-04-22T08:00:00+08:00",
          "2022-04-22T20:00:00+08:00",
          "2022-04-23T08:00:00+08:00",
          "2022-04-23T20:00:00+08:00",
          "2022-04-24T08:00:00+08:00",
          "2022-04-24T20:00:00+08:00",
          "2022-04-25T08:00:00+08:00",
          "2022-04-25T20:00:00+08:00",
          "2022-04-26T08:00:00+08:00",
          "2022-04-26T20:00:00+08:00",
          "2022-04-27T08:00:00+08:00",
          "2022-04-27T20:00:00+08:00",
          "2022-04-28T08:00:00+08:00",
          "2022-04-28T20:00:00+08:00",
          "2022-04-29T08:00:00+08:00",
          "2022-04-29T20:00:00+08:00",
          "2022-04-30T08:00:00+08:00",
          "2022-04-30T20:00:00+08:00",
          "2022-05-01T08:00:00+08:00",
          "2022-05-01T20:00:00+08:00",
          "2022-05-02T08:00:00+08:00",
          "2022-05-02T20:00:00+08:00",
          "2022-05-03T08:00:00+08:00",
          "2022-05-03T20:00:00+08:00",
          "2022-05-04T08:00:00+08:00",
          "2022-05-04T20:00:00+08:00",
          "2022-05-05T08:00:00+08:00",
          "2022-05-05T20:00:00+08:00",
          "2022-05-06T08:00:00+08:00",
          "2022-05-06T20:00:00+08:00",
          "2022-05-07T08:00:00+08:00",
          "2022-05-07T20:00:00+08:00",
          "2022-05-08T08:00:00+08:00",
          "2022-05-08T20:00:00+08:00",
          "2022-05-09T08:00:00+08:00",
          "2022-05-09T20:00:00+08:00",
          "2022-05-10T08:00:00+08:00",
          "2022-05-10T20:00:00+08:00",
          "2022-05-11T08:00:00+08:00",
          "2022-05-11T20:00:00+08:00",
          "2022-05-12T08:00:00+08:00",
          "2022-05-12T20:00:00+08:00",
          "2022-05-13T08:00:00+08:00",
          "2022-05-13T20:00:00+08:00",
          "2022-05-14T08:00:00+08:00",
          "2022-05-14T20:00:00+08:00",
          "2022-05-15T08:00:00+08:00",
          "2022-05-15T20:00:00+08:00",
          "2022-05-16T08:00:00+08:00",
          "2022-05-16T20:00:00+08:00",
          "2022-05-17T08:00:00+08:00",
          "2022-05-17T20:00:00+08:00",
          "2022-05-18T08:00:00+08:00",
          "2022-05-18T20:00:00+08:00",
          "2022-05-19T08:00:00+08:00",
          "2022-05-19T20:00:00+08:00",
          "2022-05-20T08:00:00+08:00",
          "2022-05-20T20:00:00+08:00",
          "2022-05-21T08:00:00+08:00",
          "2022-05-21T20:00:00+08:00",
          "2022-05-22T08:00:00+08:00",
          "2022-05-22T20:00:00+08:00",
          "2022-05-23T08:00:00+08:00",
          "2022-05-23T20:00:00+08:00",
          "2022-05-24T08:00:00+08:00",
          "2022-05-24T20:00:00+08:00",
          "2022-05-25T08:00:00+08:00",
          "2022-05-25T20:00:00+08:00",
          "2022-05-26T08:00:00+08:00",
          "2022-05-26T20:00:00+08:00",
          "2022-05-27T08:00:00+08:00",
          "2022-05-27T20:00:00+08:00",
          "2022-05-28T08:00:00+08:00",
          "2022-05-28T20:00:00+08:00",
          "2022-05-29T08:00:00+08:00",
          "2022-05-29T20:00:00+08:00",
          "2022-05-30T08:00:00+08:00",
          "2022-05-30T20:00:00+08:00",
          "2022-05-31T08:00:00+08:00",
          "2022-05-31T20:00:00+08:00",
          "2022-06-01T08:00:00+08:00",
          "2022-06-01T20:00:00+08:00",
          "2022-06-02T08:00:00+08:00",
          "2022-06-02T20:00:00+08:00",
          "2022-06-03T08:00:00+08:00",
          "2022-06-03T20:00:00+08:00",
          "2022-06-04T08:00:00+08:00",
          "2022-06-04T20:00:00+08:00",
          "2022-06-05T08:00:00+08:00",
          "2022-06-05T20:00:00+08:00",
          "2022-06-06T08:00:00+08:00",
          "2022-06-06T20:00:00+08:00",
          "2022-06-07T08:00:00+08:00",
          "2022-06-07T20:00:00+08:00",
          "2022-06-08T08:00:00+08:00",
          "2022-06-08T20:00:00+08:00",
          "2022-06-09T08:00:00+08:00",
          "2022-06-09T20:00:00+08:00",
          "2022-06-10T08:00:00+08:00",
          "2022-06-10T20:00:00+08:00",
          "2022-06-11T08:00:00+08:00",
          "2022-06-11T20:00:00+08:00",
          "2022-06-12T08:00:00+08:00",
          "2022-06-12T20:00:00+08:00",
          "2022-06-13T08:00:00+08:00",
          "2022-06-13T20:00:00+08:00",
          "2022-06-14T08:00:00+08:00",
          "2022-06-14T20:00:00+08:00",
          "2022-06-15T08:00:00+08:00",
          "2022-06-15T20:00:00+08:00",
          "2022-06-16T08:00:00+08:00",
          "2022-06-16T20:00:00+08:00",
          "2022-06-17T08:00:00+08:00",
          "2022-06-17T20:00:00+08:00",
          "2022-06-18T08:00:00+08:00",
          "2022-06-18T20:00:00+08:00",
          "2022-06-19T08:00:00+08:00",
          "2022-06-19T20:00:00+08:00",
          "2022-06-20T08:00:00+08:00",
          "2022-06-20T20:00:00+08:00",
          "2022-06-21T08:00:00+08:00",
          "2022-06-21T20:00:00+08:00",
          "2022-06-22T08:00:00+08:00",
          "2022-06-22T20:00:00+08:00",
          "2022-06-23T08:00:00+08:00",
          "2022-06-23T20:00:00+08:00",
          "2022-06-24T08:00:00+08:00",
          "2022-06-24T20:00:00+08:00",
          "2022-06-25T08:00:00+08:00",
          "2022-06-25T20:00:00+08:00",
          "2022-06-26T08:00:00+08:00",
          "2022-06-26T20:00:00+08:00",
          "2022-06-27T08:00:00+08:00",
          "2022-06-27T20:00:00+08:00",
          "2022-06-28T08:00:00+08:00",
          "2022-06-28T20:00:00+08:00",
          "2022-06-29T08:00:00+08:00",
          "2022-06-29T20:00:00+08:00",
          "2022-06-30T08:00:00+08:00",
          "2022-06-30T20:00:00+08:00",
          "2022-07-01T08:00:00+08:00",
          "2022-07-01T20:00:00+08:00",
          "2022-07-02T08:00:00+08:00",
          "2022-07-02T20:00:00+08:00",
          "2022-07-03T08:00:00+08:00",
          "2022-07-03T20:00:00+08:00",
          "2022-07-04T08:00:00+08:00",
          "2022-07-04T20:00:00+08:00",
          "2022-07-05T08:00:00+08:00",
          "2022-07-05T20:00:00+08:00",
          "2022-07-06T08:00:00+08:00",
          "2022-07-06T20:00:00+08:00",
          "2022-07-07T08:00:00+08:00",
          "2022-07-07T20:00:00+08:00",
          "2022-07-08T08:00:00+08:00",
          "2022-07-08T20:00:00+08:00",
          "2022-07-09T08:00:00+08:00",
          "2022-07-09T20:00:00+08:00",
          "2022-07-10T08:00:00+08:00",
          "2022-07-10T20:00:00+08:00",
          "2022-07-11T08:00:00+08:00",
          "2022-07-11T20:00:00+08:00",
          "2022-07-12T08:00:00+08:00",
          "2022-07-12T20:00:00+08:00",
          "2022-07-13T08:00:00+08:00",
          "2022-07-13T20:00:00+08:00",
          "2022-07-14T08:00:00+08:00",
          "2022-07-14T20:00:00+08:00",
          "2022-07-15T08:00:00+08:00",
          "2022-07-15T20:00:00+08:00",
          "2022-07-16T08:00:00+08:00",
          "2022-07-16T20:00:00+08:00",
          "2022-07-17T08:00:00+08:00",
          "2022-07-17T20:00:00+08:00",
          "2022-07-18T08:00:00+08:00",
          "2022-07-18T20:00:00+08:00",
          "2022-07-19T08:00:00+08:00",
          "2022-07-19T20:00:00+08:00",
          "2022-07-20T08:00:00+08:00",
          "2022-07-20T20:00:00+08:00",
          "2022-07-21T08:00:00+08:00",
          "2022-07-21T20:00:00+08:00",
          "2022-07-22T08:00:00+08:00",
          "2022-07-22T20:00:00+08:00",
          "2022-07-23T08:00:00+08:00",
          "2022-07-23T20:00:00+08:00",
          "2022-07-24T08:00:00+08:00",
          "2022-07-24T20:00:00+08:00",
          "2022-07-25T08:00:00+08:00",
          "2022-07-25T20:00:00+08:00",
          "2022-07-26T08:00:00+08:00",
          "2022-07-26T20:00:00+08:00",
          "2022-07-27T08:00:00+08:00",
          "2022-07-27T20:00:00+08:00",
          "2022-07-28T08:00:00+08:00",
          "2022-07-28T20:00:00+08:00",
          "2022-07-29T08:00:00+08:00",
          "2022-07-29T20:00:00+08:00",
          "2022-07-30T08:00:00+08:00",
          "2022-07-30T20:00:00+08:00",
          "2022-07-31T08:00:00+08:00",
          "2022-07-31T20:00:00+08:00",
          "2022-08-01T08:00:00+08:00",
          "2022-08-01T20:00:00+08:00",
          "2022-08-02T08:00:00+08:00",
          "2022-08-02T20:00:00+08:00",
          "2022-08-03T08:00:00+08:00",
          "2022-08-03T20:00:00+08:00",
          "2022-08-04T08:00:00+08:00",
          "2022-08-04T20:00:00+08:00",
          "2022-08-05T08:00:00+08:00",
          "2022-08-05T20:00:00+08:00",
          "2022-08-06T08:00:00+08:00",
          "2022-08-06T20:00:00+08:00",
          "2022-08-07T08:00:00+08:00",
          "2022-08-07T20:00:00+08:00",
          "2022-08-08T08:00:00+08:00",
          "2022-08-08T20:00:00+08:00",
          "2022-08-09T08:00:00+08:00",
          "2022-08-09T20:00:00+08:00",
          "2022-08-10T08:00:00+08:00",
          "2022-08-10T20:00:00+08:00",
          "2022-08-11T08:00:00+08:00",
          "2022-08-11T20:00:00+08:00",
          "2022-08-12T08:00:00+08:00",
          "2022-08-12T20:00:00+08:00",
          "2022-08-13T08:00:00+08:00",
          "2022-08-13T20:00:00+08:00",
          "2022-08-14T08:00:00+08:00",
          "2022-08-14T20:00:00+08:00",
          "2022-08-15T08:00:00+08:00",
          "2022-08-15T20:00:00+08:00",
          "2022-08-16T08:00:00+08:00",
          "2022-08-16T20:00:00+08:00",
          "2022-08-17T08:00:00+08:00",
          "2022-08-17T20:00:00+08:00",
          "2022-08-18T08:00:00+08:00",
          "2022-08-18T20:00:00+08:00",
          "2022-08-19T08:00:00+08:00",
          "2022-08-19T20:00:00+08:00",
          "2022-08-20T08:00:00+08:00",
          "2022-08-20T20:00:00+08:00",
          "2022-08-21T08:00:00+08:00",
          "2022-08-21T20:00:00+08:00",
          "2022-08-22T08:00:00+08:00",
          "2022-08-22T20:00:00+08:00",
          "2022-08-23T08:00:00+08:00",
          "2022-08-23T20:00:00+08:00",
          "2022-08-24T08:00:00+08:00",
          "2022-08-24T20:00:00+08:00",
          "2022-08-25T08:00:00+08:00",
          "2022-08-25T20:00:00+08:00",
          "2022-08-26T08:00:00+08:00",
          "2022-08-26T20:00:00+08:00",
          "2022-08-27T08:00:00+08:00",
          "2022-08-27T20:00:00+08:00",
          "2022-08-28T08:00:00+08:00",
          "2022-08-28T20:00:00+08:00",
          "2022-08-29T08:00:00+08:00",
          "2022-08-29T20:00:00+08:00",
          "2022-08-30T08:00:00+08:00",
          "2022-08-30T20:00:00+08:00",
          "2022-08-31T08:00:00+08:00",
          "2022-08-31T20:00:00+08:00",
          "2022-09-01T08:00:00+08:00",
          "2022-09-01T20:00:00+08:00",
          "2022-09-02T08:00:00+08:00",
          "2022-09-02T20:00:00+08:00",
          "2022-09-03T08:00:00+08:00",
          "2022-09-03T20:00:00+08:00",
          "2022-09-04T08:00:00+08:00",
          "2022-09-04T20:00:00+08:00",
          "2022-09-05T08:00:00+08:00",
          "2022-09-05T20:00:00+08:00",
          "2022-09-06T08:00:00+08:00",
          "2022-09-06T20:00:00+08:00",
          "2022-09-07T08:00:00+08:00",
          "2022-09-07T20:00:00+08:00",
          "2022-09-08T08:00:00+08:00",
          "2022-09-08T20:00:00+08:00",
          "2022-09-09T08:00:00+08:00",
          "2022-09-09T20:00:00+08:00",
          "2022-09-10T08:00:00+08:00",
          "2022-09-10T20:00:00+08:00",
          "2022-09-11T08:00:00+08:00",
          "2022-09-11T20:00:00+08:00",
          "2022-09-12T08:00:00+08:00",
          "2022-09-12T20:00:00+08:00",
          "2022-09-13T08:00:00+08:00",
          "2022-09-13T20:00:00+08:00",
          "2022-09-14T08:00:00+08:00",
          "2022-09-14T20:00:00+08:00",
          "2022-09-15T08:00:00+08:00",
          "2022-09-15T20:00:00+08:00",
          "2022-09-16T08:00:00+08:00",
          "2022-09-16T20:00:00+08:00",
          "2022-09-17T08:00:00+08:00",
          "2022-09-17T20:00:00+08:00",
          "2022-09-18T08:00:00+08:00",
          "2022-09-18T20:00:00+08:00",
          "2022-09-19T08:00:00+08:00",
          "2022-09-19T20:00:00+08:00",
          "2022-09-20T08:00:00+08:00",
          "2022-09-20T20:00:00+08:00",
          "2022-09-21T08:00:00+08:00",
          "2022-09-21T20:00:00+08:00",
          "2022-09-22T08:00:00+08:00",
          "2022-09-22T20:00:00+08:00",
          "2022-09-23T08:00:00+08:00",
          "2022-09-23T20:00:00+08:00",
          "2022-09-24T08:00:00+08:00",
          "2022-09-24T20:00:00+08:00",
          "2022-09-25T08:00:00+08:00",
          "2022-09-25T20:00:00+08:00",
          "2022-09-26T08:00:00+08:00",
          "2022-09-26T20:00:00+08:00",
          "2022-09-27T08:00:00+08:00",
          "2022-09-27T20:00:00+08:00",
          "2022-09-28T08:00:00+08:00",
          "2022-09-28T20:00:00+08:00",
          "2022-09-29T08:00:00+08:00",
          "2022-09-29T20:00:00+08:00",
          "2022-09-30T08:00:00+08:00",
          "2022-09-30T20:00:00+08:00",
          "2022-10-01T08:00:00+08:00",
          "2022-10-01T20:00:00+08:00",
          "2022-10-02T08:00:00+08:00",
          "2022-10-02T20:00:00+08:00",
          "2022-10-03T08:00:00+08:00",
          "2022-10-03T20:00:00+08:00",
          "2022-10-04T08:00:00+08:00",
          "2022-10-04T20:00:00+08:00",
          "2022-10-05T08:00:00+08:00",
          "2022-10-05T20:00:00+08:00",
          "2022-10-06T08:00:00+08:00",
          "2022-10-06T20:00:00+08:00",
          "2022-10-07T08:00:00+08:00",
          "2022-10-07T20:00:00+08:00",
          "2022-10-08T08:00:00+08:00",
          "2022-10-08T20:00:00+08:00",
          "2022-10-09T08:00:00+08:00",
          "2022-10-09T20:00:00+08:00",
          "2022-10-10T08:00:00+08:00",
          "2022-10-10T20:00:00+08:00",
          "2022-10-11T08:00:00+08:00",
          "2022-10-11T20:00:00+08:00",
          "2022-10-12T08:00:00+08:00",
          "2022-10-12T20:00:00+08:00",
          "2022-10-13T08:00:00+08:00",
          "2022-10-13T20:00:00+08:00",
          "2022-10-14T08:00:00+08:00",
          "2022-10-14T20:00:00+08:00",
          "2022-10-15T08:00:00+08:00",
          "2022-10-15T20:00:00+08:00",
          "2022-10-16T08:00:00+08:00",
          "2022-10-16T20:00:00+08:00",
          "2022-10-17T08:00:00+08:00",
          "2022-10-17T20:00:00+08:00",
          "2022-10-18T08:00:00+08:00",
          "2022-10-18T20:00:00+08:00",
          "2022-10-19T08:00:00+08:00",
          "2022-10-19T20:00:00+08:00",
          "2022-10-20T08:00:00+08:00",
          "2022-10-20T20:00:00+08:00",
          "2022-10-21T08:00:00+08:00",
          "2022-10-21T20:00:00+08:00",
          "2022-10-22T08:00:00+08:00",
          "2022-10-22T20:00:00+08:00",
          "2022-10-23T08:00:00+08:00",
          "2022-10-23T20:00:00+08:00",
          "2022-10-24T08:00:00+08:00",
          "2022-10-24T20:00:00+08:00",
          "2022-10-25T08:00:00+08:00",
          "2022-10-25T20:00:00+08:00",
          "2022-10-26T08:00:00+08:00",
          "2022-10-26T20:00:00+08:00",
          "2022-10-27T08:00:00+08:00",
          "2022-10-27T20:00:00+08:00",
          "2022-10-28T08:00:00+08:00",
          "2022-10-28T20:00:00+08:00",
          "2022-10-29T08:00:00+08:00",
          "2022-10-29T20:00:00+08:00",
          "2022-10-30T08:00:00+08:00",
          "2022-10-30T20:00:00+08:00",
          "2022-10-31T08:00:00+08:00",
          "2022-10-31T20:00:00+08:00",
          "2022-11-01T08:00:00+08:00",
          "2022-11-01T20:00:00+08:00",
          "2022-11-02T08:00:00+08:00",
          "2022-11-02T20:00:00+08:00",
          "2022-11-03T08:00:00+08:00",
          "2022-11-03T20:00:00+08:00",
          "2022-11-04T08:00:00+08:00",
          "2022-11-04T20:00:00+08:00",
          "2022-11-05T08:00:00+08:00",
          "2022-11-05T20:00:00+08:00",
          "2022-11-06T08:00:00+08:00",
          "2022-11-06T20:00:00+08:00",
          "2022-11-07T08:00:00+08:00",
          "2022-11-07T20:00:00+08:00",
          "2022-11-08T08:00:00+08:00",
          "2022-11-08T20:00:00+08:00",
          "2022-11-09T08:00:00+08:00",
          "2022-11-09T20:00:00+08:00",
          "2022-11-10T08:00:00+08:00",
          "2022-11-10T20:00:00+08:00",
          "2022-11-11T08:00:00+08:00",
          "2022-11-11T20:00:00+08:00",
          "2022-11-12T08:00:00+08:00",
          "2022-11-12T20:00:00+08:00",
          "2022-11-13T08:00:00+08:00",
          "2022-11-13T20:00:00+08:00",
          "2022-11-14T08:00:00+08:00",
          "2022-11-14T20:00:00+08:00",
          "2022-11-15T08:00:00+08:00",
          "2022-11-15T20:00:00+08:00",
          "2022-11-16T08:00:00+08:00",
          "2022-11-16T20:00:00+08:00",
          "2022-11-17T08:00:00+08:00",
          "2022-11-17T20:00:00+08:00",
          "2022-11-18T08:00:00+08:00",
          "2022-11-18T20:00:00+08:00",
          "2022-11-19T08:00:00+08:00",
          "2022-11-19T20:00:00+08:00",
          "2022-11-20T08:00:00+08:00",
          "2022-11-20T20:00:00+08:00",
          "2022-11-21T08:00:00+08:00",
          "2022-11-21T20:00:00+08:00",
          "2022-11-22T08:00:00+08:00",
          "2022-11-22T20:00:00+08:00",
          "2022-11-23T08:00:00+08:00",
          "2022-11-23T20:00:00+08:00",
          "2022-11-24T08:00:00+08:00",
          "2022-11-24T20:00:00+08:00",
          "2022-11-25T08:00:00+08:00",
          "2022-11-25T20:00:00+08:00",
          "2022-11-26T08:00:00+08:00",
          "2022-11-26T20:00:00+08:00",
          "2022-11-27T08:00:00+08:00",
          "2022-11-27T20:00:00+08:00",
          "2022-11-28T08:00:00+08:00",
          "2022-11-28T20:00:00+08:00",
          "2022-11-29T08:00:00+08:00",
          "2022-11-29T20:00:00+08:00",
          "2022-11-30T08:00:00+08:00",
          "2022-11-30T20:00:00+08:00",
          "2022-12-01T08:00:00+08:00",
          "2022-12-01T20:00:00+08:00",
          "2022-12-02T08:00:00+08:00",
          "2022-12-02T20:00:00+08:00",
          "2022-12-03T08:00:00+08:00",
          "2022-12-03T20:00:00+08:00",
          "2022-12-04T08:00:00+08:00",
          "2022-12-04T20:00:00+08:00",
          "2022-12-05T08:00:00+08:00",
          "2022-12-05T20:00:00+08:00",
          "2022-12-06T08:00:00+08:00",
          "2022-12-06T20:00:00+08:00",
          "2022-12-07T08:00:00+08:00",
          "2022-12-07T20:00:00+08:00",
          "2022-12-08T08:00:00+08:00",
          "2022-12-08T20:00:00+08:00",
          "2022-12-09T08:00:00+08:00",
          "2022-12-09T20:00:00+08:00",
          "2022-12-10T08:00:00+08:00",
          "2022-12-10T20:00:00+08:00",
          "2022-12-11T08:00:00+08:00",
          "2022-12-11T20:00:00+08:00",
          "2022-12-12T08:00:00+08:00",
          "2022-12-12T20:00:00+08:00",
          "2022-12-13T08:00:00+08:00",
          "2022-12-13T20:00:00+08:00",
          "2022-12-14T08:00:00+08:00",
          "2022-12-14T20:00:00+08:00",
          "2022-12-15T08:00:00+08:00",
          "2022-12-15T20:00:00+08:00",
          "2022-12-16T08:00:00+08:00",
          "2022-12-16T20:00:00+08:00",
          "2022-12-17T08:00:00+08:00",
          "2022-12-17T20:00:00+08:00",
          "2022-12-18T08:00:00+08:00",
          "2022-12-18T20:00:00+08:00",
          "2022-12-19T08:00:00+08:00",
          "2022-12-19T20:00:00+08:00",
          "2022-12-20T08:00:00+08:00",
          "2022-12-20T20:00:00+08:00",
          "2022-12-21T08:00:00+08:00",
          "2022-12-21T20:00:00+08:00",
          "2022-12-22T08:00:00+08:00",
          "2022-12-22T20:00:00+08:00",
          "2022-12-23T08:00:00+08:00",
          "2022-12-23T20:00:00+08:00",
          "2022-12-24T08:00:00+08:00",
          "2022-12-24T20:00:00+08:00",
          "2022-12-25T08:00:00+08:00",
          "2022-12-25T20:00:00+08:00",
          "2022-12-26T08:00:00+08:00",
          "2022-12-26T20:00:00+08:00",
          "2022-12-27T08:00:00+08:00",
          "2022-12-27T20:00:00+08:00",
          "2022-12-28T08:00:00+08:00",
          "2022-12-28T20:00:00+08:00",
          "2022-12-29T08:00:00+08:00",
          "2022-12-29T20:00:00+08:00",
          "2022-12-30T08:00:00+08:00",
          "2022-12-30T20:00:00+08:00",
          "2022-12-31T08:00:00+08:00",
          "2022-12-31T20:00:00+08:00",
          "2023-01-01T08:00:00+08:00",
          "2023-01-01T20:00:00+08:00",
          "2023-01-02T08:00:00+08:00",
          "2023-01-02T20:00:00+08:00",
          "2023-01-03T08:00:00+08:00",
          "2023-01-03T20:00:00+08:00",
          "2023-01-04T08:00:00+08:00",
          "2023-01-04T20:00:00+08:00",
          "2023-01-05T08:00:00+08:00",
          "2023-01-05T20:00:00+08:00",
          "2023-01-06T08:00:00+08:00",
          "2023-01-06T20:00:00+08:00",
          "2023-01-07T08:00:00+08:00",
          "2023-01-07T20:00:00+08:00",
          "2023-01-08T08:00:00+08:00",
          "2023-01-08T20:00:00+08:00",
          "2023-01-09T08:00:00+08:00",
          "2023-01-09T20:00:00+08:00",
          "2023-01-10T08:00:00+08:00",
          "2023-01-10T20:00:00+08:00",
          "2023-01-11T08:00:00+08:00",
          "2023-01-11T20:00:00+08:00",
          "2023-01-12T08:00:00+08:00",
          "2023-01-12T20:00:00+08:00",
          "2023-01-13T08:00:00+08:00",
          "2023-01-13T20:00:00+08:00",
          "2023-01-14T08:00:00+08:00",
          "2023-01-14T20:00:00+08:00",
          "2023-01-15T08:00:00+08:00",
          "2023-01-15T20:00:00+08:00",
          "2023-01-16T08:00:00+08:00",
          "2023-01-16T20:00:00+08:00",
          "2023-01-17T08:00:00+08:00",
          "2023-01-17T20:00:00+08:00",
          "2023-01-18T08:00:00+08:00",
          "2023-01-18T20:00:00+08:00",
          "2023-01-19T08:00:00+08:00",
          "2023-01-19T20:00:00+08:00",
          "2023-01-20T08:00:00+08:00",
          "2023-01-20T20:00:00+08:00",
          "2023-01-21T08:00:00+08:00",
          "2023-01-21T20:00:00+08:00",
          "2023-01-22T08:00:00+08:00",
          "2023-01-22T20:00:00+08:00",
          "2023-01-23T08:00:00+08:00",
          "2023-01-23T20:00:00+08:00",
          "2023-01-24T08:00:00+08:00",
          "2023-01-24T20:00:00+08:00",
          "2023-01-25T08:00:00+08:00",
          "2023-01-25T20:00:00+08:00",
          "2023-01-26T08:00:00+08:00",
          "2023-01-26T20:00:00+08:00",
          "2023-01-27T08:00:00+08:00",
          "2023-01-27T20:00:00+08:00",
          "2023-01-28T08:00:00+08:00",
          "2023-01-28T20:00:00+08:00",
          "2023-01-29T08:00:00+08:00",
          "2023-01-29T20:00:00+08:00",
          "2023-01-30T08:00:00+08:00",
          "2023-01-30T20:00:00+08:00",
          "2023-01-31T08:00:00+08:00",
          "2023-01-31T20:00:00+08:00",
          "2023-02-01T08:00:00+08:00",
          "2023-02-01T20:00:00+08:00",
          "2023-02-02T08:00:00+08:00"
         ],
         "xhoverformat": "%y/%m/%d_%H:00",
         "y": [
          44544.859375,
          44173.1484375,
          43873.55859375,
          43164.05078125,
          40515.69921875,
          40178.94140625,
          39974.44140625,
          39842.5390625,
          40079.171875,
          38343.98828125,
          38386.890625,
          37577.328125,
          37008.16015625,
          37519.48046875,
          38230.328125,
          38782.91015625,
          37250.01171875,
          35442.21875,
          38327.2109375,
          38745.01171875,
          39219.171875,
          38928.2109375,
          39116.71875,
          39479.80078125,
          37699.0703125,
          38333.6484375,
          43160,
          43609.9609375,
          44421.19921875,
          44103.28125,
          43892.98046875,
          43382.23046875,
          42454,
          41662.0703125,
          39148.66015625,
          39129.58984375,
          39397.9609375,
          38352.48046875,
          38420.80859375,
          38337.05078125,
          37988,
          38983.05078125,
          38730.62890625,
          42151.66015625,
          41941.7109375,
          39076.37890625,
          39422,
          39893.6796875,
          38729.5703125,
          39079.16015625,
          38807.359375,
          38741.12109375,
          37777.33984375,
          38897.640625,
          39671.37109375,
          38645.01171875,
          39280.328125,
          40521.6015625,
          41114,
          40748.51171875,
          40917.8984375,
          40390.41015625,
          41757.51171875,
          41721.78125,
          42201.12890625,
          41586.859375,
          41262.109375,
          41244.28125,
          41002.25,
          42983,
          42364.12890625,
          42039.78125,
          42882.76171875,
          42968.421875,
          43991.4609375,
          44585.28125,
          44313.16015625,
          44334.48828125,
          44511.26953125,
          44569.5,
          46827.76171875,
          47252.5390625,
          47122.2109375,
          47896.1015625,
          47434.80078125,
          47319.91015625,
          47067.98828125,
          47152.01171875,
          45510.33984375,
          45064.26171875,
          46283.48828125,
          46542.83984375,
          45811,
          46252.58984375,
          46407.3515625,
          46160.08984375,
          46580.51171875,
          46683.37109375,
          45497.55078125,
          44819.19140625,
          43170.46875,
          43766.73828125,
          43444.19140625,
          43298.7890625,
          42252.01171875,
          42445.1015625,
          42753.96875,
          42680.359375,
          42158.8515625,
          41067.62890625,
          39530.44921875,
          40378,
          40074.94140625,
          39722.640625,
          41147.7890625,
          40919.98828125,
          39942.37890625,
          40220,
          40551.8984375,
          40416.94921875,
          40378.7109375,
          40464.76953125,
          39678.12109375,
          38998.5390625,
          40801.12890625,
          40687.30078125,
          41493.1796875,
          42090,
          41358.19140625,
          42440,
          40480.01171875,
          40585.5390625,
          39709.1796875,
          39602.9296875,
          39441.6015625,
          39609.109375,
          39450.12890625,
          38832.2890625,
          40426.078125,
          40467.03125,
          38112.6484375,
          39017.19140625,
          39235.71875,
          39679.30859375,
          39742.0703125,
          38749.5390625,
          38596.109375,
          38580.01171875,
          37630.80078125,
          37942.78125,
          38468.3515625,
          38528.76953125,
          38525.16015625,
          38570.01171875,
          37728.94921875,
          39002.30078125,
          39690,
          39492.828125,
          36552.96875,
          35780.83984375,
          36013.76953125,
          36055.859375,
          35472.390625,
          34786.23828125,
          34038.3984375,
          33122.30859375,
          30076.310546875,
          31622.240234375,
          31017.099609375,
          31533.150390625,
          29103.939453125,
          28472.98046875,
          29029.75,
          30668.080078125,
          29287.05078125,
          29074.240234375,
          30086.740234375,
          30241.4609375,
          31328.890625,
          29950.94921875,
          29874.009765625,
          30319.470703125,
          30444.9296875,
          29833.580078125,
          28715.3203125,
          29500.19921875,
          30319.23046875,
          30414.0703125,
          29201.009765625,
          29288,
          29445.060546875,
          29897.51953125,
          30293.939453125,
          30397.109375,
          29109.150390625,
          29295.740234375,
          29654.580078125,
          29409.009765625,
          29542.150390625,
          28998.400390625,
          29201.349609375,
          28912.9609375,
          28629.80078125,
          28828.01953125,
          29031.330078125,
          29213.849609375,
          29468.099609375,
          30665.369140625,
          31734.220703125,
          31767,
          31801.0390625,
          31609.16015625,
          29805.830078125,
          30173.669921875,
          30452.619140625,
          29758.189453125,
          29700.2109375,
          29681.689453125,
          29864.0390625,
          29718.689453125,
          29919.2109375,
          31396.7109375,
          31373.099609375,
          29521.5,
          31125.330078125,
          30410.849609375,
          30204.76953125,
          30299.470703125,
          30109.9296875,
          30020.560546875,
          29091.880859375,
          28734.009765625,
          28424.69921875,
          27457.390625,
          26574.529296875,
          23729.4296875,
          22487.41015625,
          21994.560546875,
          22136.41015625,
          21177.029296875,
          22583.720703125,
          21044.55078125,
          20401.310546875,
          20972.0390625,
          20468.810546875,
          19243.98046875,
          18970.7890625,
          19692.5,
          20574,
          20801.7890625,
          20573.890625,
          20962.419921875,
          20723.51953125,
          20506.30078125,
          19987.990234375,
          20638.7890625,
          21110.130859375,
          21169.529296875,
          21237.689453125,
          21340.220703125,
          21491.189453125,
          21708.599609375,
          21038.0703125,
          21304.55078125,
          20742.560546875,
          20998.130859375,
          20281.2890625,
          20104.80078125,
          20123.009765625,
          19101.359375,
          19942.2109375,
          19191.029296875,
          19279.80078125,
          19195.2109375,
          19252.810546875,
          19089.80078125,
          19315.830078125,
          19806.490234375,
          20236.7109375,
          19520.390625,
          20175.830078125,
          20103.4296875,
          20564.509765625,
          20507.830078125,
          21624.98046875,
          21583.98046875,
          21594.75,
          21447.2890625,
          21591.830078125,
          21301.83984375,
          20862.470703125,
          20473.060546875,
          19963.609375,
          19773.900390625,
          19328.75,
          19843.890625,
          20234.869140625,
          19738.130859375,
          20588.83984375,
          20769.740234375,
          20830.0390625,
          20616.689453125,
          21195.599609375,
          21456.7890625,
          20798.16015625,
          22133.310546875,
          22432.580078125,
          21974.490234375,
          23396.619140625,
          23728.55078125,
          23223.30078125,
          22606.0390625,
          23152.189453125,
          23610.580078125,
          22684.830078125,
          22227.529296875,
          22451.0703125,
          22657.630859375,
          22579.6796875,
          21941.25,
          21310.900390625,
          21090.220703125,
          21254.669921875,
          21200.23046875,
          22952.44921875,
          23114.51953125,
          23842.9296875,
          23720.560546875,
          23773.75,
          23956.380859375,
          23643.509765625,
          23801.0703125,
          23293.3203125,
          23227.7890625,
          23268.009765625,
          22882.609375,
          22987.7890625,
          23402.830078125,
          22818.369140625,
          22885.009765625,
          22622.98046875,
          23405.130859375,
          23312.419921875,
          23157.419921875,
          22954.2109375,
          23038.48046875,
          23174.390625,
          24166.119140625,
          23810,
          23263.919921875,
          23149.94921875,
          23123.94921875,
          23954.05078125,
          24559.369140625,
          23934.390625,
          23672.990234375,
          24403.6796875,
          24435.91015625,
          24441.380859375,
          24564.490234375,
          24305.240234375,
          24049.48046875,
          24094.8203125,
          24038.029296875,
          23854.740234375,
          23746.650390625,
          23342.66015625,
          23534.4296875,
          23191.19921875,
          21467.91015625,
          20834.390625,
          21233.66015625,
          21140.0703125,
          21524.390625,
          21515.609375,
          21248.7109375,
          21399.830078125,
          21456.33984375,
          21529.119140625,
          21438.919921875,
          21368.080078125,
          21720.310546875,
          21559.0390625,
          21189.76953125,
          20241.05078125,
          20211.2109375,
          20037.599609375,
          20034.970703125,
          19555.609375,
          19825.08984375,
          20285.73046875,
          20403.80078125,
          19811.66015625,
          20314.779296875,
          20050.01953125,
          20074.009765625,
          20131.4609375,
          20090.810546875,
          19951.859375,
          19798.759765625,
          19831.900390625,
          19850.7890625,
          20000.30078125,
          19701.94921875,
          19796.83984375,
          19918.330078125,
          18790.609375,
          18739.58984375,
          19292.83984375,
          19311.150390625,
          19319.76953125,
          20937.720703125,
          21360.109375,
          21285.109375,
          21648.33984375,
          21613.029296875,
          21826.869140625,
          22312.75,
          22395.740234375,
          22522.740234375,
          20173.5703125,
          20225.349609375,
          20226.7109375,
          20148.349609375,
          19701.880859375,
          19867.8203125,
          19803.30078125,
          19810.220703125,
          20113.619140625,
          19922.919921875,
          19416.1796875,
          18684.869140625,
          19537.01953125,
          19219.490234375,
          18875,
          19165.9296875,
          18461.359375,
          19229.810546875,
          19401.630859375,
          18862.720703125,
          19289.91015625,
          19051.630859375,
          18920.5,
          19117.939453125,
          18807.380859375,
          18876.619140625,
          19227.8203125,
          20229.259765625,
          19079.130859375,
          18955.4296875,
          19412.8203125,
          19463.0390625,
          19591.509765625,
          19436.2890625,
          19422.609375,
          19312.0390625,
          19310.94921875,
          19192.4296875,
          19056.80078125,
          19245.41015625,
          19629.080078125,
          19939,
          20337.8203125,
          20027.4296875,
          20158.259765625,
          20245.220703125,
          19960.669921875,
          19998.900390625,
          19530.08984375,
          19534.2109375,
          19417.9609375,
          19468.349609375,
          19439.01953125,
          19331.619140625,
          19131.869140625,
          19153.6796875,
          19060,
          19114.619140625,
          19155.529296875,
          18753.19921875,
          19375.130859375,
          19596.25,
          19176.9296875,
          19167.330078125,
          19069.390625,
          19144.689453125,
          19262.98046875,
          19457.580078125,
          19549.859375,
          19563.58984375,
          19327.439453125,
          19200.83984375,
          19123.970703125,
          19208.08984375,
          19041.919921875,
          18945.650390625,
          19164.369140625,
          19178.66015625,
          19204.349609375,
          19151.779296875,
          19570.400390625,
          19423.630859375,
          19329.720703125,
          19302.130859375,
          20080.0703125,
          20607.810546875,
          20771.58984375,
          20622.83984375,
          20295.109375,
          20173.740234375,
          20591.83984375,
          20698.890625,
          20809.669921875,
          20769.83984375,
          20627.48046875,
          20730.5390625,
          20490.740234375,
          20522.7890625,
          20483.619140625,
          20431.330078125,
          20151.83984375,
          20130.640625,
          20207.8203125,
          20558.4609375,
          21148.51953125,
          21404.630859375,
          21299.369140625,
          21255.080078125,
          20905.580078125,
          20733.4609375,
          20591.130859375,
          19699.0390625,
          18547.23046875,
          17818.080078125,
          15922.8095703125,
          16397.5703125,
          17601.150390625,
          17350.69921875,
          17070.310546875,
          16866.919921875,
          16812.080078125,
          16659.23046875,
          16329.849609375,
          16749.80078125,
          16619.4609375,
          16781.619140625,
          16900.5703125,
          16707.900390625,
          16662.759765625,
          16595.080078125,
          16692.560546875,
          16749.880859375,
          16700.44921875,
          16676.259765625,
          16700.6796875,
          16525.380859375,
          16280.23046875,
          16084.419921875,
          15781.2900390625,
          15747.8798828125,
          16226.9404296875,
          16585.76953125,
          16603.109375,
          16572.109375,
          16598.94921875,
          16531.640625,
          16522.140625,
          16580.310546875,
          16458.5703125,
          16550.91015625,
          16428.779296875,
          16213.2900390625,
          16212.91015625,
          16497.640625,
          16442.529296875,
          16879.3203125,
          17163.640625,
          17102.470703125,
          16977.369140625,
          16996.919921875,
          17092.740234375,
          16943.4296875,
          16885.19921875,
          16948.640625,
          17105.69921875,
          17310.509765625,
          16966.349609375,
          16983.900390625,
          17088.9609375,
          16793.2890625,
          16836.640625,
          16851.01953125,
          17224.099609375,
          17241.439453125,
          17128.560546875,
          17168.609375,
          17127.490234375,
          17167.4296875,
          17085.05078125,
          16988.619140625,
          17209.830078125,
          17444.630859375,
          17774.69921875,
          17824.9296875,
          17803.150390625,
          17720.330078125,
          17356.33984375,
          17027.08984375,
          16632.119140625,
          16708.220703125,
          16776.51953125,
          16711.349609375,
          16738.2109375,
          16734.4609375,
          16438.880859375,
          16814.25,
          16895.560546875,
          16878.470703125,
          16824.669921875,
          16834.80078125,
          16821.4296875,
          16844.01953125,
          16778.5,
          16824.41015625,
          16836.119140625,
          16821.509765625,
          16832.109375,
          16864.0390625,
          16919.390625,
          16832.400390625,
          16706.359375,
          16678.98046875,
          16547.310546875,
          16598.69921875,
          16633.470703125,
          16496.26953125,
          16607.48046875,
          16567.150390625,
          16542.400390625,
          16556.66015625,
          16616.75,
          16735.109375,
          16672.869140625,
          16721.029296875,
          16675.1796875,
          16835.349609375,
          16850.359375,
          16833.599609375,
          16831.849609375,
          16738.220703125,
          16950.650390625,
          16918.30078125,
          16943.5703125,
          16927.419921875,
          17127.830078125,
          17238.9296875,
          17178.259765625,
          17252.890625,
          17440.66015625,
          17437.75,
          17943.259765625,
          18200.80078125,
          18846.619140625,
          18920.94921875,
          19930.009765625,
          20721.73046875,
          20954.919921875,
          20722.23046875,
          20871.5,
          20817.560546875,
          21185.650390625,
          21219.75,
          21134.810546875,
          21204.669921875,
          20677.470703125,
          20743.720703125,
          21071.58984375,
          20963.509765625,
          22667.2109375,
          22905.650390625,
          22783.55078125,
          22783.609375,
          22707.880859375,
          22904.580078125,
          22916.44921875,
          22917.66015625,
          22632.890625,
          22598.470703125,
          23060.939453125,
          22992.939453125,
          23009.650390625,
          22967.470703125,
          23074.16015625,
          22981.509765625,
          23022.599609375,
          23432.169921875,
          23742.30078125,
          23077.470703125,
          22826.150390625,
          22865.4296875,
          23125.130859375,
          23077.51953125,
          23732.66015625,
          23981.580078125
         ],
         "yhoverformat": "$000,.0f"
        },
        {
         "mode": "markers",
         "name": "验证集预测值",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2022-02-16T08:00:00+08:00",
          "2022-02-16T20:00:00+08:00",
          "2022-02-17T08:00:00+08:00",
          "2022-02-17T20:00:00+08:00",
          "2022-02-18T08:00:00+08:00",
          "2022-02-18T20:00:00+08:00",
          "2022-02-19T08:00:00+08:00",
          "2022-02-19T20:00:00+08:00",
          "2022-02-20T08:00:00+08:00",
          "2022-02-20T20:00:00+08:00",
          "2022-02-21T08:00:00+08:00",
          "2022-02-21T20:00:00+08:00",
          "2022-02-22T08:00:00+08:00",
          "2022-02-22T20:00:00+08:00",
          "2022-02-23T08:00:00+08:00",
          "2022-02-23T20:00:00+08:00",
          "2022-02-24T08:00:00+08:00",
          "2022-02-24T20:00:00+08:00",
          "2022-02-25T08:00:00+08:00",
          "2022-02-25T20:00:00+08:00",
          "2022-02-26T08:00:00+08:00",
          "2022-02-26T20:00:00+08:00",
          "2022-02-27T08:00:00+08:00",
          "2022-02-27T20:00:00+08:00",
          "2022-02-28T08:00:00+08:00",
          "2022-02-28T20:00:00+08:00",
          "2022-03-01T08:00:00+08:00",
          "2022-03-01T20:00:00+08:00",
          "2022-03-02T08:00:00+08:00",
          "2022-03-02T20:00:00+08:00",
          "2022-03-03T08:00:00+08:00",
          "2022-03-03T20:00:00+08:00",
          "2022-03-04T08:00:00+08:00",
          "2022-03-04T20:00:00+08:00",
          "2022-03-05T08:00:00+08:00",
          "2022-03-05T20:00:00+08:00",
          "2022-03-06T08:00:00+08:00",
          "2022-03-06T20:00:00+08:00",
          "2022-03-07T08:00:00+08:00",
          "2022-03-07T20:00:00+08:00",
          "2022-03-08T08:00:00+08:00",
          "2022-03-08T20:00:00+08:00",
          "2022-03-09T08:00:00+08:00",
          "2022-03-09T20:00:00+08:00",
          "2022-03-10T08:00:00+08:00",
          "2022-03-10T20:00:00+08:00",
          "2022-03-11T08:00:00+08:00",
          "2022-03-11T20:00:00+08:00",
          "2022-03-12T08:00:00+08:00",
          "2022-03-12T20:00:00+08:00",
          "2022-03-13T08:00:00+08:00",
          "2022-03-13T20:00:00+08:00",
          "2022-03-14T08:00:00+08:00",
          "2022-03-14T20:00:00+08:00",
          "2022-03-15T08:00:00+08:00",
          "2022-03-15T20:00:00+08:00",
          "2022-03-16T08:00:00+08:00",
          "2022-03-16T20:00:00+08:00",
          "2022-03-17T08:00:00+08:00",
          "2022-03-17T20:00:00+08:00",
          "2022-03-18T08:00:00+08:00",
          "2022-03-18T20:00:00+08:00",
          "2022-03-19T08:00:00+08:00",
          "2022-03-19T20:00:00+08:00",
          "2022-03-20T08:00:00+08:00",
          "2022-03-20T20:00:00+08:00",
          "2022-03-21T08:00:00+08:00",
          "2022-03-21T20:00:00+08:00",
          "2022-03-22T08:00:00+08:00",
          "2022-03-22T20:00:00+08:00",
          "2022-03-23T08:00:00+08:00",
          "2022-03-23T20:00:00+08:00",
          "2022-03-24T08:00:00+08:00",
          "2022-03-24T20:00:00+08:00",
          "2022-03-25T08:00:00+08:00",
          "2022-03-25T20:00:00+08:00",
          "2022-03-26T08:00:00+08:00",
          "2022-03-26T20:00:00+08:00",
          "2022-03-27T08:00:00+08:00",
          "2022-03-27T20:00:00+08:00",
          "2022-03-28T08:00:00+08:00",
          "2022-03-28T20:00:00+08:00",
          "2022-03-29T08:00:00+08:00",
          "2022-03-29T20:00:00+08:00",
          "2022-03-30T08:00:00+08:00",
          "2022-03-30T20:00:00+08:00",
          "2022-03-31T08:00:00+08:00",
          "2022-03-31T20:00:00+08:00",
          "2022-04-01T08:00:00+08:00",
          "2022-04-01T20:00:00+08:00",
          "2022-04-02T08:00:00+08:00",
          "2022-04-02T20:00:00+08:00",
          "2022-04-03T08:00:00+08:00",
          "2022-04-03T20:00:00+08:00",
          "2022-04-04T08:00:00+08:00",
          "2022-04-04T20:00:00+08:00",
          "2022-04-05T08:00:00+08:00",
          "2022-04-05T20:00:00+08:00",
          "2022-04-06T08:00:00+08:00",
          "2022-04-06T20:00:00+08:00",
          "2022-04-07T08:00:00+08:00",
          "2022-04-07T20:00:00+08:00",
          "2022-04-08T08:00:00+08:00",
          "2022-04-08T20:00:00+08:00",
          "2022-04-09T08:00:00+08:00",
          "2022-04-09T20:00:00+08:00",
          "2022-04-10T08:00:00+08:00",
          "2022-04-10T20:00:00+08:00",
          "2022-04-11T08:00:00+08:00",
          "2022-04-11T20:00:00+08:00",
          "2022-04-12T08:00:00+08:00",
          "2022-04-12T20:00:00+08:00",
          "2022-04-13T08:00:00+08:00",
          "2022-04-13T20:00:00+08:00",
          "2022-04-14T08:00:00+08:00",
          "2022-04-14T20:00:00+08:00",
          "2022-04-15T08:00:00+08:00",
          "2022-04-15T20:00:00+08:00",
          "2022-04-16T08:00:00+08:00",
          "2022-04-16T20:00:00+08:00",
          "2022-04-17T08:00:00+08:00",
          "2022-04-17T20:00:00+08:00",
          "2022-04-18T08:00:00+08:00",
          "2022-04-18T20:00:00+08:00",
          "2022-04-19T08:00:00+08:00",
          "2022-04-19T20:00:00+08:00",
          "2022-04-20T08:00:00+08:00",
          "2022-04-20T20:00:00+08:00",
          "2022-04-21T08:00:00+08:00",
          "2022-04-21T20:00:00+08:00",
          "2022-04-22T08:00:00+08:00",
          "2022-04-22T20:00:00+08:00",
          "2022-04-23T08:00:00+08:00",
          "2022-04-23T20:00:00+08:00",
          "2022-04-24T08:00:00+08:00",
          "2022-04-24T20:00:00+08:00",
          "2022-04-25T08:00:00+08:00",
          "2022-04-25T20:00:00+08:00",
          "2022-04-26T08:00:00+08:00",
          "2022-04-26T20:00:00+08:00",
          "2022-04-27T08:00:00+08:00",
          "2022-04-27T20:00:00+08:00",
          "2022-04-28T08:00:00+08:00",
          "2022-04-28T20:00:00+08:00",
          "2022-04-29T08:00:00+08:00",
          "2022-04-29T20:00:00+08:00",
          "2022-04-30T08:00:00+08:00",
          "2022-04-30T20:00:00+08:00",
          "2022-05-01T08:00:00+08:00",
          "2022-05-01T20:00:00+08:00",
          "2022-05-02T08:00:00+08:00",
          "2022-05-02T20:00:00+08:00",
          "2022-05-03T08:00:00+08:00",
          "2022-05-03T20:00:00+08:00",
          "2022-05-04T08:00:00+08:00",
          "2022-05-04T20:00:00+08:00",
          "2022-05-05T08:00:00+08:00",
          "2022-05-05T20:00:00+08:00",
          "2022-05-06T08:00:00+08:00",
          "2022-05-06T20:00:00+08:00",
          "2022-05-07T08:00:00+08:00",
          "2022-05-07T20:00:00+08:00",
          "2022-05-08T08:00:00+08:00",
          "2022-05-08T20:00:00+08:00",
          "2022-05-09T08:00:00+08:00",
          "2022-05-09T20:00:00+08:00",
          "2022-05-10T08:00:00+08:00",
          "2022-05-10T20:00:00+08:00",
          "2022-05-11T08:00:00+08:00",
          "2022-05-11T20:00:00+08:00",
          "2022-05-12T08:00:00+08:00",
          "2022-05-12T20:00:00+08:00",
          "2022-05-13T08:00:00+08:00",
          "2022-05-13T20:00:00+08:00",
          "2022-05-14T08:00:00+08:00",
          "2022-05-14T20:00:00+08:00",
          "2022-05-15T08:00:00+08:00",
          "2022-05-15T20:00:00+08:00",
          "2022-05-16T08:00:00+08:00",
          "2022-05-16T20:00:00+08:00",
          "2022-05-17T08:00:00+08:00",
          "2022-05-17T20:00:00+08:00",
          "2022-05-18T08:00:00+08:00",
          "2022-05-18T20:00:00+08:00",
          "2022-05-19T08:00:00+08:00",
          "2022-05-19T20:00:00+08:00",
          "2022-05-20T08:00:00+08:00",
          "2022-05-20T20:00:00+08:00",
          "2022-05-21T08:00:00+08:00",
          "2022-05-21T20:00:00+08:00",
          "2022-05-22T08:00:00+08:00",
          "2022-05-22T20:00:00+08:00",
          "2022-05-23T08:00:00+08:00",
          "2022-05-23T20:00:00+08:00",
          "2022-05-24T08:00:00+08:00",
          "2022-05-24T20:00:00+08:00",
          "2022-05-25T08:00:00+08:00",
          "2022-05-25T20:00:00+08:00",
          "2022-05-26T08:00:00+08:00",
          "2022-05-26T20:00:00+08:00",
          "2022-05-27T08:00:00+08:00",
          "2022-05-27T20:00:00+08:00",
          "2022-05-28T08:00:00+08:00",
          "2022-05-28T20:00:00+08:00",
          "2022-05-29T08:00:00+08:00",
          "2022-05-29T20:00:00+08:00",
          "2022-05-30T08:00:00+08:00",
          "2022-05-30T20:00:00+08:00",
          "2022-05-31T08:00:00+08:00",
          "2022-05-31T20:00:00+08:00",
          "2022-06-01T08:00:00+08:00",
          "2022-06-01T20:00:00+08:00",
          "2022-06-02T08:00:00+08:00",
          "2022-06-02T20:00:00+08:00",
          "2022-06-03T08:00:00+08:00",
          "2022-06-03T20:00:00+08:00",
          "2022-06-04T08:00:00+08:00",
          "2022-06-04T20:00:00+08:00",
          "2022-06-05T08:00:00+08:00",
          "2022-06-05T20:00:00+08:00",
          "2022-06-06T08:00:00+08:00",
          "2022-06-06T20:00:00+08:00",
          "2022-06-07T08:00:00+08:00",
          "2022-06-07T20:00:00+08:00",
          "2022-06-08T08:00:00+08:00",
          "2022-06-08T20:00:00+08:00",
          "2022-06-09T08:00:00+08:00",
          "2022-06-09T20:00:00+08:00",
          "2022-06-10T08:00:00+08:00",
          "2022-06-10T20:00:00+08:00",
          "2022-06-11T08:00:00+08:00",
          "2022-06-11T20:00:00+08:00",
          "2022-06-12T08:00:00+08:00",
          "2022-06-12T20:00:00+08:00",
          "2022-06-13T08:00:00+08:00",
          "2022-06-13T20:00:00+08:00",
          "2022-06-14T08:00:00+08:00",
          "2022-06-14T20:00:00+08:00",
          "2022-06-15T08:00:00+08:00",
          "2022-06-15T20:00:00+08:00",
          "2022-06-16T08:00:00+08:00",
          "2022-06-16T20:00:00+08:00",
          "2022-06-17T08:00:00+08:00",
          "2022-06-17T20:00:00+08:00",
          "2022-06-18T08:00:00+08:00",
          "2022-06-18T20:00:00+08:00",
          "2022-06-19T08:00:00+08:00",
          "2022-06-19T20:00:00+08:00",
          "2022-06-20T08:00:00+08:00",
          "2022-06-20T20:00:00+08:00",
          "2022-06-21T08:00:00+08:00",
          "2022-06-21T20:00:00+08:00",
          "2022-06-22T08:00:00+08:00",
          "2022-06-22T20:00:00+08:00",
          "2022-06-23T08:00:00+08:00",
          "2022-06-23T20:00:00+08:00",
          "2022-06-24T08:00:00+08:00",
          "2022-06-24T20:00:00+08:00",
          "2022-06-25T08:00:00+08:00",
          "2022-06-25T20:00:00+08:00",
          "2022-06-26T08:00:00+08:00",
          "2022-06-26T20:00:00+08:00",
          "2022-06-27T08:00:00+08:00",
          "2022-06-27T20:00:00+08:00",
          "2022-06-28T08:00:00+08:00",
          "2022-06-28T20:00:00+08:00",
          "2022-06-29T08:00:00+08:00",
          "2022-06-29T20:00:00+08:00",
          "2022-06-30T08:00:00+08:00",
          "2022-06-30T20:00:00+08:00",
          "2022-07-01T08:00:00+08:00",
          "2022-07-01T20:00:00+08:00",
          "2022-07-02T08:00:00+08:00",
          "2022-07-02T20:00:00+08:00",
          "2022-07-03T08:00:00+08:00",
          "2022-07-03T20:00:00+08:00",
          "2022-07-04T08:00:00+08:00",
          "2022-07-04T20:00:00+08:00",
          "2022-07-05T08:00:00+08:00",
          "2022-07-05T20:00:00+08:00",
          "2022-07-06T08:00:00+08:00",
          "2022-07-06T20:00:00+08:00",
          "2022-07-07T08:00:00+08:00",
          "2022-07-07T20:00:00+08:00",
          "2022-07-08T08:00:00+08:00",
          "2022-07-08T20:00:00+08:00",
          "2022-07-09T08:00:00+08:00",
          "2022-07-09T20:00:00+08:00",
          "2022-07-10T08:00:00+08:00",
          "2022-07-10T20:00:00+08:00",
          "2022-07-11T08:00:00+08:00",
          "2022-07-11T20:00:00+08:00",
          "2022-07-12T08:00:00+08:00",
          "2022-07-12T20:00:00+08:00",
          "2022-07-13T08:00:00+08:00",
          "2022-07-13T20:00:00+08:00",
          "2022-07-14T08:00:00+08:00",
          "2022-07-14T20:00:00+08:00",
          "2022-07-15T08:00:00+08:00",
          "2022-07-15T20:00:00+08:00",
          "2022-07-16T08:00:00+08:00",
          "2022-07-16T20:00:00+08:00",
          "2022-07-17T08:00:00+08:00",
          "2022-07-17T20:00:00+08:00",
          "2022-07-18T08:00:00+08:00",
          "2022-07-18T20:00:00+08:00",
          "2022-07-19T08:00:00+08:00",
          "2022-07-19T20:00:00+08:00",
          "2022-07-20T08:00:00+08:00",
          "2022-07-20T20:00:00+08:00",
          "2022-07-21T08:00:00+08:00",
          "2022-07-21T20:00:00+08:00",
          "2022-07-22T08:00:00+08:00",
          "2022-07-22T20:00:00+08:00",
          "2022-07-23T08:00:00+08:00",
          "2022-07-23T20:00:00+08:00",
          "2022-07-24T08:00:00+08:00",
          "2022-07-24T20:00:00+08:00",
          "2022-07-25T08:00:00+08:00",
          "2022-07-25T20:00:00+08:00",
          "2022-07-26T08:00:00+08:00",
          "2022-07-26T20:00:00+08:00",
          "2022-07-27T08:00:00+08:00",
          "2022-07-27T20:00:00+08:00",
          "2022-07-28T08:00:00+08:00",
          "2022-07-28T20:00:00+08:00",
          "2022-07-29T08:00:00+08:00",
          "2022-07-29T20:00:00+08:00",
          "2022-07-30T08:00:00+08:00",
          "2022-07-30T20:00:00+08:00",
          "2022-07-31T08:00:00+08:00",
          "2022-07-31T20:00:00+08:00",
          "2022-08-01T08:00:00+08:00",
          "2022-08-01T20:00:00+08:00",
          "2022-08-02T08:00:00+08:00",
          "2022-08-02T20:00:00+08:00",
          "2022-08-03T08:00:00+08:00",
          "2022-08-03T20:00:00+08:00",
          "2022-08-04T08:00:00+08:00",
          "2022-08-04T20:00:00+08:00",
          "2022-08-05T08:00:00+08:00",
          "2022-08-05T20:00:00+08:00",
          "2022-08-06T08:00:00+08:00",
          "2022-08-06T20:00:00+08:00",
          "2022-08-07T08:00:00+08:00",
          "2022-08-07T20:00:00+08:00",
          "2022-08-08T08:00:00+08:00",
          "2022-08-08T20:00:00+08:00",
          "2022-08-09T08:00:00+08:00",
          "2022-08-09T20:00:00+08:00",
          "2022-08-10T08:00:00+08:00",
          "2022-08-10T20:00:00+08:00",
          "2022-08-11T08:00:00+08:00",
          "2022-08-11T20:00:00+08:00",
          "2022-08-12T08:00:00+08:00",
          "2022-08-12T20:00:00+08:00",
          "2022-08-13T08:00:00+08:00",
          "2022-08-13T20:00:00+08:00",
          "2022-08-14T08:00:00+08:00",
          "2022-08-14T20:00:00+08:00",
          "2022-08-15T08:00:00+08:00",
          "2022-08-15T20:00:00+08:00",
          "2022-08-16T08:00:00+08:00",
          "2022-08-16T20:00:00+08:00",
          "2022-08-17T08:00:00+08:00",
          "2022-08-17T20:00:00+08:00",
          "2022-08-18T08:00:00+08:00",
          "2022-08-18T20:00:00+08:00",
          "2022-08-19T08:00:00+08:00",
          "2022-08-19T20:00:00+08:00",
          "2022-08-20T08:00:00+08:00",
          "2022-08-20T20:00:00+08:00",
          "2022-08-21T08:00:00+08:00",
          "2022-08-21T20:00:00+08:00",
          "2022-08-22T08:00:00+08:00",
          "2022-08-22T20:00:00+08:00",
          "2022-08-23T08:00:00+08:00",
          "2022-08-23T20:00:00+08:00",
          "2022-08-24T08:00:00+08:00",
          "2022-08-24T20:00:00+08:00",
          "2022-08-25T08:00:00+08:00",
          "2022-08-25T20:00:00+08:00",
          "2022-08-26T08:00:00+08:00",
          "2022-08-26T20:00:00+08:00",
          "2022-08-27T08:00:00+08:00",
          "2022-08-27T20:00:00+08:00",
          "2022-08-28T08:00:00+08:00",
          "2022-08-28T20:00:00+08:00",
          "2022-08-29T08:00:00+08:00",
          "2022-08-29T20:00:00+08:00",
          "2022-08-30T08:00:00+08:00",
          "2022-08-30T20:00:00+08:00",
          "2022-08-31T08:00:00+08:00",
          "2022-08-31T20:00:00+08:00",
          "2022-09-01T08:00:00+08:00",
          "2022-09-01T20:00:00+08:00",
          "2022-09-02T08:00:00+08:00",
          "2022-09-02T20:00:00+08:00",
          "2022-09-03T08:00:00+08:00",
          "2022-09-03T20:00:00+08:00",
          "2022-09-04T08:00:00+08:00",
          "2022-09-04T20:00:00+08:00",
          "2022-09-05T08:00:00+08:00",
          "2022-09-05T20:00:00+08:00",
          "2022-09-06T08:00:00+08:00",
          "2022-09-06T20:00:00+08:00",
          "2022-09-07T08:00:00+08:00",
          "2022-09-07T20:00:00+08:00",
          "2022-09-08T08:00:00+08:00",
          "2022-09-08T20:00:00+08:00",
          "2022-09-09T08:00:00+08:00",
          "2022-09-09T20:00:00+08:00",
          "2022-09-10T08:00:00+08:00",
          "2022-09-10T20:00:00+08:00",
          "2022-09-11T08:00:00+08:00",
          "2022-09-11T20:00:00+08:00",
          "2022-09-12T08:00:00+08:00",
          "2022-09-12T20:00:00+08:00",
          "2022-09-13T08:00:00+08:00",
          "2022-09-13T20:00:00+08:00",
          "2022-09-14T08:00:00+08:00",
          "2022-09-14T20:00:00+08:00",
          "2022-09-15T08:00:00+08:00",
          "2022-09-15T20:00:00+08:00",
          "2022-09-16T08:00:00+08:00",
          "2022-09-16T20:00:00+08:00",
          "2022-09-17T08:00:00+08:00",
          "2022-09-17T20:00:00+08:00",
          "2022-09-18T08:00:00+08:00",
          "2022-09-18T20:00:00+08:00",
          "2022-09-19T08:00:00+08:00",
          "2022-09-19T20:00:00+08:00",
          "2022-09-20T08:00:00+08:00",
          "2022-09-20T20:00:00+08:00",
          "2022-09-21T08:00:00+08:00",
          "2022-09-21T20:00:00+08:00",
          "2022-09-22T08:00:00+08:00",
          "2022-09-22T20:00:00+08:00",
          "2022-09-23T08:00:00+08:00",
          "2022-09-23T20:00:00+08:00",
          "2022-09-24T08:00:00+08:00",
          "2022-09-24T20:00:00+08:00",
          "2022-09-25T08:00:00+08:00",
          "2022-09-25T20:00:00+08:00",
          "2022-09-26T08:00:00+08:00",
          "2022-09-26T20:00:00+08:00",
          "2022-09-27T08:00:00+08:00",
          "2022-09-27T20:00:00+08:00",
          "2022-09-28T08:00:00+08:00",
          "2022-09-28T20:00:00+08:00",
          "2022-09-29T08:00:00+08:00",
          "2022-09-29T20:00:00+08:00",
          "2022-09-30T08:00:00+08:00",
          "2022-09-30T20:00:00+08:00",
          "2022-10-01T08:00:00+08:00",
          "2022-10-01T20:00:00+08:00",
          "2022-10-02T08:00:00+08:00",
          "2022-10-02T20:00:00+08:00",
          "2022-10-03T08:00:00+08:00",
          "2022-10-03T20:00:00+08:00",
          "2022-10-04T08:00:00+08:00",
          "2022-10-04T20:00:00+08:00",
          "2022-10-05T08:00:00+08:00",
          "2022-10-05T20:00:00+08:00",
          "2022-10-06T08:00:00+08:00",
          "2022-10-06T20:00:00+08:00",
          "2022-10-07T08:00:00+08:00",
          "2022-10-07T20:00:00+08:00",
          "2022-10-08T08:00:00+08:00",
          "2022-10-08T20:00:00+08:00",
          "2022-10-09T08:00:00+08:00",
          "2022-10-09T20:00:00+08:00",
          "2022-10-10T08:00:00+08:00",
          "2022-10-10T20:00:00+08:00",
          "2022-10-11T08:00:00+08:00",
          "2022-10-11T20:00:00+08:00",
          "2022-10-12T08:00:00+08:00",
          "2022-10-12T20:00:00+08:00",
          "2022-10-13T08:00:00+08:00",
          "2022-10-13T20:00:00+08:00",
          "2022-10-14T08:00:00+08:00",
          "2022-10-14T20:00:00+08:00",
          "2022-10-15T08:00:00+08:00",
          "2022-10-15T20:00:00+08:00",
          "2022-10-16T08:00:00+08:00",
          "2022-10-16T20:00:00+08:00",
          "2022-10-17T08:00:00+08:00",
          "2022-10-17T20:00:00+08:00",
          "2022-10-18T08:00:00+08:00",
          "2022-10-18T20:00:00+08:00",
          "2022-10-19T08:00:00+08:00",
          "2022-10-19T20:00:00+08:00",
          "2022-10-20T08:00:00+08:00",
          "2022-10-20T20:00:00+08:00",
          "2022-10-21T08:00:00+08:00",
          "2022-10-21T20:00:00+08:00",
          "2022-10-22T08:00:00+08:00",
          "2022-10-22T20:00:00+08:00",
          "2022-10-23T08:00:00+08:00",
          "2022-10-23T20:00:00+08:00",
          "2022-10-24T08:00:00+08:00",
          "2022-10-24T20:00:00+08:00",
          "2022-10-25T08:00:00+08:00",
          "2022-10-25T20:00:00+08:00",
          "2022-10-26T08:00:00+08:00",
          "2022-10-26T20:00:00+08:00",
          "2022-10-27T08:00:00+08:00",
          "2022-10-27T20:00:00+08:00",
          "2022-10-28T08:00:00+08:00",
          "2022-10-28T20:00:00+08:00",
          "2022-10-29T08:00:00+08:00",
          "2022-10-29T20:00:00+08:00",
          "2022-10-30T08:00:00+08:00",
          "2022-10-30T20:00:00+08:00",
          "2022-10-31T08:00:00+08:00",
          "2022-10-31T20:00:00+08:00",
          "2022-11-01T08:00:00+08:00",
          "2022-11-01T20:00:00+08:00",
          "2022-11-02T08:00:00+08:00",
          "2022-11-02T20:00:00+08:00",
          "2022-11-03T08:00:00+08:00",
          "2022-11-03T20:00:00+08:00",
          "2022-11-04T08:00:00+08:00",
          "2022-11-04T20:00:00+08:00",
          "2022-11-05T08:00:00+08:00",
          "2022-11-05T20:00:00+08:00",
          "2022-11-06T08:00:00+08:00",
          "2022-11-06T20:00:00+08:00",
          "2022-11-07T08:00:00+08:00",
          "2022-11-07T20:00:00+08:00",
          "2022-11-08T08:00:00+08:00",
          "2022-11-08T20:00:00+08:00",
          "2022-11-09T08:00:00+08:00",
          "2022-11-09T20:00:00+08:00",
          "2022-11-10T08:00:00+08:00",
          "2022-11-10T20:00:00+08:00",
          "2022-11-11T08:00:00+08:00",
          "2022-11-11T20:00:00+08:00",
          "2022-11-12T08:00:00+08:00",
          "2022-11-12T20:00:00+08:00",
          "2022-11-13T08:00:00+08:00",
          "2022-11-13T20:00:00+08:00",
          "2022-11-14T08:00:00+08:00",
          "2022-11-14T20:00:00+08:00",
          "2022-11-15T08:00:00+08:00",
          "2022-11-15T20:00:00+08:00",
          "2022-11-16T08:00:00+08:00",
          "2022-11-16T20:00:00+08:00",
          "2022-11-17T08:00:00+08:00",
          "2022-11-17T20:00:00+08:00",
          "2022-11-18T08:00:00+08:00",
          "2022-11-18T20:00:00+08:00",
          "2022-11-19T08:00:00+08:00",
          "2022-11-19T20:00:00+08:00",
          "2022-11-20T08:00:00+08:00",
          "2022-11-20T20:00:00+08:00",
          "2022-11-21T08:00:00+08:00",
          "2022-11-21T20:00:00+08:00",
          "2022-11-22T08:00:00+08:00",
          "2022-11-22T20:00:00+08:00",
          "2022-11-23T08:00:00+08:00",
          "2022-11-23T20:00:00+08:00",
          "2022-11-24T08:00:00+08:00",
          "2022-11-24T20:00:00+08:00",
          "2022-11-25T08:00:00+08:00",
          "2022-11-25T20:00:00+08:00",
          "2022-11-26T08:00:00+08:00",
          "2022-11-26T20:00:00+08:00",
          "2022-11-27T08:00:00+08:00",
          "2022-11-27T20:00:00+08:00",
          "2022-11-28T08:00:00+08:00",
          "2022-11-28T20:00:00+08:00",
          "2022-11-29T08:00:00+08:00",
          "2022-11-29T20:00:00+08:00",
          "2022-11-30T08:00:00+08:00",
          "2022-11-30T20:00:00+08:00",
          "2022-12-01T08:00:00+08:00",
          "2022-12-01T20:00:00+08:00",
          "2022-12-02T08:00:00+08:00",
          "2022-12-02T20:00:00+08:00",
          "2022-12-03T08:00:00+08:00",
          "2022-12-03T20:00:00+08:00",
          "2022-12-04T08:00:00+08:00",
          "2022-12-04T20:00:00+08:00",
          "2022-12-05T08:00:00+08:00",
          "2022-12-05T20:00:00+08:00",
          "2022-12-06T08:00:00+08:00",
          "2022-12-06T20:00:00+08:00",
          "2022-12-07T08:00:00+08:00",
          "2022-12-07T20:00:00+08:00",
          "2022-12-08T08:00:00+08:00",
          "2022-12-08T20:00:00+08:00",
          "2022-12-09T08:00:00+08:00",
          "2022-12-09T20:00:00+08:00",
          "2022-12-10T08:00:00+08:00",
          "2022-12-10T20:00:00+08:00",
          "2022-12-11T08:00:00+08:00",
          "2022-12-11T20:00:00+08:00",
          "2022-12-12T08:00:00+08:00",
          "2022-12-12T20:00:00+08:00",
          "2022-12-13T08:00:00+08:00",
          "2022-12-13T20:00:00+08:00",
          "2022-12-14T08:00:00+08:00",
          "2022-12-14T20:00:00+08:00",
          "2022-12-15T08:00:00+08:00",
          "2022-12-15T20:00:00+08:00",
          "2022-12-16T08:00:00+08:00",
          "2022-12-16T20:00:00+08:00",
          "2022-12-17T08:00:00+08:00",
          "2022-12-17T20:00:00+08:00",
          "2022-12-18T08:00:00+08:00",
          "2022-12-18T20:00:00+08:00",
          "2022-12-19T08:00:00+08:00",
          "2022-12-19T20:00:00+08:00",
          "2022-12-20T08:00:00+08:00",
          "2022-12-20T20:00:00+08:00",
          "2022-12-21T08:00:00+08:00",
          "2022-12-21T20:00:00+08:00",
          "2022-12-22T08:00:00+08:00",
          "2022-12-22T20:00:00+08:00",
          "2022-12-23T08:00:00+08:00",
          "2022-12-23T20:00:00+08:00",
          "2022-12-24T08:00:00+08:00",
          "2022-12-24T20:00:00+08:00",
          "2022-12-25T08:00:00+08:00",
          "2022-12-25T20:00:00+08:00",
          "2022-12-26T08:00:00+08:00",
          "2022-12-26T20:00:00+08:00",
          "2022-12-27T08:00:00+08:00",
          "2022-12-27T20:00:00+08:00",
          "2022-12-28T08:00:00+08:00",
          "2022-12-28T20:00:00+08:00",
          "2022-12-29T08:00:00+08:00",
          "2022-12-29T20:00:00+08:00",
          "2022-12-30T08:00:00+08:00",
          "2022-12-30T20:00:00+08:00",
          "2022-12-31T08:00:00+08:00",
          "2022-12-31T20:00:00+08:00",
          "2023-01-01T08:00:00+08:00",
          "2023-01-01T20:00:00+08:00",
          "2023-01-02T08:00:00+08:00",
          "2023-01-02T20:00:00+08:00",
          "2023-01-03T08:00:00+08:00",
          "2023-01-03T20:00:00+08:00",
          "2023-01-04T08:00:00+08:00",
          "2023-01-04T20:00:00+08:00",
          "2023-01-05T08:00:00+08:00",
          "2023-01-05T20:00:00+08:00",
          "2023-01-06T08:00:00+08:00",
          "2023-01-06T20:00:00+08:00",
          "2023-01-07T08:00:00+08:00",
          "2023-01-07T20:00:00+08:00",
          "2023-01-08T08:00:00+08:00",
          "2023-01-08T20:00:00+08:00",
          "2023-01-09T08:00:00+08:00",
          "2023-01-09T20:00:00+08:00",
          "2023-01-10T08:00:00+08:00",
          "2023-01-10T20:00:00+08:00",
          "2023-01-11T08:00:00+08:00",
          "2023-01-11T20:00:00+08:00",
          "2023-01-12T08:00:00+08:00",
          "2023-01-12T20:00:00+08:00",
          "2023-01-13T08:00:00+08:00",
          "2023-01-13T20:00:00+08:00",
          "2023-01-14T08:00:00+08:00",
          "2023-01-14T20:00:00+08:00",
          "2023-01-15T08:00:00+08:00",
          "2023-01-15T20:00:00+08:00",
          "2023-01-16T08:00:00+08:00",
          "2023-01-16T20:00:00+08:00",
          "2023-01-17T08:00:00+08:00",
          "2023-01-17T20:00:00+08:00",
          "2023-01-18T08:00:00+08:00",
          "2023-01-18T20:00:00+08:00",
          "2023-01-19T08:00:00+08:00",
          "2023-01-19T20:00:00+08:00",
          "2023-01-20T08:00:00+08:00",
          "2023-01-20T20:00:00+08:00",
          "2023-01-21T08:00:00+08:00",
          "2023-01-21T20:00:00+08:00",
          "2023-01-22T08:00:00+08:00",
          "2023-01-22T20:00:00+08:00",
          "2023-01-23T08:00:00+08:00",
          "2023-01-23T20:00:00+08:00",
          "2023-01-24T08:00:00+08:00",
          "2023-01-24T20:00:00+08:00",
          "2023-01-25T08:00:00+08:00",
          "2023-01-25T20:00:00+08:00",
          "2023-01-26T08:00:00+08:00",
          "2023-01-26T20:00:00+08:00",
          "2023-01-27T08:00:00+08:00",
          "2023-01-27T20:00:00+08:00",
          "2023-01-28T08:00:00+08:00",
          "2023-01-28T20:00:00+08:00",
          "2023-01-29T08:00:00+08:00",
          "2023-01-29T20:00:00+08:00",
          "2023-01-30T08:00:00+08:00",
          "2023-01-30T20:00:00+08:00",
          "2023-01-31T08:00:00+08:00",
          "2023-01-31T20:00:00+08:00",
          "2023-02-01T08:00:00+08:00",
          "2023-02-01T20:00:00+08:00",
          "2023-02-02T08:00:00+08:00",
          "2023-02-02T20:00:00+08:00"
         ],
         "xhoverformat": "%y/%m/%d_%H:00",
         "y": [
          44323.53598960303,
          43426.244986097794,
          42832.17196658347,
          42771.2091767285,
          41514.38266065065,
          42124.31895828899,
          41523.572953493334,
          42130.178540973924,
          42799.85945206694,
          41950.913860701025,
          40004.73828164954,
          41713.461982371286,
          39422.07217342779,
          39702.97362836404,
          39870.268569007516,
          38586.48411759036,
          39782.30929392716,
          38950.89233576413,
          38242.46493416699,
          38523.713338078,
          39034.1909296168,
          39171.07772011636,
          40247.06286558043,
          40738.20768924756,
          39721.519978559576,
          41350.8154939916,
          41172.65719544934,
          42193.25771957496,
          42070.81171640288,
          43729.894728812855,
          42976.841979083605,
          42496.826507529244,
          41364.23085434921,
          41258.091452991124,
          41170.98798538186,
          40178.320708713494,
          39395.9353506621,
          40143.926500348374,
          39435.78436790826,
          40120.25301530352,
          39994.54183678003,
          39924.23069954943,
          39869.620930921286,
          40842.488986096345,
          40559.06636861013,
          40466.67385662999,
          38935.26035695663,
          39757.29812783515,
          39081.23335560644,
          40791.418097010814,
          39916.486027434934,
          38802.97642285563,
          39420.82315711863,
          38943.282588250935,
          39595.00696240552,
          39124.78701690538,
          39442.48048312124,
          40454.70797198918,
          40807.84420347167,
          40386.493948561605,
          40759.942115022335,
          40312.21217306424,
          41494.914968177676,
          40694.310933962464,
          41755.06116444571,
          41267.08514141478,
          41050.46947651403,
          42348.13959689904,
          41359.06902460242,
          41632.27592227608,
          42253.64997111587,
          41598.19782297686,
          42422.74133645091,
          43758.69534870703,
          43645.06570448261,
          44476.62915221276,
          43888.17670608964,
          44099.86955053732,
          45108.716214395594,
          44943.08662883099,
          45533.339354634285,
          45800.38983550016,
          47020.80983915925,
          46988.35469006421,
          47006.14160750387,
          46554.85351106012,
          46527.510076859966,
          46519.029101921245,
          45001.46272132965,
          45416.914843668696,
          45783.40090070246,
          46308.77031323593,
          45416.39442020655,
          45634.56364551652,
          45892.046044656076,
          45696.802436600905,
          46261.79342205264,
          46055.06194794411,
          45849.38674071431,
          46080.701477179304,
          43794.23834367748,
          43184.733804763295,
          43013.587730499916,
          43167.532845592126,
          42033.7614208865,
          42165.459396719,
          42146.8860616032,
          42219.29816762544,
          41763.74645378068,
          41312.011178658344,
          41524.69861016702,
          41857.839015737176,
          40654.25760232005,
          40706.03395432094,
          40646.231516037136,
          40279.56812952738,
          40613.16727874195,
          40526.059956144076,
          40042.23575584451,
          40333.74613943137,
          40026.2414081078,
          39886.26291674422,
          39356.80336129712,
          39836.29840938933,
          40298.03737995075,
          40324.95676540397,
          40960.70606676489,
          40694.021809816826,
          41115.54168422567,
          40893.08185659442,
          39522.579436428845,
          39674.25396323064,
          39337.551548186224,
          39157.16506622825,
          39314.73387061246,
          38943.32884811424,
          39237.2832944789,
          38585.219681326766,
          38821.47651318787,
          39045.509176171385,
          38546.28815136943,
          38952.796700136736,
          38840.847830945626,
          38339.30995798996,
          38717.71563980123,
          38578.38864151249,
          37610.94070032099,
          38250.10166660044,
          37228.29067605175,
          37561.5621512346,
          38151.75705220876,
          38017.665128450375,
          38350.32366044447,
          36899.76083685737,
          37439.2896225499,
          37587.64114917116,
          36512.334481702186,
          36146.60400243569,
          36180.55874209944,
          35269.027410674375,
          35712.49373523146,
          34478.24974242225,
          35086.99099359196,
          34310.86999202939,
          34422.14809320262,
          32736.758638528176,
          32710.654583165655,
          32249.584453127816,
          32374.119860125706,
          32174.921033758208,
          32216.65899542254,
          32306.033051322218,
          30656.591365422126,
          30575.09690623824,
          30299.42281585513,
          30985.360213910233,
          30290.20361059788,
          30265.05559241027,
          29671.45673649479,
          29734.84431418451,
          31007.111987133747,
          29903.19166671811,
          30107.197663880423,
          29450.664191436725,
          29205.371266277278,
          29237.39658414247,
          29939.222317747543,
          29904.08409658098,
          28935.932619968433,
          29042.804469162136,
          29239.701867330357,
          29671.364216768186,
          29438.761914107952,
          28052.126366615765,
          28842.437581246257,
          28455.072905908808,
          28957.67668321473,
          29165.639826183677,
          29205.70279529761,
          28805.940476595195,
          29083.00814536028,
          28746.51004471211,
          28678.15531420033,
          29528.536888822917,
          28844.897063978486,
          29837.911289621614,
          30509.33465556568,
          28952.16404950456,
          29948.45887045353,
          29643.40398439067,
          30133.147519698603,
          30095.954589603705,
          29575.991798593666,
          29299.40407591034,
          28457.073644996624,
          29436.50096328906,
          29303.552043653097,
          28643.35825952562,
          27918.984697549375,
          28412.184230144838,
          28245.44248036668,
          28474.910677288197,
          27770.681358285718,
          28195.460625563053,
          28578.57903094823,
          28554.85928604007,
          27722.204876533713,
          28066.360912052685,
          28502.813084836587,
          26394.710636778967,
          25010.93356333999,
          26528.65028848732,
          26284.00885640271,
          25709.515324031003,
          24992.312040866585,
          23560.22425018577,
          22730.409039789578,
          21762.6507720151,
          21878.416079928633,
          21411.65598670463,
          21879.56101154536,
          20857.264292431995,
          19950.324252439663,
          20661.27088909247,
          20161.174781876383,
          19801.228713016957,
          19424.715830612462,
          19956.661853712052,
          19551.566158269765,
          19853.299971646396,
          20384.67738392623,
          20558.471835363656,
          20690.77311691339,
          20221.125637221616,
          19645.225034007803,
          20430.027469916735,
          20900.910473457538,
          20853.65602309443,
          20901.606298901374,
          21115.999562869314,
          21295.94657612592,
          21395.79078108631,
          20799.285263760015,
          21014.02933169133,
          20596.921491744928,
          20738.156709394883,
          20167.87282458367,
          19849.643514951225,
          19543.09289330826,
          20060.94893304375,
          19596.18379642465,
          19081.55245466018,
          18897.114379674662,
          19062.016336139408,
          18720.203169947257,
          18748.402411618503,
          19212.08429643861,
          19593.290627473965,
          19753.812353132293,
          19674.316705541918,
          20071.11453800439,
          21226.674358325312,
          21037.475372408284,
          21151.498225470772,
          21133.277621812653,
          21389.67869664752,
          20530.941434215987,
          20819.94607520732,
          20754.578960867133,
          20368.97023284086,
          20850.64720448549,
          20219.01695845276,
          19216.00674734777,
          19442.838132061064,
          19457.58731847722,
          19148.16087533231,
          19558.58609252586,
          20257.30663280678,
          20248.532678733813,
          20267.2370834623,
          20731.194599967916,
          20715.215672185645,
          20984.827792981872,
          21253.169695031596,
          20888.990848680027,
          22083.13139499491,
          21825.824397837045,
          22153.176538011525,
          22921.011241560103,
          23439.883070793934,
          22928.40703720553,
          22598.12896067719,
          22926.832274358952,
          23494.984350469662,
          22762.264738155995,
          22491.035449638497,
          22347.58361353865,
          22643.723838444334,
          22103.373940178193,
          22175.429459754145,
          21297.37292191107,
          21045.22004452278,
          21189.822594722267,
          21292.00485027372,
          22646.102366415784,
          22891.505158750573,
          23775.463684153277,
          23354.591447830666,
          23686.496329551795,
          23825.486016337294,
          23504.68735679728,
          23362.32262748503,
          22760.0481197061,
          23048.98722589109,
          22533.278414810542,
          22566.533474041848,
          22809.07971981773,
          22899.353915557498,
          22762.332200456643,
          23513.640567840543,
          22740.850276435725,
          23684.98131902865,
          22908.56926582614,
          22987.317045626463,
          24096.19873638125,
          22941.635430615628,
          23162.364368361654,
          23739.970804034732,
          23810.63852771162,
          23024.880054627778,
          23182.65895589115,
          23094.840387895005,
          23806.741134228418,
          23909.624997706618,
          23863.376699370332,
          23754.278594255215,
          24115.743528626394,
          23406.051690765657,
          24383.345272863284,
          24174.940733698662,
          24310.91774688661,
          24027.263830083888,
          23649.428686586674,
          23436.34804623993,
          24089.255901897326,
          23919.9467297059,
          22714.82524833968,
          22509.81502664485,
          21672.979881992564,
          21463.413063762477,
          21427.44794753939,
          20988.607609312516,
          21297.92225778778,
          21321.113869256573,
          20552.681642473675,
          19999.438734806376,
          21213.230085553136,
          21186.975685634883,
          21180.108023428824,
          21056.513233651407,
          21213.987590814708,
          20497.90803682967,
          20922.627551783575,
          20676.95105525758,
          19864.61436321237,
          20069.49737028312,
          19401.504944200628,
          19823.556807037443,
          19493.05899615842,
          19881.106004479574,
          20219.9864880878,
          19353.687665500678,
          19842.997514590155,
          20272.45673803822,
          19755.263756343396,
          19587.895570916357,
          19958.022664690856,
          19494.117190531455,
          19923.057918011677,
          19714.03465317539,
          19497.442118206294,
          19641.620619658846,
          19929.439851653064,
          19498.914723854745,
          18955.855732343975,
          18922.455147292698,
          18460.375010239426,
          19237.143649888225,
          18859.72484516073,
          19094.007922854275,
          19428.64791899314,
          20610.647178685525,
          21074.78009717283,
          21121.957447763765,
          21567.90638498473,
          20697.555969370063,
          21474.51350346068,
          20401.02639061492,
          21175.262302747928,
          21077.773495827336,
          19859.219306654762,
          19975.663092563394,
          19941.27081169258,
          20096.069806761574,
          19868.347919679713,
          19357.388454564847,
          19670.234272605507,
          18677.449418784585,
          18705.221720467438,
          19761.212003766326,
          19028.992576224613,
          18831.155524583068,
          19328.742034215014,
          18946.198022132507,
          18747.37216591288,
          19254.589400836034,
          18614.542750923778,
          19294.284218537854,
          19350.586327165132,
          18529.26462042064,
          19665.13990515936,
          18418.43080668454,
          18708.246922777966,
          19049.054900690448,
          19272.22404622566,
          18869.385446613654,
          18891.11601739982,
          19840.645971538965,
          19115.97364794556,
          18961.1679066465,
          19221.38060146803,
          18942.868275721907,
          19550.82021797402,
          19399.824169167317,
          19195.424963166937,
          19226.598328549648,
          19210.193424526136,
          18812.16970568616,
          19112.90122202458,
          19661.91327969404,
          19583.6570109413,
          19454.60162979993,
          20177.845680113882,
          19006.28572957334,
          18647.228235588176,
          18707.793961616466,
          19201.90905400645,
          19076.914903364144,
          19397.996904566884,
          18705.39904994343,
          18736.68517374294,
          18983.926795643987,
          19407.339469459606,
          19124.261873453856,
          19040.020734886406,
          19164.322915073484,
          18690.509156443062,
          19192.05955811171,
          18914.720112649724,
          18769.14514157374,
          19286.258132254938,
          19603.23264309531,
          19500.246622418985,
          18779.708774108207,
          19210.7658903345,
          19042.139051126782,
          19284.513749909587,
          19495.52811636217,
          19721.59621333098,
          20003.897029132117,
          19176.469984178897,
          19081.695089238696,
          19033.53086156398,
          19289.09925885941,
          19401.73238852853,
          18961.168870393652,
          18888.758691865718,
          19092.581577069126,
          19431.400380859617,
          19764.174562511966,
          19411.589594400488,
          19359.100069507025,
          19579.05800953135,
          19810.904734424315,
          19978.188110101968,
          20238.58295313525,
          20880.03956513107,
          20288.90982941934,
          20181.92618555599,
          20281.286589446012,
          19999.136118200608,
          20840.21367781656,
          20773.144586005714,
          20655.781385313952,
          20444.373810023302,
          20762.25617068098,
          21630.920028779423,
          20446.260826947168,
          20887.90952437534,
          21258.39513209043,
          20073.822667501867,
          19994.722156243864,
          20665.23959986493,
          20443.396570411045,
          20462.875827849843,
          20514.544240169693,
          21199.679800594226,
          21143.593571329024,
          21008.865574450232,
          21075.766974256607,
          20187.53904896998,
          19532.838623609627,
          17950.11715305131,
          16249.7915450132,
          15844.829810425057,
          15692.040228168247,
          15839.481977477903,
          16205.989236949014,
          15625.261224250193,
          15808.497506537009,
          16357.041183090538,
          16209.898195398042,
          16233.525420579595,
          16866.28228703316,
          15887.43899951491,
          15919.969320887816,
          16933.324393923744,
          16725.513523004833,
          16258.257099997487,
          16418.798100598855,
          16793.246636603493,
          16071.749859876349,
          16951.45344160241,
          16801.63991055137,
          16171.19507351576,
          16716.15264691622,
          16755.8744495383,
          16799.39727092837,
          16076.869284748449,
          16496.662124260678,
          16146.625303619425,
          15802.742008544505,
          16385.912156526814,
          16483.92909688677,
          15641.166907248902,
          16855.00740910042,
          16425.030653431662,
          16529.749491487513,
          17250.831856939243,
          16428.143556733034,
          16988.868033542298,
          16505.783991055563,
          16335.645032566159,
          16423.268923637574,
          16318.651279032698,
          16774.22708655626,
          17034.48507749394,
          17080.297762117465,
          17617.56077825534,
          16827.495319148642,
          16936.741841325187,
          16899.00150284788,
          16731.847269288613,
          17129.414171978482,
          16996.236844267463,
          17035.99816052278,
          16923.610786377452,
          16744.401039692224,
          16872.060914957314,
          16639.432591123972,
          16639.470177262905,
          16612.863045886974,
          17172.178524359828,
          17620.24577782117,
          17089.363731577527,
          16915.893099183217,
          16939.11747805518,
          16192.43316950719,
          17121.3350796022,
          17935.15883350314,
          18507.015553666628,
          17618.32984848274,
          18021.70043652272,
          17564.146056095837,
          17391.78180543217,
          17558.912909059785,
          17826.24094910547,
          17645.712796316133,
          16536.85230799869,
          16569.208191137062,
          17335.074923000997,
          16719.340722495457,
          16764.00172927219,
          17242.433764255606,
          16650.973463270697,
          17403.546266918187,
          16714.128777896753,
          16573.01691988227,
          15526.99274588353,
          16628.02375233709,
          16577.54749524442,
          15590.241543983342,
          16167.664867697516,
          15936.976566881407,
          16373.139615519673,
          16654.194306253106,
          16926.619604986394,
          16145.339664918487,
          16722.723474999424,
          16853.66201807605,
          15838.671466122963,
          14640.212368819397,
          15613.0553665685,
          15883.870243810583,
          16465.088803809835,
          15929.82459926547,
          16372.54980226257,
          16160.033917746972,
          16283.111175303698,
          16110.931000346085,
          16370.709045202007,
          16542.530706219026,
          16661.981383242295,
          16491.35284119961,
          16895.87896207499,
          16638.763750600396,
          16709.930695302086,
          16824.048959332635,
          17312.4249374317,
          16651.031288099824,
          17001.07870995975,
          17131.132533150725,
          16973.23027225188,
          16567.719201787026,
          16547.59712499776,
          17181.274369981606,
          17449.215353216045,
          17074.2878348768,
          17853.985302120913,
          17218.262985680252,
          17940.552926313598,
          18175.952023209305,
          18561.832527932478,
          18923.740785993636,
          20077.390459459042,
          20911.926103406353,
          20895.97801553295,
          21619.81958908122,
          22425.62785791792,
          23274.18987791729,
          23918.89046282717,
          23927.111226034816,
          23733.673680142732,
          24574.134441581322,
          24932.286013243487,
          23823.999918228714,
          23442.25967127108,
          23655.58317590016,
          24222.324326180154,
          24426.420915574767,
          25146.609887416475,
          25381.53674820764,
          25629.909809265286,
          26287.248974328162,
          25415.451010491,
          26933.447222312447,
          27373.598256666217,
          24918.240362248383,
          25879.583928978303,
          25023.52781112399,
          24039.48799896147,
          25065.44310226431,
          24882.48919789307,
          24340.606941655045,
          25556.237121968064,
          25697.732551349094,
          24536.712139664218,
          26147.627069410868,
          27538.751751137668,
          26347.095744980965,
          27308.718798385005,
          27022.520589100663,
          27908.940524729904,
          26508.076214283705
         ],
         "yhoverformat": "$000,.0f"
        },
        {
         "mode": "markers+lines",
         "name": "最后第31时序预测值",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2023-02-02T20:00:00+08:00",
          "2023-02-03T08:00:00+08:00",
          "2023-02-03T20:00:00+08:00",
          "2023-02-04T08:00:00+08:00",
          "2023-02-04T20:00:00+08:00",
          "2023-02-05T08:00:00+08:00",
          "2023-02-05T20:00:00+08:00"
         ],
         "xhoverformat": "%y/%m/%d_%H:00",
         "y": [
          26508.076214283705,
          27396.049710322175,
          28288.959519273838,
          27129.858891916,
          25891.32044179691,
          26569.085264001973,
          26426.75908457581
         ],
         "yhoverformat": "$000,.0f"
        },
        {
         "mode": "markers+lines",
         "name": "最后第15时序预测值",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2023-01-25T20:00:00+08:00",
          "2023-01-26T08:00:00+08:00",
          "2023-01-26T20:00:00+08:00",
          "2023-01-27T08:00:00+08:00",
          "2023-01-27T20:00:00+08:00",
          "2023-01-28T08:00:00+08:00",
          "2023-01-28T20:00:00+08:00"
         ],
         "xhoverformat": "%y/%m/%d_%H:00",
         "y": [
          24918.240362248383,
          23776.662585610524,
          24250.21516876272,
          23914.07943704375,
          24561.572961200494,
          24455.30249022972,
          23557.333008729387
         ],
         "yhoverformat": "$000,.0f"
        }
       ],
       "layout": {
        "autosize": true,
        "margin": {
         "b": 10,
         "l": 50,
         "pad": 0,
         "r": 15,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "rgb(0,125,125)",
          "family": "SimHei",
          "size": 20
         },
         "text": "DLinear+3.5日预测,始于:2023年02月02日08时,止于:2023年02月05日20时"
        },
        "xaxis": {
         "autorange": false,
         "range": [
          "2023-01-13 05:45:33.9445",
          "2023-02-15 19:06:17.7516"
         ],
         "tickangle": -30,
         "tickformat": "%y/%m/%d_%H:",
         "title": {
          "text": "交易日期"
         },
         "type": "date"
        },
        "yaxis": {
         "autorange": false,
         "range": [
          19862.329839249964,
          32551.381359891213
         ],
         "tickformat": "$000,.0f",
         "title": {
          "text": "收盘价"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"ffac50e6-b9a9-4b98-bbe3-2b174a71e4c8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ffac50e6-b9a9-4b98-bbe3-2b174a71e4c8\")) {                    Plotly.newPlot(                        \"ffac50e6-b9a9-4b98-bbe3-2b174a71e4c8\",                        [{\"mode\":\"markers\",\"name\":\"\\u771f\\u5b9e\\u503c\",\"showlegend\":true,\"x\":[\"2022-02-15T20:00:00+08:00\",\"2022-02-16T08:00:00+08:00\",\"2022-02-16T20:00:00+08:00\",\"2022-02-17T08:00:00+08:00\",\"2022-02-17T20:00:00+08:00\",\"2022-02-18T08:00:00+08:00\",\"2022-02-18T20:00:00+08:00\",\"2022-02-19T08:00:00+08:00\",\"2022-02-19T20:00:00+08:00\",\"2022-02-20T08:00:00+08:00\",\"2022-02-20T20:00:00+08:00\",\"2022-02-21T08:00:00+08:00\",\"2022-02-21T20:00:00+08:00\",\"2022-02-22T08:00:00+08:00\",\"2022-02-22T20:00:00+08:00\",\"2022-02-23T08:00:00+08:00\",\"2022-02-23T20:00:00+08:00\",\"2022-02-24T08:00:00+08:00\",\"2022-02-24T20:00:00+08:00\",\"2022-02-25T08:00:00+08:00\",\"2022-02-25T20:00:00+08:00\",\"2022-02-26T08:00:00+08:00\",\"2022-02-26T20:00:00+08:00\",\"2022-02-27T08:00:00+08:00\",\"2022-02-27T20:00:00+08:00\",\"2022-02-28T08:00:00+08:00\",\"2022-02-28T20:00:00+08:00\",\"2022-03-01T08:00:00+08:00\",\"2022-03-01T20:00:00+08:00\",\"2022-03-02T08:00:00+08:00\",\"2022-03-02T20:00:00+08:00\",\"2022-03-03T08:00:00+08:00\",\"2022-03-03T20:00:00+08:00\",\"2022-03-04T08:00:00+08:00\",\"2022-03-04T20:00:00+08:00\",\"2022-03-05T08:00:00+08:00\",\"2022-03-05T20:00:00+08:00\",\"2022-03-06T08:00:00+08:00\",\"2022-03-06T20:00:00+08:00\",\"2022-03-07T08:00:00+08:00\",\"2022-03-07T20:00:00+08:00\",\"2022-03-08T08:00:00+08:00\",\"2022-03-08T20:00:00+08:00\",\"2022-03-09T08:00:00+08:00\",\"2022-03-09T20:00:00+08:00\",\"2022-03-10T08:00:00+08:00\",\"2022-03-10T20:00:00+08:00\",\"2022-03-11T08:00:00+08:00\",\"2022-03-11T20:00:00+08:00\",\"2022-03-12T08:00:00+08:00\",\"2022-03-12T20:00:00+08:00\",\"2022-03-13T08:00:00+08:00\",\"2022-03-13T20:00:00+08:00\",\"2022-03-14T08:00:00+08:00\",\"2022-03-14T20:00:00+08:00\",\"2022-03-15T08:00:00+08:00\",\"2022-03-15T20:00:00+08:00\",\"2022-03-16T08:00:00+08:00\",\"2022-03-16T20:00:00+08:00\",\"2022-03-17T08:00:00+08:00\",\"2022-03-17T20:00:00+08:00\",\"2022-03-18T08:00:00+08:00\",\"2022-03-18T20:00:00+08:00\",\"2022-03-19T08:00:00+08:00\",\"2022-03-19T20:00:00+08:00\",\"2022-03-20T08:00:00+08:00\",\"2022-03-20T20:00:00+08:00\",\"2022-03-21T08:00:00+08:00\",\"2022-03-21T20:00:00+08:00\",\"2022-03-22T08:00:00+08:00\",\"2022-03-22T20:00:00+08:00\",\"2022-03-23T08:00:00+08:00\",\"2022-03-23T20:00:00+08:00\",\"2022-03-24T08:00:00+08:00\",\"2022-03-24T20:00:00+08:00\",\"2022-03-25T08:00:00+08:00\",\"2022-03-25T20:00:00+08:00\",\"2022-03-26T08:00:00+08:00\",\"2022-03-26T20:00:00+08:00\",\"2022-03-27T08:00:00+08:00\",\"2022-03-27T20:00:00+08:00\",\"2022-03-28T08:00:00+08:00\",\"2022-03-28T20:00:00+08:00\",\"2022-03-29T08:00:00+08:00\",\"2022-03-29T20:00:00+08:00\",\"2022-03-30T08:00:00+08:00\",\"2022-03-30T20:00:00+08:00\",\"2022-03-31T08:00:00+08:00\",\"2022-03-31T20:00:00+08:00\",\"2022-04-01T08:00:00+08:00\",\"2022-04-01T20:00:00+08:00\",\"2022-04-02T08:00:00+08:00\",\"2022-04-02T20:00:00+08:00\",\"2022-04-03T08:00:00+08:00\",\"2022-04-03T20:00:00+08:00\",\"2022-04-04T08:00:00+08:00\",\"2022-04-04T20:00:00+08:00\",\"2022-04-05T08:00:00+08:00\",\"2022-04-05T20:00:00+08:00\",\"2022-04-06T08:00:00+08:00\",\"2022-04-06T20:00:00+08:00\",\"2022-04-07T08:00:00+08:00\",\"2022-04-07T20:00:00+08:00\",\"2022-04-08T08:00:00+08:00\",\"2022-04-08T20:00:00+08:00\",\"2022-04-09T08:00:00+08:00\",\"2022-04-09T20:00:00+08:00\",\"2022-04-10T08:00:00+08:00\",\"2022-04-10T20:00:00+08:00\",\"2022-04-11T08:00:00+08:00\",\"2022-04-11T20:00:00+08:00\",\"2022-04-12T08:00:00+08:00\",\"2022-04-12T20:00:00+08:00\",\"2022-04-13T08:00:00+08:00\",\"2022-04-13T20:00:00+08:00\",\"2022-04-14T08:00:00+08:00\",\"2022-04-14T20:00:00+08:00\",\"2022-04-15T08:00:00+08:00\",\"2022-04-15T20:00:00+08:00\",\"2022-04-16T08:00:00+08:00\",\"2022-04-16T20:00:00+08:00\",\"2022-04-17T08:00:00+08:00\",\"2022-04-17T20:00:00+08:00\",\"2022-04-18T08:00:00+08:00\",\"2022-04-18T20:00:00+08:00\",\"2022-04-19T08:00:00+08:00\",\"2022-04-19T20:00:00+08:00\",\"2022-04-20T08:00:00+08:00\",\"2022-04-20T20:00:00+08:00\",\"2022-04-21T08:00:00+08:00\",\"2022-04-21T20:00:00+08:00\",\"2022-04-22T08:00:00+08:00\",\"2022-04-22T20:00:00+08:00\",\"2022-04-23T08:00:00+08:00\",\"2022-04-23T20:00:00+08:00\",\"2022-04-24T08:00:00+08:00\",\"2022-04-24T20:00:00+08:00\",\"2022-04-25T08:00:00+08:00\",\"2022-04-25T20:00:00+08:00\",\"2022-04-26T08:00:00+08:00\",\"2022-04-26T20:00:00+08:00\",\"2022-04-27T08:00:00+08:00\",\"2022-04-27T20:00:00+08:00\",\"2022-04-28T08:00:00+08:00\",\"2022-04-28T20:00:00+08:00\",\"2022-04-29T08:00:00+08:00\",\"2022-04-29T20:00:00+08:00\",\"2022-04-30T08:00:00+08:00\",\"2022-04-30T20:00:00+08:00\",\"2022-05-01T08:00:00+08:00\",\"2022-05-01T20:00:00+08:00\",\"2022-05-02T08:00:00+08:00\",\"2022-05-02T20:00:00+08:00\",\"2022-05-03T08:00:00+08:00\",\"2022-05-03T20:00:00+08:00\",\"2022-05-04T08:00:00+08:00\",\"2022-05-04T20:00:00+08:00\",\"2022-05-05T08:00:00+08:00\",\"2022-05-05T20:00:00+08:00\",\"2022-05-06T08:00:00+08:00\",\"2022-05-06T20:00:00+08:00\",\"2022-05-07T08:00:00+08:00\",\"2022-05-07T20:00:00+08:00\",\"2022-05-08T08:00:00+08:00\",\"2022-05-08T20:00:00+08:00\",\"2022-05-09T08:00:00+08:00\",\"2022-05-09T20:00:00+08:00\",\"2022-05-10T08:00:00+08:00\",\"2022-05-10T20:00:00+08:00\",\"2022-05-11T08:00:00+08:00\",\"2022-05-11T20:00:00+08:00\",\"2022-05-12T08:00:00+08:00\",\"2022-05-12T20:00:00+08:00\",\"2022-05-13T08:00:00+08:00\",\"2022-05-13T20:00:00+08:00\",\"2022-05-14T08:00:00+08:00\",\"2022-05-14T20:00:00+08:00\",\"2022-05-15T08:00:00+08:00\",\"2022-05-15T20:00:00+08:00\",\"2022-05-16T08:00:00+08:00\",\"2022-05-16T20:00:00+08:00\",\"2022-05-17T08:00:00+08:00\",\"2022-05-17T20:00:00+08:00\",\"2022-05-18T08:00:00+08:00\",\"2022-05-18T20:00:00+08:00\",\"2022-05-19T08:00:00+08:00\",\"2022-05-19T20:00:00+08:00\",\"2022-05-20T08:00:00+08:00\",\"2022-05-20T20:00:00+08:00\",\"2022-05-21T08:00:00+08:00\",\"2022-05-21T20:00:00+08:00\",\"2022-05-22T08:00:00+08:00\",\"2022-05-22T20:00:00+08:00\",\"2022-05-23T08:00:00+08:00\",\"2022-05-23T20:00:00+08:00\",\"2022-05-24T08:00:00+08:00\",\"2022-05-24T20:00:00+08:00\",\"2022-05-25T08:00:00+08:00\",\"2022-05-25T20:00:00+08:00\",\"2022-05-26T08:00:00+08:00\",\"2022-05-26T20:00:00+08:00\",\"2022-05-27T08:00:00+08:00\",\"2022-05-27T20:00:00+08:00\",\"2022-05-28T08:00:00+08:00\",\"2022-05-28T20:00:00+08:00\",\"2022-05-29T08:00:00+08:00\",\"2022-05-29T20:00:00+08:00\",\"2022-05-30T08:00:00+08:00\",\"2022-05-30T20:00:00+08:00\",\"2022-05-31T08:00:00+08:00\",\"2022-05-31T20:00:00+08:00\",\"2022-06-01T08:00:00+08:00\",\"2022-06-01T20:00:00+08:00\",\"2022-06-02T08:00:00+08:00\",\"2022-06-02T20:00:00+08:00\",\"2022-06-03T08:00:00+08:00\",\"2022-06-03T20:00:00+08:00\",\"2022-06-04T08:00:00+08:00\",\"2022-06-04T20:00:00+08:00\",\"2022-06-05T08:00:00+08:00\",\"2022-06-05T20:00:00+08:00\",\"2022-06-06T08:00:00+08:00\",\"2022-06-06T20:00:00+08:00\",\"2022-06-07T08:00:00+08:00\",\"2022-06-07T20:00:00+08:00\",\"2022-06-08T08:00:00+08:00\",\"2022-06-08T20:00:00+08:00\",\"2022-06-09T08:00:00+08:00\",\"2022-06-09T20:00:00+08:00\",\"2022-06-10T08:00:00+08:00\",\"2022-06-10T20:00:00+08:00\",\"2022-06-11T08:00:00+08:00\",\"2022-06-11T20:00:00+08:00\",\"2022-06-12T08:00:00+08:00\",\"2022-06-12T20:00:00+08:00\",\"2022-06-13T08:00:00+08:00\",\"2022-06-13T20:00:00+08:00\",\"2022-06-14T08:00:00+08:00\",\"2022-06-14T20:00:00+08:00\",\"2022-06-15T08:00:00+08:00\",\"2022-06-15T20:00:00+08:00\",\"2022-06-16T08:00:00+08:00\",\"2022-06-16T20:00:00+08:00\",\"2022-06-17T08:00:00+08:00\",\"2022-06-17T20:00:00+08:00\",\"2022-06-18T08:00:00+08:00\",\"2022-06-18T20:00:00+08:00\",\"2022-06-19T08:00:00+08:00\",\"2022-06-19T20:00:00+08:00\",\"2022-06-20T08:00:00+08:00\",\"2022-06-20T20:00:00+08:00\",\"2022-06-21T08:00:00+08:00\",\"2022-06-21T20:00:00+08:00\",\"2022-06-22T08:00:00+08:00\",\"2022-06-22T20:00:00+08:00\",\"2022-06-23T08:00:00+08:00\",\"2022-06-23T20:00:00+08:00\",\"2022-06-24T08:00:00+08:00\",\"2022-06-24T20:00:00+08:00\",\"2022-06-25T08:00:00+08:00\",\"2022-06-25T20:00:00+08:00\",\"2022-06-26T08:00:00+08:00\",\"2022-06-26T20:00:00+08:00\",\"2022-06-27T08:00:00+08:00\",\"2022-06-27T20:00:00+08:00\",\"2022-06-28T08:00:00+08:00\",\"2022-06-28T20:00:00+08:00\",\"2022-06-29T08:00:00+08:00\",\"2022-06-29T20:00:00+08:00\",\"2022-06-30T08:00:00+08:00\",\"2022-06-30T20:00:00+08:00\",\"2022-07-01T08:00:00+08:00\",\"2022-07-01T20:00:00+08:00\",\"2022-07-02T08:00:00+08:00\",\"2022-07-02T20:00:00+08:00\",\"2022-07-03T08:00:00+08:00\",\"2022-07-03T20:00:00+08:00\",\"2022-07-04T08:00:00+08:00\",\"2022-07-04T20:00:00+08:00\",\"2022-07-05T08:00:00+08:00\",\"2022-07-05T20:00:00+08:00\",\"2022-07-06T08:00:00+08:00\",\"2022-07-06T20:00:00+08:00\",\"2022-07-07T08:00:00+08:00\",\"2022-07-07T20:00:00+08:00\",\"2022-07-08T08:00:00+08:00\",\"2022-07-08T20:00:00+08:00\",\"2022-07-09T08:00:00+08:00\",\"2022-07-09T20:00:00+08:00\",\"2022-07-10T08:00:00+08:00\",\"2022-07-10T20:00:00+08:00\",\"2022-07-11T08:00:00+08:00\",\"2022-07-11T20:00:00+08:00\",\"2022-07-12T08:00:00+08:00\",\"2022-07-12T20:00:00+08:00\",\"2022-07-13T08:00:00+08:00\",\"2022-07-13T20:00:00+08:00\",\"2022-07-14T08:00:00+08:00\",\"2022-07-14T20:00:00+08:00\",\"2022-07-15T08:00:00+08:00\",\"2022-07-15T20:00:00+08:00\",\"2022-07-16T08:00:00+08:00\",\"2022-07-16T20:00:00+08:00\",\"2022-07-17T08:00:00+08:00\",\"2022-07-17T20:00:00+08:00\",\"2022-07-18T08:00:00+08:00\",\"2022-07-18T20:00:00+08:00\",\"2022-07-19T08:00:00+08:00\",\"2022-07-19T20:00:00+08:00\",\"2022-07-20T08:00:00+08:00\",\"2022-07-20T20:00:00+08:00\",\"2022-07-21T08:00:00+08:00\",\"2022-07-21T20:00:00+08:00\",\"2022-07-22T08:00:00+08:00\",\"2022-07-22T20:00:00+08:00\",\"2022-07-23T08:00:00+08:00\",\"2022-07-23T20:00:00+08:00\",\"2022-07-24T08:00:00+08:00\",\"2022-07-24T20:00:00+08:00\",\"2022-07-25T08:00:00+08:00\",\"2022-07-25T20:00:00+08:00\",\"2022-07-26T08:00:00+08:00\",\"2022-07-26T20:00:00+08:00\",\"2022-07-27T08:00:00+08:00\",\"2022-07-27T20:00:00+08:00\",\"2022-07-28T08:00:00+08:00\",\"2022-07-28T20:00:00+08:00\",\"2022-07-29T08:00:00+08:00\",\"2022-07-29T20:00:00+08:00\",\"2022-07-30T08:00:00+08:00\",\"2022-07-30T20:00:00+08:00\",\"2022-07-31T08:00:00+08:00\",\"2022-07-31T20:00:00+08:00\",\"2022-08-01T08:00:00+08:00\",\"2022-08-01T20:00:00+08:00\",\"2022-08-02T08:00:00+08:00\",\"2022-08-02T20:00:00+08:00\",\"2022-08-03T08:00:00+08:00\",\"2022-08-03T20:00:00+08:00\",\"2022-08-04T08:00:00+08:00\",\"2022-08-04T20:00:00+08:00\",\"2022-08-05T08:00:00+08:00\",\"2022-08-05T20:00:00+08:00\",\"2022-08-06T08:00:00+08:00\",\"2022-08-06T20:00:00+08:00\",\"2022-08-07T08:00:00+08:00\",\"2022-08-07T20:00:00+08:00\",\"2022-08-08T08:00:00+08:00\",\"2022-08-08T20:00:00+08:00\",\"2022-08-09T08:00:00+08:00\",\"2022-08-09T20:00:00+08:00\",\"2022-08-10T08:00:00+08:00\",\"2022-08-10T20:00:00+08:00\",\"2022-08-11T08:00:00+08:00\",\"2022-08-11T20:00:00+08:00\",\"2022-08-12T08:00:00+08:00\",\"2022-08-12T20:00:00+08:00\",\"2022-08-13T08:00:00+08:00\",\"2022-08-13T20:00:00+08:00\",\"2022-08-14T08:00:00+08:00\",\"2022-08-14T20:00:00+08:00\",\"2022-08-15T08:00:00+08:00\",\"2022-08-15T20:00:00+08:00\",\"2022-08-16T08:00:00+08:00\",\"2022-08-16T20:00:00+08:00\",\"2022-08-17T08:00:00+08:00\",\"2022-08-17T20:00:00+08:00\",\"2022-08-18T08:00:00+08:00\",\"2022-08-18T20:00:00+08:00\",\"2022-08-19T08:00:00+08:00\",\"2022-08-19T20:00:00+08:00\",\"2022-08-20T08:00:00+08:00\",\"2022-08-20T20:00:00+08:00\",\"2022-08-21T08:00:00+08:00\",\"2022-08-21T20:00:00+08:00\",\"2022-08-22T08:00:00+08:00\",\"2022-08-22T20:00:00+08:00\",\"2022-08-23T08:00:00+08:00\",\"2022-08-23T20:00:00+08:00\",\"2022-08-24T08:00:00+08:00\",\"2022-08-24T20:00:00+08:00\",\"2022-08-25T08:00:00+08:00\",\"2022-08-25T20:00:00+08:00\",\"2022-08-26T08:00:00+08:00\",\"2022-08-26T20:00:00+08:00\",\"2022-08-27T08:00:00+08:00\",\"2022-08-27T20:00:00+08:00\",\"2022-08-28T08:00:00+08:00\",\"2022-08-28T20:00:00+08:00\",\"2022-08-29T08:00:00+08:00\",\"2022-08-29T20:00:00+08:00\",\"2022-08-30T08:00:00+08:00\",\"2022-08-30T20:00:00+08:00\",\"2022-08-31T08:00:00+08:00\",\"2022-08-31T20:00:00+08:00\",\"2022-09-01T08:00:00+08:00\",\"2022-09-01T20:00:00+08:00\",\"2022-09-02T08:00:00+08:00\",\"2022-09-02T20:00:00+08:00\",\"2022-09-03T08:00:00+08:00\",\"2022-09-03T20:00:00+08:00\",\"2022-09-04T08:00:00+08:00\",\"2022-09-04T20:00:00+08:00\",\"2022-09-05T08:00:00+08:00\",\"2022-09-05T20:00:00+08:00\",\"2022-09-06T08:00:00+08:00\",\"2022-09-06T20:00:00+08:00\",\"2022-09-07T08:00:00+08:00\",\"2022-09-07T20:00:00+08:00\",\"2022-09-08T08:00:00+08:00\",\"2022-09-08T20:00:00+08:00\",\"2022-09-09T08:00:00+08:00\",\"2022-09-09T20:00:00+08:00\",\"2022-09-10T08:00:00+08:00\",\"2022-09-10T20:00:00+08:00\",\"2022-09-11T08:00:00+08:00\",\"2022-09-11T20:00:00+08:00\",\"2022-09-12T08:00:00+08:00\",\"2022-09-12T20:00:00+08:00\",\"2022-09-13T08:00:00+08:00\",\"2022-09-13T20:00:00+08:00\",\"2022-09-14T08:00:00+08:00\",\"2022-09-14T20:00:00+08:00\",\"2022-09-15T08:00:00+08:00\",\"2022-09-15T20:00:00+08:00\",\"2022-09-16T08:00:00+08:00\",\"2022-09-16T20:00:00+08:00\",\"2022-09-17T08:00:00+08:00\",\"2022-09-17T20:00:00+08:00\",\"2022-09-18T08:00:00+08:00\",\"2022-09-18T20:00:00+08:00\",\"2022-09-19T08:00:00+08:00\",\"2022-09-19T20:00:00+08:00\",\"2022-09-20T08:00:00+08:00\",\"2022-09-20T20:00:00+08:00\",\"2022-09-21T08:00:00+08:00\",\"2022-09-21T20:00:00+08:00\",\"2022-09-22T08:00:00+08:00\",\"2022-09-22T20:00:00+08:00\",\"2022-09-23T08:00:00+08:00\",\"2022-09-23T20:00:00+08:00\",\"2022-09-24T08:00:00+08:00\",\"2022-09-24T20:00:00+08:00\",\"2022-09-25T08:00:00+08:00\",\"2022-09-25T20:00:00+08:00\",\"2022-09-26T08:00:00+08:00\",\"2022-09-26T20:00:00+08:00\",\"2022-09-27T08:00:00+08:00\",\"2022-09-27T20:00:00+08:00\",\"2022-09-28T08:00:00+08:00\",\"2022-09-28T20:00:00+08:00\",\"2022-09-29T08:00:00+08:00\",\"2022-09-29T20:00:00+08:00\",\"2022-09-30T08:00:00+08:00\",\"2022-09-30T20:00:00+08:00\",\"2022-10-01T08:00:00+08:00\",\"2022-10-01T20:00:00+08:00\",\"2022-10-02T08:00:00+08:00\",\"2022-10-02T20:00:00+08:00\",\"2022-10-03T08:00:00+08:00\",\"2022-10-03T20:00:00+08:00\",\"2022-10-04T08:00:00+08:00\",\"2022-10-04T20:00:00+08:00\",\"2022-10-05T08:00:00+08:00\",\"2022-10-05T20:00:00+08:00\",\"2022-10-06T08:00:00+08:00\",\"2022-10-06T20:00:00+08:00\",\"2022-10-07T08:00:00+08:00\",\"2022-10-07T20:00:00+08:00\",\"2022-10-08T08:00:00+08:00\",\"2022-10-08T20:00:00+08:00\",\"2022-10-09T08:00:00+08:00\",\"2022-10-09T20:00:00+08:00\",\"2022-10-10T08:00:00+08:00\",\"2022-10-10T20:00:00+08:00\",\"2022-10-11T08:00:00+08:00\",\"2022-10-11T20:00:00+08:00\",\"2022-10-12T08:00:00+08:00\",\"2022-10-12T20:00:00+08:00\",\"2022-10-13T08:00:00+08:00\",\"2022-10-13T20:00:00+08:00\",\"2022-10-14T08:00:00+08:00\",\"2022-10-14T20:00:00+08:00\",\"2022-10-15T08:00:00+08:00\",\"2022-10-15T20:00:00+08:00\",\"2022-10-16T08:00:00+08:00\",\"2022-10-16T20:00:00+08:00\",\"2022-10-17T08:00:00+08:00\",\"2022-10-17T20:00:00+08:00\",\"2022-10-18T08:00:00+08:00\",\"2022-10-18T20:00:00+08:00\",\"2022-10-19T08:00:00+08:00\",\"2022-10-19T20:00:00+08:00\",\"2022-10-20T08:00:00+08:00\",\"2022-10-20T20:00:00+08:00\",\"2022-10-21T08:00:00+08:00\",\"2022-10-21T20:00:00+08:00\",\"2022-10-22T08:00:00+08:00\",\"2022-10-22T20:00:00+08:00\",\"2022-10-23T08:00:00+08:00\",\"2022-10-23T20:00:00+08:00\",\"2022-10-24T08:00:00+08:00\",\"2022-10-24T20:00:00+08:00\",\"2022-10-25T08:00:00+08:00\",\"2022-10-25T20:00:00+08:00\",\"2022-10-26T08:00:00+08:00\",\"2022-10-26T20:00:00+08:00\",\"2022-10-27T08:00:00+08:00\",\"2022-10-27T20:00:00+08:00\",\"2022-10-28T08:00:00+08:00\",\"2022-10-28T20:00:00+08:00\",\"2022-10-29T08:00:00+08:00\",\"2022-10-29T20:00:00+08:00\",\"2022-10-30T08:00:00+08:00\",\"2022-10-30T20:00:00+08:00\",\"2022-10-31T08:00:00+08:00\",\"2022-10-31T20:00:00+08:00\",\"2022-11-01T08:00:00+08:00\",\"2022-11-01T20:00:00+08:00\",\"2022-11-02T08:00:00+08:00\",\"2022-11-02T20:00:00+08:00\",\"2022-11-03T08:00:00+08:00\",\"2022-11-03T20:00:00+08:00\",\"2022-11-04T08:00:00+08:00\",\"2022-11-04T20:00:00+08:00\",\"2022-11-05T08:00:00+08:00\",\"2022-11-05T20:00:00+08:00\",\"2022-11-06T08:00:00+08:00\",\"2022-11-06T20:00:00+08:00\",\"2022-11-07T08:00:00+08:00\",\"2022-11-07T20:00:00+08:00\",\"2022-11-08T08:00:00+08:00\",\"2022-11-08T20:00:00+08:00\",\"2022-11-09T08:00:00+08:00\",\"2022-11-09T20:00:00+08:00\",\"2022-11-10T08:00:00+08:00\",\"2022-11-10T20:00:00+08:00\",\"2022-11-11T08:00:00+08:00\",\"2022-11-11T20:00:00+08:00\",\"2022-11-12T08:00:00+08:00\",\"2022-11-12T20:00:00+08:00\",\"2022-11-13T08:00:00+08:00\",\"2022-11-13T20:00:00+08:00\",\"2022-11-14T08:00:00+08:00\",\"2022-11-14T20:00:00+08:00\",\"2022-11-15T08:00:00+08:00\",\"2022-11-15T20:00:00+08:00\",\"2022-11-16T08:00:00+08:00\",\"2022-11-16T20:00:00+08:00\",\"2022-11-17T08:00:00+08:00\",\"2022-11-17T20:00:00+08:00\",\"2022-11-18T08:00:00+08:00\",\"2022-11-18T20:00:00+08:00\",\"2022-11-19T08:00:00+08:00\",\"2022-11-19T20:00:00+08:00\",\"2022-11-20T08:00:00+08:00\",\"2022-11-20T20:00:00+08:00\",\"2022-11-21T08:00:00+08:00\",\"2022-11-21T20:00:00+08:00\",\"2022-11-22T08:00:00+08:00\",\"2022-11-22T20:00:00+08:00\",\"2022-11-23T08:00:00+08:00\",\"2022-11-23T20:00:00+08:00\",\"2022-11-24T08:00:00+08:00\",\"2022-11-24T20:00:00+08:00\",\"2022-11-25T08:00:00+08:00\",\"2022-11-25T20:00:00+08:00\",\"2022-11-26T08:00:00+08:00\",\"2022-11-26T20:00:00+08:00\",\"2022-11-27T08:00:00+08:00\",\"2022-11-27T20:00:00+08:00\",\"2022-11-28T08:00:00+08:00\",\"2022-11-28T20:00:00+08:00\",\"2022-11-29T08:00:00+08:00\",\"2022-11-29T20:00:00+08:00\",\"2022-11-30T08:00:00+08:00\",\"2022-11-30T20:00:00+08:00\",\"2022-12-01T08:00:00+08:00\",\"2022-12-01T20:00:00+08:00\",\"2022-12-02T08:00:00+08:00\",\"2022-12-02T20:00:00+08:00\",\"2022-12-03T08:00:00+08:00\",\"2022-12-03T20:00:00+08:00\",\"2022-12-04T08:00:00+08:00\",\"2022-12-04T20:00:00+08:00\",\"2022-12-05T08:00:00+08:00\",\"2022-12-05T20:00:00+08:00\",\"2022-12-06T08:00:00+08:00\",\"2022-12-06T20:00:00+08:00\",\"2022-12-07T08:00:00+08:00\",\"2022-12-07T20:00:00+08:00\",\"2022-12-08T08:00:00+08:00\",\"2022-12-08T20:00:00+08:00\",\"2022-12-09T08:00:00+08:00\",\"2022-12-09T20:00:00+08:00\",\"2022-12-10T08:00:00+08:00\",\"2022-12-10T20:00:00+08:00\",\"2022-12-11T08:00:00+08:00\",\"2022-12-11T20:00:00+08:00\",\"2022-12-12T08:00:00+08:00\",\"2022-12-12T20:00:00+08:00\",\"2022-12-13T08:00:00+08:00\",\"2022-12-13T20:00:00+08:00\",\"2022-12-14T08:00:00+08:00\",\"2022-12-14T20:00:00+08:00\",\"2022-12-15T08:00:00+08:00\",\"2022-12-15T20:00:00+08:00\",\"2022-12-16T08:00:00+08:00\",\"2022-12-16T20:00:00+08:00\",\"2022-12-17T08:00:00+08:00\",\"2022-12-17T20:00:00+08:00\",\"2022-12-18T08:00:00+08:00\",\"2022-12-18T20:00:00+08:00\",\"2022-12-19T08:00:00+08:00\",\"2022-12-19T20:00:00+08:00\",\"2022-12-20T08:00:00+08:00\",\"2022-12-20T20:00:00+08:00\",\"2022-12-21T08:00:00+08:00\",\"2022-12-21T20:00:00+08:00\",\"2022-12-22T08:00:00+08:00\",\"2022-12-22T20:00:00+08:00\",\"2022-12-23T08:00:00+08:00\",\"2022-12-23T20:00:00+08:00\",\"2022-12-24T08:00:00+08:00\",\"2022-12-24T20:00:00+08:00\",\"2022-12-25T08:00:00+08:00\",\"2022-12-25T20:00:00+08:00\",\"2022-12-26T08:00:00+08:00\",\"2022-12-26T20:00:00+08:00\",\"2022-12-27T08:00:00+08:00\",\"2022-12-27T20:00:00+08:00\",\"2022-12-28T08:00:00+08:00\",\"2022-12-28T20:00:00+08:00\",\"2022-12-29T08:00:00+08:00\",\"2022-12-29T20:00:00+08:00\",\"2022-12-30T08:00:00+08:00\",\"2022-12-30T20:00:00+08:00\",\"2022-12-31T08:00:00+08:00\",\"2022-12-31T20:00:00+08:00\",\"2023-01-01T08:00:00+08:00\",\"2023-01-01T20:00:00+08:00\",\"2023-01-02T08:00:00+08:00\",\"2023-01-02T20:00:00+08:00\",\"2023-01-03T08:00:00+08:00\",\"2023-01-03T20:00:00+08:00\",\"2023-01-04T08:00:00+08:00\",\"2023-01-04T20:00:00+08:00\",\"2023-01-05T08:00:00+08:00\",\"2023-01-05T20:00:00+08:00\",\"2023-01-06T08:00:00+08:00\",\"2023-01-06T20:00:00+08:00\",\"2023-01-07T08:00:00+08:00\",\"2023-01-07T20:00:00+08:00\",\"2023-01-08T08:00:00+08:00\",\"2023-01-08T20:00:00+08:00\",\"2023-01-09T08:00:00+08:00\",\"2023-01-09T20:00:00+08:00\",\"2023-01-10T08:00:00+08:00\",\"2023-01-10T20:00:00+08:00\",\"2023-01-11T08:00:00+08:00\",\"2023-01-11T20:00:00+08:00\",\"2023-01-12T08:00:00+08:00\",\"2023-01-12T20:00:00+08:00\",\"2023-01-13T08:00:00+08:00\",\"2023-01-13T20:00:00+08:00\",\"2023-01-14T08:00:00+08:00\",\"2023-01-14T20:00:00+08:00\",\"2023-01-15T08:00:00+08:00\",\"2023-01-15T20:00:00+08:00\",\"2023-01-16T08:00:00+08:00\",\"2023-01-16T20:00:00+08:00\",\"2023-01-17T08:00:00+08:00\",\"2023-01-17T20:00:00+08:00\",\"2023-01-18T08:00:00+08:00\",\"2023-01-18T20:00:00+08:00\",\"2023-01-19T08:00:00+08:00\",\"2023-01-19T20:00:00+08:00\",\"2023-01-20T08:00:00+08:00\",\"2023-01-20T20:00:00+08:00\",\"2023-01-21T08:00:00+08:00\",\"2023-01-21T20:00:00+08:00\",\"2023-01-22T08:00:00+08:00\",\"2023-01-22T20:00:00+08:00\",\"2023-01-23T08:00:00+08:00\",\"2023-01-23T20:00:00+08:00\",\"2023-01-24T08:00:00+08:00\",\"2023-01-24T20:00:00+08:00\",\"2023-01-25T08:00:00+08:00\",\"2023-01-25T20:00:00+08:00\",\"2023-01-26T08:00:00+08:00\",\"2023-01-26T20:00:00+08:00\",\"2023-01-27T08:00:00+08:00\",\"2023-01-27T20:00:00+08:00\",\"2023-01-28T08:00:00+08:00\",\"2023-01-28T20:00:00+08:00\",\"2023-01-29T08:00:00+08:00\",\"2023-01-29T20:00:00+08:00\",\"2023-01-30T08:00:00+08:00\",\"2023-01-30T20:00:00+08:00\",\"2023-01-31T08:00:00+08:00\",\"2023-01-31T20:00:00+08:00\",\"2023-02-01T08:00:00+08:00\",\"2023-02-01T20:00:00+08:00\",\"2023-02-02T08:00:00+08:00\"],\"xhoverformat\":\"%y/%m/%d_%H:00\",\"y\":[44544.859375,44173.1484375,43873.55859375,43164.05078125,40515.69921875,40178.94140625,39974.44140625,39842.5390625,40079.171875,38343.98828125,38386.890625,37577.328125,37008.16015625,37519.48046875,38230.328125,38782.91015625,37250.01171875,35442.21875,38327.2109375,38745.01171875,39219.171875,38928.2109375,39116.71875,39479.80078125,37699.0703125,38333.6484375,43160.0,43609.9609375,44421.19921875,44103.28125,43892.98046875,43382.23046875,42454.0,41662.0703125,39148.66015625,39129.58984375,39397.9609375,38352.48046875,38420.80859375,38337.05078125,37988.0,38983.05078125,38730.62890625,42151.66015625,41941.7109375,39076.37890625,39422.0,39893.6796875,38729.5703125,39079.16015625,38807.359375,38741.12109375,37777.33984375,38897.640625,39671.37109375,38645.01171875,39280.328125,40521.6015625,41114.0,40748.51171875,40917.8984375,40390.41015625,41757.51171875,41721.78125,42201.12890625,41586.859375,41262.109375,41244.28125,41002.25,42983.0,42364.12890625,42039.78125,42882.76171875,42968.421875,43991.4609375,44585.28125,44313.16015625,44334.48828125,44511.26953125,44569.5,46827.76171875,47252.5390625,47122.2109375,47896.1015625,47434.80078125,47319.91015625,47067.98828125,47152.01171875,45510.33984375,45064.26171875,46283.48828125,46542.83984375,45811.0,46252.58984375,46407.3515625,46160.08984375,46580.51171875,46683.37109375,45497.55078125,44819.19140625,43170.46875,43766.73828125,43444.19140625,43298.7890625,42252.01171875,42445.1015625,42753.96875,42680.359375,42158.8515625,41067.62890625,39530.44921875,40378.0,40074.94140625,39722.640625,41147.7890625,40919.98828125,39942.37890625,40220.0,40551.8984375,40416.94921875,40378.7109375,40464.76953125,39678.12109375,38998.5390625,40801.12890625,40687.30078125,41493.1796875,42090.0,41358.19140625,42440.0,40480.01171875,40585.5390625,39709.1796875,39602.9296875,39441.6015625,39609.109375,39450.12890625,38832.2890625,40426.078125,40467.03125,38112.6484375,39017.19140625,39235.71875,39679.30859375,39742.0703125,38749.5390625,38596.109375,38580.01171875,37630.80078125,37942.78125,38468.3515625,38528.76953125,38525.16015625,38570.01171875,37728.94921875,39002.30078125,39690.0,39492.828125,36552.96875,35780.83984375,36013.76953125,36055.859375,35472.390625,34786.23828125,34038.3984375,33122.30859375,30076.310546875,31622.240234375,31017.099609375,31533.150390625,29103.939453125,28472.98046875,29029.75,30668.080078125,29287.05078125,29074.240234375,30086.740234375,30241.4609375,31328.890625,29950.94921875,29874.009765625,30319.470703125,30444.9296875,29833.580078125,28715.3203125,29500.19921875,30319.23046875,30414.0703125,29201.009765625,29288.0,29445.060546875,29897.51953125,30293.939453125,30397.109375,29109.150390625,29295.740234375,29654.580078125,29409.009765625,29542.150390625,28998.400390625,29201.349609375,28912.9609375,28629.80078125,28828.01953125,29031.330078125,29213.849609375,29468.099609375,30665.369140625,31734.220703125,31767.0,31801.0390625,31609.16015625,29805.830078125,30173.669921875,30452.619140625,29758.189453125,29700.2109375,29681.689453125,29864.0390625,29718.689453125,29919.2109375,31396.7109375,31373.099609375,29521.5,31125.330078125,30410.849609375,30204.76953125,30299.470703125,30109.9296875,30020.560546875,29091.880859375,28734.009765625,28424.69921875,27457.390625,26574.529296875,23729.4296875,22487.41015625,21994.560546875,22136.41015625,21177.029296875,22583.720703125,21044.55078125,20401.310546875,20972.0390625,20468.810546875,19243.98046875,18970.7890625,19692.5,20574.0,20801.7890625,20573.890625,20962.419921875,20723.51953125,20506.30078125,19987.990234375,20638.7890625,21110.130859375,21169.529296875,21237.689453125,21340.220703125,21491.189453125,21708.599609375,21038.0703125,21304.55078125,20742.560546875,20998.130859375,20281.2890625,20104.80078125,20123.009765625,19101.359375,19942.2109375,19191.029296875,19279.80078125,19195.2109375,19252.810546875,19089.80078125,19315.830078125,19806.490234375,20236.7109375,19520.390625,20175.830078125,20103.4296875,20564.509765625,20507.830078125,21624.98046875,21583.98046875,21594.75,21447.2890625,21591.830078125,21301.83984375,20862.470703125,20473.060546875,19963.609375,19773.900390625,19328.75,19843.890625,20234.869140625,19738.130859375,20588.83984375,20769.740234375,20830.0390625,20616.689453125,21195.599609375,21456.7890625,20798.16015625,22133.310546875,22432.580078125,21974.490234375,23396.619140625,23728.55078125,23223.30078125,22606.0390625,23152.189453125,23610.580078125,22684.830078125,22227.529296875,22451.0703125,22657.630859375,22579.6796875,21941.25,21310.900390625,21090.220703125,21254.669921875,21200.23046875,22952.44921875,23114.51953125,23842.9296875,23720.560546875,23773.75,23956.380859375,23643.509765625,23801.0703125,23293.3203125,23227.7890625,23268.009765625,22882.609375,22987.7890625,23402.830078125,22818.369140625,22885.009765625,22622.98046875,23405.130859375,23312.419921875,23157.419921875,22954.2109375,23038.48046875,23174.390625,24166.119140625,23810.0,23263.919921875,23149.94921875,23123.94921875,23954.05078125,24559.369140625,23934.390625,23672.990234375,24403.6796875,24435.91015625,24441.380859375,24564.490234375,24305.240234375,24049.48046875,24094.8203125,24038.029296875,23854.740234375,23746.650390625,23342.66015625,23534.4296875,23191.19921875,21467.91015625,20834.390625,21233.66015625,21140.0703125,21524.390625,21515.609375,21248.7109375,21399.830078125,21456.33984375,21529.119140625,21438.919921875,21368.080078125,21720.310546875,21559.0390625,21189.76953125,20241.05078125,20211.2109375,20037.599609375,20034.970703125,19555.609375,19825.08984375,20285.73046875,20403.80078125,19811.66015625,20314.779296875,20050.01953125,20074.009765625,20131.4609375,20090.810546875,19951.859375,19798.759765625,19831.900390625,19850.7890625,20000.30078125,19701.94921875,19796.83984375,19918.330078125,18790.609375,18739.58984375,19292.83984375,19311.150390625,19319.76953125,20937.720703125,21360.109375,21285.109375,21648.33984375,21613.029296875,21826.869140625,22312.75,22395.740234375,22522.740234375,20173.5703125,20225.349609375,20226.7109375,20148.349609375,19701.880859375,19867.8203125,19803.30078125,19810.220703125,20113.619140625,19922.919921875,19416.1796875,18684.869140625,19537.01953125,19219.490234375,18875.0,19165.9296875,18461.359375,19229.810546875,19401.630859375,18862.720703125,19289.91015625,19051.630859375,18920.5,19117.939453125,18807.380859375,18876.619140625,19227.8203125,20229.259765625,19079.130859375,18955.4296875,19412.8203125,19463.0390625,19591.509765625,19436.2890625,19422.609375,19312.0390625,19310.94921875,19192.4296875,19056.80078125,19245.41015625,19629.080078125,19939.0,20337.8203125,20027.4296875,20158.259765625,20245.220703125,19960.669921875,19998.900390625,19530.08984375,19534.2109375,19417.9609375,19468.349609375,19439.01953125,19331.619140625,19131.869140625,19153.6796875,19060.0,19114.619140625,19155.529296875,18753.19921875,19375.130859375,19596.25,19176.9296875,19167.330078125,19069.390625,19144.689453125,19262.98046875,19457.580078125,19549.859375,19563.58984375,19327.439453125,19200.83984375,19123.970703125,19208.08984375,19041.919921875,18945.650390625,19164.369140625,19178.66015625,19204.349609375,19151.779296875,19570.400390625,19423.630859375,19329.720703125,19302.130859375,20080.0703125,20607.810546875,20771.58984375,20622.83984375,20295.109375,20173.740234375,20591.83984375,20698.890625,20809.669921875,20769.83984375,20627.48046875,20730.5390625,20490.740234375,20522.7890625,20483.619140625,20431.330078125,20151.83984375,20130.640625,20207.8203125,20558.4609375,21148.51953125,21404.630859375,21299.369140625,21255.080078125,20905.580078125,20733.4609375,20591.130859375,19699.0390625,18547.23046875,17818.080078125,15922.8095703125,16397.5703125,17601.150390625,17350.69921875,17070.310546875,16866.919921875,16812.080078125,16659.23046875,16329.849609375,16749.80078125,16619.4609375,16781.619140625,16900.5703125,16707.900390625,16662.759765625,16595.080078125,16692.560546875,16749.880859375,16700.44921875,16676.259765625,16700.6796875,16525.380859375,16280.23046875,16084.419921875,15781.2900390625,15747.8798828125,16226.9404296875,16585.76953125,16603.109375,16572.109375,16598.94921875,16531.640625,16522.140625,16580.310546875,16458.5703125,16550.91015625,16428.779296875,16213.2900390625,16212.91015625,16497.640625,16442.529296875,16879.3203125,17163.640625,17102.470703125,16977.369140625,16996.919921875,17092.740234375,16943.4296875,16885.19921875,16948.640625,17105.69921875,17310.509765625,16966.349609375,16983.900390625,17088.9609375,16793.2890625,16836.640625,16851.01953125,17224.099609375,17241.439453125,17128.560546875,17168.609375,17127.490234375,17167.4296875,17085.05078125,16988.619140625,17209.830078125,17444.630859375,17774.69921875,17824.9296875,17803.150390625,17720.330078125,17356.33984375,17027.08984375,16632.119140625,16708.220703125,16776.51953125,16711.349609375,16738.2109375,16734.4609375,16438.880859375,16814.25,16895.560546875,16878.470703125,16824.669921875,16834.80078125,16821.4296875,16844.01953125,16778.5,16824.41015625,16836.119140625,16821.509765625,16832.109375,16864.0390625,16919.390625,16832.400390625,16706.359375,16678.98046875,16547.310546875,16598.69921875,16633.470703125,16496.26953125,16607.48046875,16567.150390625,16542.400390625,16556.66015625,16616.75,16735.109375,16672.869140625,16721.029296875,16675.1796875,16835.349609375,16850.359375,16833.599609375,16831.849609375,16738.220703125,16950.650390625,16918.30078125,16943.5703125,16927.419921875,17127.830078125,17238.9296875,17178.259765625,17252.890625,17440.66015625,17437.75,17943.259765625,18200.80078125,18846.619140625,18920.94921875,19930.009765625,20721.73046875,20954.919921875,20722.23046875,20871.5,20817.560546875,21185.650390625,21219.75,21134.810546875,21204.669921875,20677.470703125,20743.720703125,21071.58984375,20963.509765625,22667.2109375,22905.650390625,22783.55078125,22783.609375,22707.880859375,22904.580078125,22916.44921875,22917.66015625,22632.890625,22598.470703125,23060.939453125,22992.939453125,23009.650390625,22967.470703125,23074.16015625,22981.509765625,23022.599609375,23432.169921875,23742.30078125,23077.470703125,22826.150390625,22865.4296875,23125.130859375,23077.51953125,23732.66015625,23981.580078125],\"yhoverformat\":\"$000,.0f\",\"type\":\"scatter\"},{\"mode\":\"markers\",\"name\":\"\\u9a8c\\u8bc1\\u96c6\\u9884\\u6d4b\\u503c\",\"showlegend\":true,\"x\":[\"2022-02-16T08:00:00+08:00\",\"2022-02-16T20:00:00+08:00\",\"2022-02-17T08:00:00+08:00\",\"2022-02-17T20:00:00+08:00\",\"2022-02-18T08:00:00+08:00\",\"2022-02-18T20:00:00+08:00\",\"2022-02-19T08:00:00+08:00\",\"2022-02-19T20:00:00+08:00\",\"2022-02-20T08:00:00+08:00\",\"2022-02-20T20:00:00+08:00\",\"2022-02-21T08:00:00+08:00\",\"2022-02-21T20:00:00+08:00\",\"2022-02-22T08:00:00+08:00\",\"2022-02-22T20:00:00+08:00\",\"2022-02-23T08:00:00+08:00\",\"2022-02-23T20:00:00+08:00\",\"2022-02-24T08:00:00+08:00\",\"2022-02-24T20:00:00+08:00\",\"2022-02-25T08:00:00+08:00\",\"2022-02-25T20:00:00+08:00\",\"2022-02-26T08:00:00+08:00\",\"2022-02-26T20:00:00+08:00\",\"2022-02-27T08:00:00+08:00\",\"2022-02-27T20:00:00+08:00\",\"2022-02-28T08:00:00+08:00\",\"2022-02-28T20:00:00+08:00\",\"2022-03-01T08:00:00+08:00\",\"2022-03-01T20:00:00+08:00\",\"2022-03-02T08:00:00+08:00\",\"2022-03-02T20:00:00+08:00\",\"2022-03-03T08:00:00+08:00\",\"2022-03-03T20:00:00+08:00\",\"2022-03-04T08:00:00+08:00\",\"2022-03-04T20:00:00+08:00\",\"2022-03-05T08:00:00+08:00\",\"2022-03-05T20:00:00+08:00\",\"2022-03-06T08:00:00+08:00\",\"2022-03-06T20:00:00+08:00\",\"2022-03-07T08:00:00+08:00\",\"2022-03-07T20:00:00+08:00\",\"2022-03-08T08:00:00+08:00\",\"2022-03-08T20:00:00+08:00\",\"2022-03-09T08:00:00+08:00\",\"2022-03-09T20:00:00+08:00\",\"2022-03-10T08:00:00+08:00\",\"2022-03-10T20:00:00+08:00\",\"2022-03-11T08:00:00+08:00\",\"2022-03-11T20:00:00+08:00\",\"2022-03-12T08:00:00+08:00\",\"2022-03-12T20:00:00+08:00\",\"2022-03-13T08:00:00+08:00\",\"2022-03-13T20:00:00+08:00\",\"2022-03-14T08:00:00+08:00\",\"2022-03-14T20:00:00+08:00\",\"2022-03-15T08:00:00+08:00\",\"2022-03-15T20:00:00+08:00\",\"2022-03-16T08:00:00+08:00\",\"2022-03-16T20:00:00+08:00\",\"2022-03-17T08:00:00+08:00\",\"2022-03-17T20:00:00+08:00\",\"2022-03-18T08:00:00+08:00\",\"2022-03-18T20:00:00+08:00\",\"2022-03-19T08:00:00+08:00\",\"2022-03-19T20:00:00+08:00\",\"2022-03-20T08:00:00+08:00\",\"2022-03-20T20:00:00+08:00\",\"2022-03-21T08:00:00+08:00\",\"2022-03-21T20:00:00+08:00\",\"2022-03-22T08:00:00+08:00\",\"2022-03-22T20:00:00+08:00\",\"2022-03-23T08:00:00+08:00\",\"2022-03-23T20:00:00+08:00\",\"2022-03-24T08:00:00+08:00\",\"2022-03-24T20:00:00+08:00\",\"2022-03-25T08:00:00+08:00\",\"2022-03-25T20:00:00+08:00\",\"2022-03-26T08:00:00+08:00\",\"2022-03-26T20:00:00+08:00\",\"2022-03-27T08:00:00+08:00\",\"2022-03-27T20:00:00+08:00\",\"2022-03-28T08:00:00+08:00\",\"2022-03-28T20:00:00+08:00\",\"2022-03-29T08:00:00+08:00\",\"2022-03-29T20:00:00+08:00\",\"2022-03-30T08:00:00+08:00\",\"2022-03-30T20:00:00+08:00\",\"2022-03-31T08:00:00+08:00\",\"2022-03-31T20:00:00+08:00\",\"2022-04-01T08:00:00+08:00\",\"2022-04-01T20:00:00+08:00\",\"2022-04-02T08:00:00+08:00\",\"2022-04-02T20:00:00+08:00\",\"2022-04-03T08:00:00+08:00\",\"2022-04-03T20:00:00+08:00\",\"2022-04-04T08:00:00+08:00\",\"2022-04-04T20:00:00+08:00\",\"2022-04-05T08:00:00+08:00\",\"2022-04-05T20:00:00+08:00\",\"2022-04-06T08:00:00+08:00\",\"2022-04-06T20:00:00+08:00\",\"2022-04-07T08:00:00+08:00\",\"2022-04-07T20:00:00+08:00\",\"2022-04-08T08:00:00+08:00\",\"2022-04-08T20:00:00+08:00\",\"2022-04-09T08:00:00+08:00\",\"2022-04-09T20:00:00+08:00\",\"2022-04-10T08:00:00+08:00\",\"2022-04-10T20:00:00+08:00\",\"2022-04-11T08:00:00+08:00\",\"2022-04-11T20:00:00+08:00\",\"2022-04-12T08:00:00+08:00\",\"2022-04-12T20:00:00+08:00\",\"2022-04-13T08:00:00+08:00\",\"2022-04-13T20:00:00+08:00\",\"2022-04-14T08:00:00+08:00\",\"2022-04-14T20:00:00+08:00\",\"2022-04-15T08:00:00+08:00\",\"2022-04-15T20:00:00+08:00\",\"2022-04-16T08:00:00+08:00\",\"2022-04-16T20:00:00+08:00\",\"2022-04-17T08:00:00+08:00\",\"2022-04-17T20:00:00+08:00\",\"2022-04-18T08:00:00+08:00\",\"2022-04-18T20:00:00+08:00\",\"2022-04-19T08:00:00+08:00\",\"2022-04-19T20:00:00+08:00\",\"2022-04-20T08:00:00+08:00\",\"2022-04-20T20:00:00+08:00\",\"2022-04-21T08:00:00+08:00\",\"2022-04-21T20:00:00+08:00\",\"2022-04-22T08:00:00+08:00\",\"2022-04-22T20:00:00+08:00\",\"2022-04-23T08:00:00+08:00\",\"2022-04-23T20:00:00+08:00\",\"2022-04-24T08:00:00+08:00\",\"2022-04-24T20:00:00+08:00\",\"2022-04-25T08:00:00+08:00\",\"2022-04-25T20:00:00+08:00\",\"2022-04-26T08:00:00+08:00\",\"2022-04-26T20:00:00+08:00\",\"2022-04-27T08:00:00+08:00\",\"2022-04-27T20:00:00+08:00\",\"2022-04-28T08:00:00+08:00\",\"2022-04-28T20:00:00+08:00\",\"2022-04-29T08:00:00+08:00\",\"2022-04-29T20:00:00+08:00\",\"2022-04-30T08:00:00+08:00\",\"2022-04-30T20:00:00+08:00\",\"2022-05-01T08:00:00+08:00\",\"2022-05-01T20:00:00+08:00\",\"2022-05-02T08:00:00+08:00\",\"2022-05-02T20:00:00+08:00\",\"2022-05-03T08:00:00+08:00\",\"2022-05-03T20:00:00+08:00\",\"2022-05-04T08:00:00+08:00\",\"2022-05-04T20:00:00+08:00\",\"2022-05-05T08:00:00+08:00\",\"2022-05-05T20:00:00+08:00\",\"2022-05-06T08:00:00+08:00\",\"2022-05-06T20:00:00+08:00\",\"2022-05-07T08:00:00+08:00\",\"2022-05-07T20:00:00+08:00\",\"2022-05-08T08:00:00+08:00\",\"2022-05-08T20:00:00+08:00\",\"2022-05-09T08:00:00+08:00\",\"2022-05-09T20:00:00+08:00\",\"2022-05-10T08:00:00+08:00\",\"2022-05-10T20:00:00+08:00\",\"2022-05-11T08:00:00+08:00\",\"2022-05-11T20:00:00+08:00\",\"2022-05-12T08:00:00+08:00\",\"2022-05-12T20:00:00+08:00\",\"2022-05-13T08:00:00+08:00\",\"2022-05-13T20:00:00+08:00\",\"2022-05-14T08:00:00+08:00\",\"2022-05-14T20:00:00+08:00\",\"2022-05-15T08:00:00+08:00\",\"2022-05-15T20:00:00+08:00\",\"2022-05-16T08:00:00+08:00\",\"2022-05-16T20:00:00+08:00\",\"2022-05-17T08:00:00+08:00\",\"2022-05-17T20:00:00+08:00\",\"2022-05-18T08:00:00+08:00\",\"2022-05-18T20:00:00+08:00\",\"2022-05-19T08:00:00+08:00\",\"2022-05-19T20:00:00+08:00\",\"2022-05-20T08:00:00+08:00\",\"2022-05-20T20:00:00+08:00\",\"2022-05-21T08:00:00+08:00\",\"2022-05-21T20:00:00+08:00\",\"2022-05-22T08:00:00+08:00\",\"2022-05-22T20:00:00+08:00\",\"2022-05-23T08:00:00+08:00\",\"2022-05-23T20:00:00+08:00\",\"2022-05-24T08:00:00+08:00\",\"2022-05-24T20:00:00+08:00\",\"2022-05-25T08:00:00+08:00\",\"2022-05-25T20:00:00+08:00\",\"2022-05-26T08:00:00+08:00\",\"2022-05-26T20:00:00+08:00\",\"2022-05-27T08:00:00+08:00\",\"2022-05-27T20:00:00+08:00\",\"2022-05-28T08:00:00+08:00\",\"2022-05-28T20:00:00+08:00\",\"2022-05-29T08:00:00+08:00\",\"2022-05-29T20:00:00+08:00\",\"2022-05-30T08:00:00+08:00\",\"2022-05-30T20:00:00+08:00\",\"2022-05-31T08:00:00+08:00\",\"2022-05-31T20:00:00+08:00\",\"2022-06-01T08:00:00+08:00\",\"2022-06-01T20:00:00+08:00\",\"2022-06-02T08:00:00+08:00\",\"2022-06-02T20:00:00+08:00\",\"2022-06-03T08:00:00+08:00\",\"2022-06-03T20:00:00+08:00\",\"2022-06-04T08:00:00+08:00\",\"2022-06-04T20:00:00+08:00\",\"2022-06-05T08:00:00+08:00\",\"2022-06-05T20:00:00+08:00\",\"2022-06-06T08:00:00+08:00\",\"2022-06-06T20:00:00+08:00\",\"2022-06-07T08:00:00+08:00\",\"2022-06-07T20:00:00+08:00\",\"2022-06-08T08:00:00+08:00\",\"2022-06-08T20:00:00+08:00\",\"2022-06-09T08:00:00+08:00\",\"2022-06-09T20:00:00+08:00\",\"2022-06-10T08:00:00+08:00\",\"2022-06-10T20:00:00+08:00\",\"2022-06-11T08:00:00+08:00\",\"2022-06-11T20:00:00+08:00\",\"2022-06-12T08:00:00+08:00\",\"2022-06-12T20:00:00+08:00\",\"2022-06-13T08:00:00+08:00\",\"2022-06-13T20:00:00+08:00\",\"2022-06-14T08:00:00+08:00\",\"2022-06-14T20:00:00+08:00\",\"2022-06-15T08:00:00+08:00\",\"2022-06-15T20:00:00+08:00\",\"2022-06-16T08:00:00+08:00\",\"2022-06-16T20:00:00+08:00\",\"2022-06-17T08:00:00+08:00\",\"2022-06-17T20:00:00+08:00\",\"2022-06-18T08:00:00+08:00\",\"2022-06-18T20:00:00+08:00\",\"2022-06-19T08:00:00+08:00\",\"2022-06-19T20:00:00+08:00\",\"2022-06-20T08:00:00+08:00\",\"2022-06-20T20:00:00+08:00\",\"2022-06-21T08:00:00+08:00\",\"2022-06-21T20:00:00+08:00\",\"2022-06-22T08:00:00+08:00\",\"2022-06-22T20:00:00+08:00\",\"2022-06-23T08:00:00+08:00\",\"2022-06-23T20:00:00+08:00\",\"2022-06-24T08:00:00+08:00\",\"2022-06-24T20:00:00+08:00\",\"2022-06-25T08:00:00+08:00\",\"2022-06-25T20:00:00+08:00\",\"2022-06-26T08:00:00+08:00\",\"2022-06-26T20:00:00+08:00\",\"2022-06-27T08:00:00+08:00\",\"2022-06-27T20:00:00+08:00\",\"2022-06-28T08:00:00+08:00\",\"2022-06-28T20:00:00+08:00\",\"2022-06-29T08:00:00+08:00\",\"2022-06-29T20:00:00+08:00\",\"2022-06-30T08:00:00+08:00\",\"2022-06-30T20:00:00+08:00\",\"2022-07-01T08:00:00+08:00\",\"2022-07-01T20:00:00+08:00\",\"2022-07-02T08:00:00+08:00\",\"2022-07-02T20:00:00+08:00\",\"2022-07-03T08:00:00+08:00\",\"2022-07-03T20:00:00+08:00\",\"2022-07-04T08:00:00+08:00\",\"2022-07-04T20:00:00+08:00\",\"2022-07-05T08:00:00+08:00\",\"2022-07-05T20:00:00+08:00\",\"2022-07-06T08:00:00+08:00\",\"2022-07-06T20:00:00+08:00\",\"2022-07-07T08:00:00+08:00\",\"2022-07-07T20:00:00+08:00\",\"2022-07-08T08:00:00+08:00\",\"2022-07-08T20:00:00+08:00\",\"2022-07-09T08:00:00+08:00\",\"2022-07-09T20:00:00+08:00\",\"2022-07-10T08:00:00+08:00\",\"2022-07-10T20:00:00+08:00\",\"2022-07-11T08:00:00+08:00\",\"2022-07-11T20:00:00+08:00\",\"2022-07-12T08:00:00+08:00\",\"2022-07-12T20:00:00+08:00\",\"2022-07-13T08:00:00+08:00\",\"2022-07-13T20:00:00+08:00\",\"2022-07-14T08:00:00+08:00\",\"2022-07-14T20:00:00+08:00\",\"2022-07-15T08:00:00+08:00\",\"2022-07-15T20:00:00+08:00\",\"2022-07-16T08:00:00+08:00\",\"2022-07-16T20:00:00+08:00\",\"2022-07-17T08:00:00+08:00\",\"2022-07-17T20:00:00+08:00\",\"2022-07-18T08:00:00+08:00\",\"2022-07-18T20:00:00+08:00\",\"2022-07-19T08:00:00+08:00\",\"2022-07-19T20:00:00+08:00\",\"2022-07-20T08:00:00+08:00\",\"2022-07-20T20:00:00+08:00\",\"2022-07-21T08:00:00+08:00\",\"2022-07-21T20:00:00+08:00\",\"2022-07-22T08:00:00+08:00\",\"2022-07-22T20:00:00+08:00\",\"2022-07-23T08:00:00+08:00\",\"2022-07-23T20:00:00+08:00\",\"2022-07-24T08:00:00+08:00\",\"2022-07-24T20:00:00+08:00\",\"2022-07-25T08:00:00+08:00\",\"2022-07-25T20:00:00+08:00\",\"2022-07-26T08:00:00+08:00\",\"2022-07-26T20:00:00+08:00\",\"2022-07-27T08:00:00+08:00\",\"2022-07-27T20:00:00+08:00\",\"2022-07-28T08:00:00+08:00\",\"2022-07-28T20:00:00+08:00\",\"2022-07-29T08:00:00+08:00\",\"2022-07-29T20:00:00+08:00\",\"2022-07-30T08:00:00+08:00\",\"2022-07-30T20:00:00+08:00\",\"2022-07-31T08:00:00+08:00\",\"2022-07-31T20:00:00+08:00\",\"2022-08-01T08:00:00+08:00\",\"2022-08-01T20:00:00+08:00\",\"2022-08-02T08:00:00+08:00\",\"2022-08-02T20:00:00+08:00\",\"2022-08-03T08:00:00+08:00\",\"2022-08-03T20:00:00+08:00\",\"2022-08-04T08:00:00+08:00\",\"2022-08-04T20:00:00+08:00\",\"2022-08-05T08:00:00+08:00\",\"2022-08-05T20:00:00+08:00\",\"2022-08-06T08:00:00+08:00\",\"2022-08-06T20:00:00+08:00\",\"2022-08-07T08:00:00+08:00\",\"2022-08-07T20:00:00+08:00\",\"2022-08-08T08:00:00+08:00\",\"2022-08-08T20:00:00+08:00\",\"2022-08-09T08:00:00+08:00\",\"2022-08-09T20:00:00+08:00\",\"2022-08-10T08:00:00+08:00\",\"2022-08-10T20:00:00+08:00\",\"2022-08-11T08:00:00+08:00\",\"2022-08-11T20:00:00+08:00\",\"2022-08-12T08:00:00+08:00\",\"2022-08-12T20:00:00+08:00\",\"2022-08-13T08:00:00+08:00\",\"2022-08-13T20:00:00+08:00\",\"2022-08-14T08:00:00+08:00\",\"2022-08-14T20:00:00+08:00\",\"2022-08-15T08:00:00+08:00\",\"2022-08-15T20:00:00+08:00\",\"2022-08-16T08:00:00+08:00\",\"2022-08-16T20:00:00+08:00\",\"2022-08-17T08:00:00+08:00\",\"2022-08-17T20:00:00+08:00\",\"2022-08-18T08:00:00+08:00\",\"2022-08-18T20:00:00+08:00\",\"2022-08-19T08:00:00+08:00\",\"2022-08-19T20:00:00+08:00\",\"2022-08-20T08:00:00+08:00\",\"2022-08-20T20:00:00+08:00\",\"2022-08-21T08:00:00+08:00\",\"2022-08-21T20:00:00+08:00\",\"2022-08-22T08:00:00+08:00\",\"2022-08-22T20:00:00+08:00\",\"2022-08-23T08:00:00+08:00\",\"2022-08-23T20:00:00+08:00\",\"2022-08-24T08:00:00+08:00\",\"2022-08-24T20:00:00+08:00\",\"2022-08-25T08:00:00+08:00\",\"2022-08-25T20:00:00+08:00\",\"2022-08-26T08:00:00+08:00\",\"2022-08-26T20:00:00+08:00\",\"2022-08-27T08:00:00+08:00\",\"2022-08-27T20:00:00+08:00\",\"2022-08-28T08:00:00+08:00\",\"2022-08-28T20:00:00+08:00\",\"2022-08-29T08:00:00+08:00\",\"2022-08-29T20:00:00+08:00\",\"2022-08-30T08:00:00+08:00\",\"2022-08-30T20:00:00+08:00\",\"2022-08-31T08:00:00+08:00\",\"2022-08-31T20:00:00+08:00\",\"2022-09-01T08:00:00+08:00\",\"2022-09-01T20:00:00+08:00\",\"2022-09-02T08:00:00+08:00\",\"2022-09-02T20:00:00+08:00\",\"2022-09-03T08:00:00+08:00\",\"2022-09-03T20:00:00+08:00\",\"2022-09-04T08:00:00+08:00\",\"2022-09-04T20:00:00+08:00\",\"2022-09-05T08:00:00+08:00\",\"2022-09-05T20:00:00+08:00\",\"2022-09-06T08:00:00+08:00\",\"2022-09-06T20:00:00+08:00\",\"2022-09-07T08:00:00+08:00\",\"2022-09-07T20:00:00+08:00\",\"2022-09-08T08:00:00+08:00\",\"2022-09-08T20:00:00+08:00\",\"2022-09-09T08:00:00+08:00\",\"2022-09-09T20:00:00+08:00\",\"2022-09-10T08:00:00+08:00\",\"2022-09-10T20:00:00+08:00\",\"2022-09-11T08:00:00+08:00\",\"2022-09-11T20:00:00+08:00\",\"2022-09-12T08:00:00+08:00\",\"2022-09-12T20:00:00+08:00\",\"2022-09-13T08:00:00+08:00\",\"2022-09-13T20:00:00+08:00\",\"2022-09-14T08:00:00+08:00\",\"2022-09-14T20:00:00+08:00\",\"2022-09-15T08:00:00+08:00\",\"2022-09-15T20:00:00+08:00\",\"2022-09-16T08:00:00+08:00\",\"2022-09-16T20:00:00+08:00\",\"2022-09-17T08:00:00+08:00\",\"2022-09-17T20:00:00+08:00\",\"2022-09-18T08:00:00+08:00\",\"2022-09-18T20:00:00+08:00\",\"2022-09-19T08:00:00+08:00\",\"2022-09-19T20:00:00+08:00\",\"2022-09-20T08:00:00+08:00\",\"2022-09-20T20:00:00+08:00\",\"2022-09-21T08:00:00+08:00\",\"2022-09-21T20:00:00+08:00\",\"2022-09-22T08:00:00+08:00\",\"2022-09-22T20:00:00+08:00\",\"2022-09-23T08:00:00+08:00\",\"2022-09-23T20:00:00+08:00\",\"2022-09-24T08:00:00+08:00\",\"2022-09-24T20:00:00+08:00\",\"2022-09-25T08:00:00+08:00\",\"2022-09-25T20:00:00+08:00\",\"2022-09-26T08:00:00+08:00\",\"2022-09-26T20:00:00+08:00\",\"2022-09-27T08:00:00+08:00\",\"2022-09-27T20:00:00+08:00\",\"2022-09-28T08:00:00+08:00\",\"2022-09-28T20:00:00+08:00\",\"2022-09-29T08:00:00+08:00\",\"2022-09-29T20:00:00+08:00\",\"2022-09-30T08:00:00+08:00\",\"2022-09-30T20:00:00+08:00\",\"2022-10-01T08:00:00+08:00\",\"2022-10-01T20:00:00+08:00\",\"2022-10-02T08:00:00+08:00\",\"2022-10-02T20:00:00+08:00\",\"2022-10-03T08:00:00+08:00\",\"2022-10-03T20:00:00+08:00\",\"2022-10-04T08:00:00+08:00\",\"2022-10-04T20:00:00+08:00\",\"2022-10-05T08:00:00+08:00\",\"2022-10-05T20:00:00+08:00\",\"2022-10-06T08:00:00+08:00\",\"2022-10-06T20:00:00+08:00\",\"2022-10-07T08:00:00+08:00\",\"2022-10-07T20:00:00+08:00\",\"2022-10-08T08:00:00+08:00\",\"2022-10-08T20:00:00+08:00\",\"2022-10-09T08:00:00+08:00\",\"2022-10-09T20:00:00+08:00\",\"2022-10-10T08:00:00+08:00\",\"2022-10-10T20:00:00+08:00\",\"2022-10-11T08:00:00+08:00\",\"2022-10-11T20:00:00+08:00\",\"2022-10-12T08:00:00+08:00\",\"2022-10-12T20:00:00+08:00\",\"2022-10-13T08:00:00+08:00\",\"2022-10-13T20:00:00+08:00\",\"2022-10-14T08:00:00+08:00\",\"2022-10-14T20:00:00+08:00\",\"2022-10-15T08:00:00+08:00\",\"2022-10-15T20:00:00+08:00\",\"2022-10-16T08:00:00+08:00\",\"2022-10-16T20:00:00+08:00\",\"2022-10-17T08:00:00+08:00\",\"2022-10-17T20:00:00+08:00\",\"2022-10-18T08:00:00+08:00\",\"2022-10-18T20:00:00+08:00\",\"2022-10-19T08:00:00+08:00\",\"2022-10-19T20:00:00+08:00\",\"2022-10-20T08:00:00+08:00\",\"2022-10-20T20:00:00+08:00\",\"2022-10-21T08:00:00+08:00\",\"2022-10-21T20:00:00+08:00\",\"2022-10-22T08:00:00+08:00\",\"2022-10-22T20:00:00+08:00\",\"2022-10-23T08:00:00+08:00\",\"2022-10-23T20:00:00+08:00\",\"2022-10-24T08:00:00+08:00\",\"2022-10-24T20:00:00+08:00\",\"2022-10-25T08:00:00+08:00\",\"2022-10-25T20:00:00+08:00\",\"2022-10-26T08:00:00+08:00\",\"2022-10-26T20:00:00+08:00\",\"2022-10-27T08:00:00+08:00\",\"2022-10-27T20:00:00+08:00\",\"2022-10-28T08:00:00+08:00\",\"2022-10-28T20:00:00+08:00\",\"2022-10-29T08:00:00+08:00\",\"2022-10-29T20:00:00+08:00\",\"2022-10-30T08:00:00+08:00\",\"2022-10-30T20:00:00+08:00\",\"2022-10-31T08:00:00+08:00\",\"2022-10-31T20:00:00+08:00\",\"2022-11-01T08:00:00+08:00\",\"2022-11-01T20:00:00+08:00\",\"2022-11-02T08:00:00+08:00\",\"2022-11-02T20:00:00+08:00\",\"2022-11-03T08:00:00+08:00\",\"2022-11-03T20:00:00+08:00\",\"2022-11-04T08:00:00+08:00\",\"2022-11-04T20:00:00+08:00\",\"2022-11-05T08:00:00+08:00\",\"2022-11-05T20:00:00+08:00\",\"2022-11-06T08:00:00+08:00\",\"2022-11-06T20:00:00+08:00\",\"2022-11-07T08:00:00+08:00\",\"2022-11-07T20:00:00+08:00\",\"2022-11-08T08:00:00+08:00\",\"2022-11-08T20:00:00+08:00\",\"2022-11-09T08:00:00+08:00\",\"2022-11-09T20:00:00+08:00\",\"2022-11-10T08:00:00+08:00\",\"2022-11-10T20:00:00+08:00\",\"2022-11-11T08:00:00+08:00\",\"2022-11-11T20:00:00+08:00\",\"2022-11-12T08:00:00+08:00\",\"2022-11-12T20:00:00+08:00\",\"2022-11-13T08:00:00+08:00\",\"2022-11-13T20:00:00+08:00\",\"2022-11-14T08:00:00+08:00\",\"2022-11-14T20:00:00+08:00\",\"2022-11-15T08:00:00+08:00\",\"2022-11-15T20:00:00+08:00\",\"2022-11-16T08:00:00+08:00\",\"2022-11-16T20:00:00+08:00\",\"2022-11-17T08:00:00+08:00\",\"2022-11-17T20:00:00+08:00\",\"2022-11-18T08:00:00+08:00\",\"2022-11-18T20:00:00+08:00\",\"2022-11-19T08:00:00+08:00\",\"2022-11-19T20:00:00+08:00\",\"2022-11-20T08:00:00+08:00\",\"2022-11-20T20:00:00+08:00\",\"2022-11-21T08:00:00+08:00\",\"2022-11-21T20:00:00+08:00\",\"2022-11-22T08:00:00+08:00\",\"2022-11-22T20:00:00+08:00\",\"2022-11-23T08:00:00+08:00\",\"2022-11-23T20:00:00+08:00\",\"2022-11-24T08:00:00+08:00\",\"2022-11-24T20:00:00+08:00\",\"2022-11-25T08:00:00+08:00\",\"2022-11-25T20:00:00+08:00\",\"2022-11-26T08:00:00+08:00\",\"2022-11-26T20:00:00+08:00\",\"2022-11-27T08:00:00+08:00\",\"2022-11-27T20:00:00+08:00\",\"2022-11-28T08:00:00+08:00\",\"2022-11-28T20:00:00+08:00\",\"2022-11-29T08:00:00+08:00\",\"2022-11-29T20:00:00+08:00\",\"2022-11-30T08:00:00+08:00\",\"2022-11-30T20:00:00+08:00\",\"2022-12-01T08:00:00+08:00\",\"2022-12-01T20:00:00+08:00\",\"2022-12-02T08:00:00+08:00\",\"2022-12-02T20:00:00+08:00\",\"2022-12-03T08:00:00+08:00\",\"2022-12-03T20:00:00+08:00\",\"2022-12-04T08:00:00+08:00\",\"2022-12-04T20:00:00+08:00\",\"2022-12-05T08:00:00+08:00\",\"2022-12-05T20:00:00+08:00\",\"2022-12-06T08:00:00+08:00\",\"2022-12-06T20:00:00+08:00\",\"2022-12-07T08:00:00+08:00\",\"2022-12-07T20:00:00+08:00\",\"2022-12-08T08:00:00+08:00\",\"2022-12-08T20:00:00+08:00\",\"2022-12-09T08:00:00+08:00\",\"2022-12-09T20:00:00+08:00\",\"2022-12-10T08:00:00+08:00\",\"2022-12-10T20:00:00+08:00\",\"2022-12-11T08:00:00+08:00\",\"2022-12-11T20:00:00+08:00\",\"2022-12-12T08:00:00+08:00\",\"2022-12-12T20:00:00+08:00\",\"2022-12-13T08:00:00+08:00\",\"2022-12-13T20:00:00+08:00\",\"2022-12-14T08:00:00+08:00\",\"2022-12-14T20:00:00+08:00\",\"2022-12-15T08:00:00+08:00\",\"2022-12-15T20:00:00+08:00\",\"2022-12-16T08:00:00+08:00\",\"2022-12-16T20:00:00+08:00\",\"2022-12-17T08:00:00+08:00\",\"2022-12-17T20:00:00+08:00\",\"2022-12-18T08:00:00+08:00\",\"2022-12-18T20:00:00+08:00\",\"2022-12-19T08:00:00+08:00\",\"2022-12-19T20:00:00+08:00\",\"2022-12-20T08:00:00+08:00\",\"2022-12-20T20:00:00+08:00\",\"2022-12-21T08:00:00+08:00\",\"2022-12-21T20:00:00+08:00\",\"2022-12-22T08:00:00+08:00\",\"2022-12-22T20:00:00+08:00\",\"2022-12-23T08:00:00+08:00\",\"2022-12-23T20:00:00+08:00\",\"2022-12-24T08:00:00+08:00\",\"2022-12-24T20:00:00+08:00\",\"2022-12-25T08:00:00+08:00\",\"2022-12-25T20:00:00+08:00\",\"2022-12-26T08:00:00+08:00\",\"2022-12-26T20:00:00+08:00\",\"2022-12-27T08:00:00+08:00\",\"2022-12-27T20:00:00+08:00\",\"2022-12-28T08:00:00+08:00\",\"2022-12-28T20:00:00+08:00\",\"2022-12-29T08:00:00+08:00\",\"2022-12-29T20:00:00+08:00\",\"2022-12-30T08:00:00+08:00\",\"2022-12-30T20:00:00+08:00\",\"2022-12-31T08:00:00+08:00\",\"2022-12-31T20:00:00+08:00\",\"2023-01-01T08:00:00+08:00\",\"2023-01-01T20:00:00+08:00\",\"2023-01-02T08:00:00+08:00\",\"2023-01-02T20:00:00+08:00\",\"2023-01-03T08:00:00+08:00\",\"2023-01-03T20:00:00+08:00\",\"2023-01-04T08:00:00+08:00\",\"2023-01-04T20:00:00+08:00\",\"2023-01-05T08:00:00+08:00\",\"2023-01-05T20:00:00+08:00\",\"2023-01-06T08:00:00+08:00\",\"2023-01-06T20:00:00+08:00\",\"2023-01-07T08:00:00+08:00\",\"2023-01-07T20:00:00+08:00\",\"2023-01-08T08:00:00+08:00\",\"2023-01-08T20:00:00+08:00\",\"2023-01-09T08:00:00+08:00\",\"2023-01-09T20:00:00+08:00\",\"2023-01-10T08:00:00+08:00\",\"2023-01-10T20:00:00+08:00\",\"2023-01-11T08:00:00+08:00\",\"2023-01-11T20:00:00+08:00\",\"2023-01-12T08:00:00+08:00\",\"2023-01-12T20:00:00+08:00\",\"2023-01-13T08:00:00+08:00\",\"2023-01-13T20:00:00+08:00\",\"2023-01-14T08:00:00+08:00\",\"2023-01-14T20:00:00+08:00\",\"2023-01-15T08:00:00+08:00\",\"2023-01-15T20:00:00+08:00\",\"2023-01-16T08:00:00+08:00\",\"2023-01-16T20:00:00+08:00\",\"2023-01-17T08:00:00+08:00\",\"2023-01-17T20:00:00+08:00\",\"2023-01-18T08:00:00+08:00\",\"2023-01-18T20:00:00+08:00\",\"2023-01-19T08:00:00+08:00\",\"2023-01-19T20:00:00+08:00\",\"2023-01-20T08:00:00+08:00\",\"2023-01-20T20:00:00+08:00\",\"2023-01-21T08:00:00+08:00\",\"2023-01-21T20:00:00+08:00\",\"2023-01-22T08:00:00+08:00\",\"2023-01-22T20:00:00+08:00\",\"2023-01-23T08:00:00+08:00\",\"2023-01-23T20:00:00+08:00\",\"2023-01-24T08:00:00+08:00\",\"2023-01-24T20:00:00+08:00\",\"2023-01-25T08:00:00+08:00\",\"2023-01-25T20:00:00+08:00\",\"2023-01-26T08:00:00+08:00\",\"2023-01-26T20:00:00+08:00\",\"2023-01-27T08:00:00+08:00\",\"2023-01-27T20:00:00+08:00\",\"2023-01-28T08:00:00+08:00\",\"2023-01-28T20:00:00+08:00\",\"2023-01-29T08:00:00+08:00\",\"2023-01-29T20:00:00+08:00\",\"2023-01-30T08:00:00+08:00\",\"2023-01-30T20:00:00+08:00\",\"2023-01-31T08:00:00+08:00\",\"2023-01-31T20:00:00+08:00\",\"2023-02-01T08:00:00+08:00\",\"2023-02-01T20:00:00+08:00\",\"2023-02-02T08:00:00+08:00\",\"2023-02-02T20:00:00+08:00\"],\"xhoverformat\":\"%y/%m/%d_%H:00\",\"y\":[44323.53598960303,43426.244986097794,42832.17196658347,42771.2091767285,41514.38266065065,42124.31895828899,41523.572953493334,42130.178540973924,42799.85945206694,41950.913860701025,40004.73828164954,41713.461982371286,39422.07217342779,39702.97362836404,39870.268569007516,38586.48411759036,39782.30929392716,38950.89233576413,38242.46493416699,38523.713338078,39034.1909296168,39171.07772011636,40247.06286558043,40738.20768924756,39721.519978559576,41350.8154939916,41172.65719544934,42193.25771957496,42070.81171640288,43729.894728812855,42976.841979083605,42496.826507529244,41364.23085434921,41258.091452991124,41170.98798538186,40178.320708713494,39395.9353506621,40143.926500348374,39435.78436790826,40120.25301530352,39994.54183678003,39924.23069954943,39869.620930921286,40842.488986096345,40559.06636861013,40466.67385662999,38935.26035695663,39757.29812783515,39081.23335560644,40791.418097010814,39916.486027434934,38802.97642285563,39420.82315711863,38943.282588250935,39595.00696240552,39124.78701690538,39442.48048312124,40454.70797198918,40807.84420347167,40386.493948561605,40759.942115022335,40312.21217306424,41494.914968177676,40694.310933962464,41755.06116444571,41267.08514141478,41050.46947651403,42348.13959689904,41359.06902460242,41632.27592227608,42253.64997111587,41598.19782297686,42422.74133645091,43758.69534870703,43645.06570448261,44476.62915221276,43888.17670608964,44099.86955053732,45108.716214395594,44943.08662883099,45533.339354634285,45800.38983550016,47020.80983915925,46988.35469006421,47006.14160750387,46554.85351106012,46527.510076859966,46519.029101921245,45001.46272132965,45416.914843668696,45783.40090070246,46308.77031323593,45416.39442020655,45634.56364551652,45892.046044656076,45696.802436600905,46261.79342205264,46055.06194794411,45849.38674071431,46080.701477179304,43794.23834367748,43184.733804763295,43013.587730499916,43167.532845592126,42033.7614208865,42165.459396719,42146.8860616032,42219.29816762544,41763.74645378068,41312.011178658344,41524.69861016702,41857.839015737176,40654.25760232005,40706.03395432094,40646.231516037136,40279.56812952738,40613.16727874195,40526.059956144076,40042.23575584451,40333.74613943137,40026.2414081078,39886.26291674422,39356.80336129712,39836.29840938933,40298.03737995075,40324.95676540397,40960.70606676489,40694.021809816826,41115.54168422567,40893.08185659442,39522.579436428845,39674.25396323064,39337.551548186224,39157.16506622825,39314.73387061246,38943.32884811424,39237.2832944789,38585.219681326766,38821.47651318787,39045.509176171385,38546.28815136943,38952.796700136736,38840.847830945626,38339.30995798996,38717.71563980123,38578.38864151249,37610.94070032099,38250.10166660044,37228.29067605175,37561.5621512346,38151.75705220876,38017.665128450375,38350.32366044447,36899.76083685737,37439.2896225499,37587.64114917116,36512.334481702186,36146.60400243569,36180.55874209944,35269.027410674375,35712.49373523146,34478.24974242225,35086.99099359196,34310.86999202939,34422.14809320262,32736.758638528176,32710.654583165655,32249.584453127816,32374.119860125706,32174.921033758208,32216.65899542254,32306.033051322218,30656.591365422126,30575.09690623824,30299.42281585513,30985.360213910233,30290.20361059788,30265.05559241027,29671.45673649479,29734.84431418451,31007.111987133747,29903.19166671811,30107.197663880423,29450.664191436725,29205.371266277278,29237.39658414247,29939.222317747543,29904.08409658098,28935.932619968433,29042.804469162136,29239.701867330357,29671.364216768186,29438.761914107952,28052.126366615765,28842.437581246257,28455.072905908808,28957.67668321473,29165.639826183677,29205.70279529761,28805.940476595195,29083.00814536028,28746.51004471211,28678.15531420033,29528.536888822917,28844.897063978486,29837.911289621614,30509.33465556568,28952.16404950456,29948.45887045353,29643.40398439067,30133.147519698603,30095.954589603705,29575.991798593666,29299.40407591034,28457.073644996624,29436.50096328906,29303.552043653097,28643.35825952562,27918.984697549375,28412.184230144838,28245.44248036668,28474.910677288197,27770.681358285718,28195.460625563053,28578.57903094823,28554.85928604007,27722.204876533713,28066.360912052685,28502.813084836587,26394.710636778967,25010.93356333999,26528.65028848732,26284.00885640271,25709.515324031003,24992.312040866585,23560.22425018577,22730.409039789578,21762.6507720151,21878.416079928633,21411.65598670463,21879.56101154536,20857.264292431995,19950.324252439663,20661.27088909247,20161.174781876383,19801.228713016957,19424.715830612462,19956.661853712052,19551.566158269765,19853.299971646396,20384.67738392623,20558.471835363656,20690.77311691339,20221.125637221616,19645.225034007803,20430.027469916735,20900.910473457538,20853.65602309443,20901.606298901374,21115.999562869314,21295.94657612592,21395.79078108631,20799.285263760015,21014.02933169133,20596.921491744928,20738.156709394883,20167.87282458367,19849.643514951225,19543.09289330826,20060.94893304375,19596.18379642465,19081.55245466018,18897.114379674662,19062.016336139408,18720.203169947257,18748.402411618503,19212.08429643861,19593.290627473965,19753.812353132293,19674.316705541918,20071.11453800439,21226.674358325312,21037.475372408284,21151.498225470772,21133.277621812653,21389.67869664752,20530.941434215987,20819.94607520732,20754.578960867133,20368.97023284086,20850.64720448549,20219.01695845276,19216.00674734777,19442.838132061064,19457.58731847722,19148.16087533231,19558.58609252586,20257.30663280678,20248.532678733813,20267.2370834623,20731.194599967916,20715.215672185645,20984.827792981872,21253.169695031596,20888.990848680027,22083.13139499491,21825.824397837045,22153.176538011525,22921.011241560103,23439.883070793934,22928.40703720553,22598.12896067719,22926.832274358952,23494.984350469662,22762.264738155995,22491.035449638497,22347.58361353865,22643.723838444334,22103.373940178193,22175.429459754145,21297.37292191107,21045.22004452278,21189.822594722267,21292.00485027372,22646.102366415784,22891.505158750573,23775.463684153277,23354.591447830666,23686.496329551795,23825.486016337294,23504.68735679728,23362.32262748503,22760.0481197061,23048.98722589109,22533.278414810542,22566.533474041848,22809.07971981773,22899.353915557498,22762.332200456643,23513.640567840543,22740.850276435725,23684.98131902865,22908.56926582614,22987.317045626463,24096.19873638125,22941.635430615628,23162.364368361654,23739.970804034732,23810.63852771162,23024.880054627778,23182.65895589115,23094.840387895005,23806.741134228418,23909.624997706618,23863.376699370332,23754.278594255215,24115.743528626394,23406.051690765657,24383.345272863284,24174.940733698662,24310.91774688661,24027.263830083888,23649.428686586674,23436.34804623993,24089.255901897326,23919.9467297059,22714.82524833968,22509.81502664485,21672.979881992564,21463.413063762477,21427.44794753939,20988.607609312516,21297.92225778778,21321.113869256573,20552.681642473675,19999.438734806376,21213.230085553136,21186.975685634883,21180.108023428824,21056.513233651407,21213.987590814708,20497.90803682967,20922.627551783575,20676.95105525758,19864.61436321237,20069.49737028312,19401.504944200628,19823.556807037443,19493.05899615842,19881.106004479574,20219.9864880878,19353.687665500678,19842.997514590155,20272.45673803822,19755.263756343396,19587.895570916357,19958.022664690856,19494.117190531455,19923.057918011677,19714.03465317539,19497.442118206294,19641.620619658846,19929.439851653064,19498.914723854745,18955.855732343975,18922.455147292698,18460.375010239426,19237.143649888225,18859.72484516073,19094.007922854275,19428.64791899314,20610.647178685525,21074.78009717283,21121.957447763765,21567.90638498473,20697.555969370063,21474.51350346068,20401.02639061492,21175.262302747928,21077.773495827336,19859.219306654762,19975.663092563394,19941.27081169258,20096.069806761574,19868.347919679713,19357.388454564847,19670.234272605507,18677.449418784585,18705.221720467438,19761.212003766326,19028.992576224613,18831.155524583068,19328.742034215014,18946.198022132507,18747.37216591288,19254.589400836034,18614.542750923778,19294.284218537854,19350.586327165132,18529.26462042064,19665.13990515936,18418.43080668454,18708.246922777966,19049.054900690448,19272.22404622566,18869.385446613654,18891.11601739982,19840.645971538965,19115.97364794556,18961.1679066465,19221.38060146803,18942.868275721907,19550.82021797402,19399.824169167317,19195.424963166937,19226.598328549648,19210.193424526136,18812.16970568616,19112.90122202458,19661.91327969404,19583.6570109413,19454.60162979993,20177.845680113882,19006.28572957334,18647.228235588176,18707.793961616466,19201.90905400645,19076.914903364144,19397.996904566884,18705.39904994343,18736.68517374294,18983.926795643987,19407.339469459606,19124.261873453856,19040.020734886406,19164.322915073484,18690.509156443062,19192.05955811171,18914.720112649724,18769.14514157374,19286.258132254938,19603.23264309531,19500.246622418985,18779.708774108207,19210.7658903345,19042.139051126782,19284.513749909587,19495.52811636217,19721.59621333098,20003.897029132117,19176.469984178897,19081.695089238696,19033.53086156398,19289.09925885941,19401.73238852853,18961.168870393652,18888.758691865718,19092.581577069126,19431.400380859617,19764.174562511966,19411.589594400488,19359.100069507025,19579.05800953135,19810.904734424315,19978.188110101968,20238.58295313525,20880.03956513107,20288.90982941934,20181.92618555599,20281.286589446012,19999.136118200608,20840.21367781656,20773.144586005714,20655.781385313952,20444.373810023302,20762.25617068098,21630.920028779423,20446.260826947168,20887.90952437534,21258.39513209043,20073.822667501867,19994.722156243864,20665.23959986493,20443.396570411045,20462.875827849843,20514.544240169693,21199.679800594226,21143.593571329024,21008.865574450232,21075.766974256607,20187.53904896998,19532.838623609627,17950.11715305131,16249.7915450132,15844.829810425057,15692.040228168247,15839.481977477903,16205.989236949014,15625.261224250193,15808.497506537009,16357.041183090538,16209.898195398042,16233.525420579595,16866.28228703316,15887.43899951491,15919.969320887816,16933.324393923744,16725.513523004833,16258.257099997487,16418.798100598855,16793.246636603493,16071.749859876349,16951.45344160241,16801.63991055137,16171.19507351576,16716.15264691622,16755.8744495383,16799.39727092837,16076.869284748449,16496.662124260678,16146.625303619425,15802.742008544505,16385.912156526814,16483.92909688677,15641.166907248902,16855.00740910042,16425.030653431662,16529.749491487513,17250.831856939243,16428.143556733034,16988.868033542298,16505.783991055563,16335.645032566159,16423.268923637574,16318.651279032698,16774.22708655626,17034.48507749394,17080.297762117465,17617.56077825534,16827.495319148642,16936.741841325187,16899.00150284788,16731.847269288613,17129.414171978482,16996.236844267463,17035.99816052278,16923.610786377452,16744.401039692224,16872.060914957314,16639.432591123972,16639.470177262905,16612.863045886974,17172.178524359828,17620.24577782117,17089.363731577527,16915.893099183217,16939.11747805518,16192.43316950719,17121.3350796022,17935.15883350314,18507.015553666628,17618.32984848274,18021.70043652272,17564.146056095837,17391.78180543217,17558.912909059785,17826.24094910547,17645.712796316133,16536.85230799869,16569.208191137062,17335.074923000997,16719.340722495457,16764.00172927219,17242.433764255606,16650.973463270697,17403.546266918187,16714.128777896753,16573.01691988227,15526.99274588353,16628.02375233709,16577.54749524442,15590.241543983342,16167.664867697516,15936.976566881407,16373.139615519673,16654.194306253106,16926.619604986394,16145.339664918487,16722.723474999424,16853.66201807605,15838.671466122963,14640.212368819397,15613.0553665685,15883.870243810583,16465.088803809835,15929.82459926547,16372.54980226257,16160.033917746972,16283.111175303698,16110.931000346085,16370.709045202007,16542.530706219026,16661.981383242295,16491.35284119961,16895.87896207499,16638.763750600396,16709.930695302086,16824.048959332635,17312.4249374317,16651.031288099824,17001.07870995975,17131.132533150725,16973.23027225188,16567.719201787026,16547.59712499776,17181.274369981606,17449.215353216045,17074.2878348768,17853.985302120913,17218.262985680252,17940.552926313598,18175.952023209305,18561.832527932478,18923.740785993636,20077.390459459042,20911.926103406353,20895.97801553295,21619.81958908122,22425.62785791792,23274.18987791729,23918.89046282717,23927.111226034816,23733.673680142732,24574.134441581322,24932.286013243487,23823.999918228714,23442.25967127108,23655.58317590016,24222.324326180154,24426.420915574767,25146.609887416475,25381.53674820764,25629.909809265286,26287.248974328162,25415.451010491,26933.447222312447,27373.598256666217,24918.240362248383,25879.583928978303,25023.52781112399,24039.48799896147,25065.44310226431,24882.48919789307,24340.606941655045,25556.237121968064,25697.732551349094,24536.712139664218,26147.627069410868,27538.751751137668,26347.095744980965,27308.718798385005,27022.520589100663,27908.940524729904,26508.076214283705],\"yhoverformat\":\"$000,.0f\",\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"\\u6700\\u540e\\u7b2c31\\u65f6\\u5e8f\\u9884\\u6d4b\\u503c\",\"showlegend\":true,\"x\":[\"2023-02-02T20:00:00+08:00\",\"2023-02-03T08:00:00+08:00\",\"2023-02-03T20:00:00+08:00\",\"2023-02-04T08:00:00+08:00\",\"2023-02-04T20:00:00+08:00\",\"2023-02-05T08:00:00+08:00\",\"2023-02-05T20:00:00+08:00\"],\"xhoverformat\":\"%y/%m/%d_%H:00\",\"y\":[26508.076214283705,27396.049710322175,28288.959519273838,27129.858891916,25891.32044179691,26569.085264001973,26426.75908457581],\"yhoverformat\":\"$000,.0f\",\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"\\u6700\\u540e\\u7b2c15\\u65f6\\u5e8f\\u9884\\u6d4b\\u503c\",\"showlegend\":true,\"x\":[\"2023-01-25T20:00:00+08:00\",\"2023-01-26T08:00:00+08:00\",\"2023-01-26T20:00:00+08:00\",\"2023-01-27T08:00:00+08:00\",\"2023-01-27T20:00:00+08:00\",\"2023-01-28T08:00:00+08:00\",\"2023-01-28T20:00:00+08:00\"],\"xhoverformat\":\"%y/%m/%d_%H:00\",\"y\":[24918.240362248383,23776.662585610524,24250.21516876272,23914.07943704375,24561.572961200494,24455.30249022972,23557.333008729387],\"yhoverformat\":\"$000,.0f\",\"type\":\"scatter\"}],                        {\"margin\":{\"b\":10,\"l\":50,\"pad\":0,\"r\":15,\"t\":50},\"title\":{\"font\":{\"color\":\"rgb(0,125,125)\",\"family\":\"SimHei\",\"size\":20},\"text\":\"DLinear+3.5\\u65e5\\u9884\\u6d4b,\\u59cb\\u4e8e:2023\\u5e7402\\u670802\\u65e508\\u65f6,\\u6b62\\u4e8e:2023\\u5e7402\\u670805\\u65e520\\u65f6\"},\"xaxis\":{\"tickangle\":-30,\"tickformat\":\"%y/%m/%d_%H:\",\"title\":{\"text\":\"\\u4ea4\\u6613\\u65e5\\u671f\"}},\"yaxis\":{\"tickformat\":\"$000,.0f\",\"title\":{\"text\":\"\\u6536\\u76d8\\u4ef7\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ffac50e6-b9a9-4b98-bbe3-2b174a71e4c8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算下预测的起始及结束日期,用于画图标注\n",
    "Predict_Start_date = datetime.datetime.strftime(data0.index[-1].tz_convert('Asia/Shanghai'), \"%Y年%m月%d日%H时\")\n",
    "Predict_End_date = data0.index[-1] + datetime.timedelta(\n",
    "    hours=L_pred * 12\n",
    ")  # 数据为每12小时一次数据\n",
    "Predict_End_date = datetime.datetime.strftime(Predict_End_date.tz_convert('Asia/Shanghai'), \"%Y年%m月%d日%H时\")\n",
    "\n",
    "# 尝试plotly绘制动态交互图\n",
    "# fig = px.scatter(\n",
    "#     x=data0.index[split:data_end],\n",
    "#     y=y_true[split:, -1],\n",
    "#     labels={'x':'日期','y':'$价格'},\n",
    "#     title='BTC',\n",
    "#     # animation_frame=np.arange(split_num,data_end),\n",
    "# )\n",
    "# 准备画布\n",
    "# fig = go.Figure()\n",
    "# 添加几组不同的数据\n",
    "trace1 = go.Scatter(  #\n",
    "    x=data0.index[-y_hat.shape[0]:],\n",
    "    # y=y_true[-y_hat.shape[0]:, -1],\n",
    "    y=data0.iloc[-y_hat.shape[0]:, -1],\n",
    "    mode=\"markers\",  # mode模式\n",
    "    name=\"真实值\",\n",
    "    showlegend=True,\n",
    "    xhoverformat=\"%y/%m/%d_%H:00\",\n",
    "    yhoverformat=\"$000,.0f\",\n",
    "    # hovertemplate='日期:%{x},价格: %{y:$.0f}',\n",
    ")\n",
    "trace2 = go.Scatter(  #\n",
    "    x=data0.index[-y_hat.shape[0]:].shift(freq=\"12h\"),  # 预测是偏离12个小时,即一个时刻\n",
    "    y=y_hat[:, 0, 0],\n",
    "    mode=\"markers\",  # mode模式\n",
    "    name=\"验证集预测值\",\n",
    "    showlegend=True,\n",
    "    xhoverformat=\"%y/%m/%d_%H:00\",\n",
    "    yhoverformat=\"$000,.0f\",\n",
    "    # hovertemplate='日期:%{x},价格: %{y:$.0f}',\n",
    ")\n",
    "\n",
    "trace_data = [trace1, trace2]\n",
    "# fig.add_trace(\n",
    "\n",
    "# )  # 名字\n",
    "# # fig.update_layout()\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=dict(\n",
    "        text=\"DLinear+{}日预测,始于:{},止于:{}\".format(\n",
    "            L_pred / 2, Predict_Start_date, Predict_End_date\n",
    "        ),\n",
    "        font=dict(color=\"rgb(0,125,125)\", family=\"SimHei\", size=20),\n",
    "    ),\n",
    "    margin=dict(l=50, b=10, t=50, r=15, pad=0),  # pad参数是刻度与标签的距离\n",
    "    xaxis=dict(title=\"交易日期\", tickangle=-30, tickformat='%y/%m/%d_%H:'),  # 设置坐标轴的标签\n",
    "    yaxis=dict(title=\"收盘价\", tickformat='$000,.0f'),\n",
    "    # calendar=\n",
    "    # template='presentation',\n",
    "    # template='simple_white'\n",
    "    # template='gridon',\n",
    "    # template='plotly'\n",
    ")\n",
    "fig = go.Figure(data=trace_data, layout=layout)\n",
    "\n",
    "for N in np.arange(Batch_size - 1, 0, -16):\n",
    "    # last_hat_draw = last_hat[N, :]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(  #\n",
    "            x=pd.date_range(\n",
    "                start=data0.index[-Batch_size + N] + datetime.timedelta(hours=12),\n",
    "                periods=7,\n",
    "                freq='12H',\n",
    "            ),\n",
    "            # y=last_hat_draw,\n",
    "            y=y_hat[-Batch_size+N, :, 0],\n",
    "            mode=\"markers+lines\",  # mode模式\n",
    "            name=\"最后第{}时序预测值\".format(N),\n",
    "            showlegend=True,\n",
    "            xhoverformat=\"%y/%m/%d_%H:00\",\n",
    "            yhoverformat=\"$000,.0f\",\n",
    "            # hovertemplate='日期:%{x},价格: %{y:$.0f}',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.write_html('./checkpoint/LTSF_Linear&FF_230202.html',\n",
    "                include_plotlyjs=True, auto_open=False)    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d72cd-333d-418d-b721-b0a3ebe7631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "# 定义超参空间Class\n",
    "class HypermodelSpace(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        # 定义超参选项\n",
    "        hp_Batch_size = hp.Int(\"Batch_size\", min_value=32, max_value=128, step=32)\n",
    "        hp_kernel_size = hp.Int(\"kernel_size\", min_value=24, max_value=60, step=1)\n",
    "        hp_L_seq = hp.Int(\"L_seq\", min_value=48, max_value=336, step=32)\n",
    "\n",
    "        model = LTSF_FF_DLinear(\n",
    "            seq_len=hp_L_seq,\n",
    "            pred_len=L_pred,\n",
    "            channels=in_features,\n",
    "            kernel_size=hp_kernel_size,\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        hp_Batch_size = hp.get(\"Batch_size\")\n",
    "        hp_L_seq = hp.get(\"L_seq\")\n",
    "\n",
    "        dataset_train = xs_ys_dataset(\n",
    "            data[:split, :],\n",
    "            hp_L_seq,\n",
    "            L_pred,\n",
    "            hp_Batch_size,\n",
    "            out_features,\n",
    "            shuffle=True,\n",
    "            buffer_size=10000,\n",
    "        )\n",
    "\n",
    "        dataset_val = xs_ys_dataset(\n",
    "            data[split:, :],\n",
    "            hp_L_seq,\n",
    "            L_pred,\n",
    "            hp_Batch_size,\n",
    "            out_features,\n",
    "            shuffle=True,\n",
    "            buffer_size=10000,\n",
    "        )\n",
    "\n",
    "        EPOCHS = 20\n",
    "        train_loss_results = []\n",
    "        logs = {}  # 字典\n",
    "        wait = 0\n",
    "        best = np.infty  # 先设置一个无穷大的数字;\n",
    "\n",
    "        patience = 20\n",
    "\n",
    "        # # Assign the model to the callbacks.\n",
    "        # for callback in callbacks:\n",
    "        #     callback.model = model\n",
    "        # callbacks.set_model(model=model)\n",
    "        callbacks = tf.keras.callbacks.CallbackList(\n",
    "            callbacks=[], add_history=True, add_progbar=True, model=model\n",
    "        )\n",
    "\n",
    "        @tf.function  # 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地执行。\n",
    "        def train_step(dataset):  # dataset是xs与xs_timestamp合集\n",
    "            xs, ys = dataset[0], dataset[1]\n",
    "\n",
    "            # 求导,根据导数优化变量\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_ = model(xs)\n",
    "                loss_value = loss_object(ys, y_)\n",
    "            gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            # 计算metric: 每个单步经过batch次运算后,共batch次 loss的平均值\n",
    "            train_loss.update_state(loss_value)\n",
    "            train_MSE.update_state(ys, y_)\n",
    "            return {\"train_loss\": train_loss.result(), \"train_MSE\": train_MSE.result()}\n",
    "\n",
    "        @tf.function\n",
    "        def test_step(dataset_val):  # validation, test 使用该单步训练\n",
    "            xs, ys = dataset_val[0], dataset_val[1]\n",
    "            y_ = model(xs)\n",
    "            loss_value = loss_object(ys, y_)\n",
    "            val_loss.update_state(loss_value)\n",
    "            val_MSE.update_state(ys, y_)\n",
    "            return {\"val_loss\": val_loss.result(), \"val_MSE\": val_MSE.result()}\n",
    "\n",
    "        train_start = time.time()\n",
    "        callbacks.on_train_begin(logs=logs)\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_start = time.time()\n",
    "            callbacks.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "            train_loss.reset_states()\n",
    "            train_MSE.reset_states()\n",
    "\n",
    "            for batch, batch_data in enumerate(dataset_train):\n",
    "                callbacks.on_batch_begin(batch, logs=logs)\n",
    "                callbacks.on_train_batch_begin(batch, logs=logs)\n",
    "                train_dict = train_step(batch_data)\n",
    "                logs[\"train_loss\"] = train_dict[\"train_loss\"]\n",
    "                logs[\"train_MSE\"] = train_dict[\"train_MSE\"]\n",
    "\n",
    "                # if batch % 10 == 0:  # 每n次batch,打印\n",
    "                #     print(\n",
    "                #         \"Epoch {} Batch {} train_loss {:.4f} train_MSE {:.4f} \".format(\n",
    "                #             epoch + 1, batch, train_loss.result(), train_MSE.result()\n",
    "                #         )\n",
    "                #     )\n",
    "                callbacks.on_train_batch_end(batch, logs=logs)\n",
    "                callbacks.on_batch_end(batch, logs=logs)\n",
    "            # End epoch 每个epoch\n",
    "            train_loss_results.append(train_loss.result())\n",
    "\n",
    "            val_loss.reset_states()\n",
    "            val_MSE.reset_states()\n",
    "            for batch_data in dataset_val:\n",
    "                callbacks.on_batch_begin(batch, logs=logs)\n",
    "                callbacks.on_test_batch_begin(batch, logs=logs)\n",
    "                test_step(batch_data)\n",
    "                callbacks.on_test_batch_end(batch, logs=logs)\n",
    "                callbacks.on_batch_end(batch, logs=logs)\n",
    "\n",
    "            logs[\"val_loss\"] = val_loss.result()\n",
    "            logs[\"val_MSE\"] = val_MSE.result()\n",
    "            # --------------------\n",
    "            # The early stopping strategy: stop the training if `val_loss` does not\n",
    "            # decrease over a certain number of epochs.\n",
    "            #             wait += 1\n",
    "            #             if (\n",
    "            #                 val_loss.result() < best\n",
    "            #             ):  # 当loss变小,在改进时,计数器恢复为0,存储模型,实现总是存储最佳模型; 当n次loss不再变小,即触发\n",
    "            #                 best = val_loss.result()\n",
    "            #                 wait = 0\n",
    "\n",
    "            #                 ckpt_save_path = ckpt_manager.save()  # 存weight\n",
    "            #                 # print(\"Saving checkpoint for epoch {} at {}\".format(epoch + 1, ckpt_save_path))\n",
    "            #             if wait >= patience:\n",
    "            #                 print(\n",
    "            #                     \"\\nepoch:{}/{} - 共耗时:{:.2f}分,历{}次训练未见val_loss减少,故提前中止\".format(\n",
    "            #                         epoch + 1, EPOCHS, (time.time() - train_start) / 60, patience\n",
    "            #                     )\n",
    "            #                 )\n",
    "            #                 break\n",
    "            # --------------------\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:  # 每n次epoch,打印\n",
    "\n",
    "                print(\n",
    "                    \"\\nepoch:{}/{} - 耗时:{:.2f}分/总{:.2f}分; train_loss {:.4f} train_MSE {:.4f}; val_loss {:.4f} val_MSE {:.4f}\".format(\n",
    "                        epoch + 1,\n",
    "                        EPOCHS,\n",
    "                        (time.time() - epoch_start) / 60,\n",
    "                        (time.time() - train_start) / 60,\n",
    "                        train_loss.result(),\n",
    "                        train_MSE.result(),\n",
    "                        val_loss.result(),\n",
    "                        val_MSE.result(),\n",
    "                    )\n",
    "                )\n",
    "            # print(\"Time taken for 1 epoch: {} mins\\n\".format((time.time() - start) / 60))\n",
    "            best = min(\n",
    "                best, float(logs[\"val_loss\"])\n",
    "            )  # 取最小的val_loss,val_loss需要转换成float数,才能被tunner接受\n",
    "            callbacks.on_epoch_end(epoch, logs=logs)\n",
    "        callbacks.on_train_end(logs=logs)\n",
    "        print(\n",
    "            \"\\nepoch:{}/{} - 共耗时:{:.2f}分,正常结束\".format(\n",
    "                epoch + 1, EPOCHS, (time.time() - train_start) / 60\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a40bd-9d97-4a5c-adf9-803620bd4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    hypermodel=HypermodelSpace(),\n",
    "    objective=kt.Objective(\"val_loss\", \"min\"),\n",
    "    max_epochs=20,\n",
    "    directory=\"Tunner_result\",\n",
    "    project_name=\"custom_training\",\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79c03f-4c65-4abc-9a3e-557873ccb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行超参数搜索\n",
    "\n",
    "start = time.time()\n",
    "tuner.search(\n",
    "    # inputs_train,\n",
    "    epochs=20,\n",
    "    # validation_data= inputs_val,\n",
    "    # callbacks=callbacks, #callbacks是个objects,而这里必须是可以copy.deepcopy. callbacklists对象不能深度复制\n",
    ")\n",
    "end = time.time()\n",
    "print(\"共耗时{:.3f}分钟\".format((end - start) / 60))\n",
    "\n",
    "# 在Docker上运行时,用于JupyterLab上自动关闭租用连接:\n",
    "# import os\n",
    "# 若释放前要保存环境并命名为 SnapName\n",
    "# os.system(\"export $(cat /proc/1/environ |tr '\\\\0' '\\\\n' | grep MATCLOUD_CANCELTOKEN)&&/public/script/matncli node cancel -url https://matpool.com/api/public/node -save -name snapName\")\n",
    "# 若释放前不需要保存环境\n",
    "# os.system(\"export $(cat /proc/1/environ |tr '\\\\0' '\\\\n' | grep MATCLOUD_CANCELTOKEN)&&/public/script/matncli node cancel -url https://matpool.com/api/public/node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed09a44-e013-4c05-ac3c-1d6d72e54805",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=7)\n",
    "for i in range(7):\n",
    "    print(best_hps[i].values)\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fce86-cce0-44c6-9368-62cb6af93d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
